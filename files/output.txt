Anarchism.
Anarchism
is
a
political
philosophy
which
considers
the
state
undesirable,
unnecessary
and
harmful,
and
instead
promotes
a
stateless
society,
or
anarchy.
It
seeks
to
diminish
or
even
abolish
authority
in
the
conduct
of
human
relations.
Anarchists
may
widely
disagree
on
what
additional
criteria
are
required
in
anarchism.
"The
Oxford
Companion
to
Philosophy"
says,
"there
is
no
single
defining
position
that
all
anarchists
hold,
and
those
considered
anarchists
at
best
share
a
certain
family
resemblance."
There
are
many
types
and
traditions
of
anarchism,
not
all
of
which
are
mutually
exclusive.
Strains
of
anarchism
have
been
divided
into
the
categories
of
social
and
individualist
anarchism
or
similar
dual
classifications.
Anarchism
is
often
considered
to
be
a
radical
left-wing
ideology,
and
much
of
anarchist
economics
and
anarchist
legal
philosophy
reflect
anti-statist
interpretations
of
communism,
collectivism,
syndicalism
or
participatory
economics.
However,
anarchism
has
always
included
an
individualist
strain
supporting
a
market
economy
and
private
property,
or
unrestrained
egoism
that
bases
right
on
might.
Others,
such
as
panarchists
and
anarchists
without
adjectives,
neither
advocate
nor
object
to
any
particular
form
of
organization
as
long
as
it
is
not
compulsory.
Differing
fundamentally,
some
anarchist
schools
of
thought
support
anything
from
extreme
individualism
to
complete
collectivism.
The
central
tendency
of
anarchism
as
a
social
movement
have
been
represented
by
communist
anarchism,
with
individualist
anarchism
being
primarily
a
philosophical
or
literary
phenomenon.
Some
anarchists
fundamentally
oppose
all
forms
of
aggression,
supporting
self-defense
or
non-violence,
while
others
have
supported
the
use
of
some
coercive
measures,
including
violent
revolution
and
terrorism,
on
the
path
to
an
anarchist
society.
Etymology
and
terminology.
The
term
"anarchism"
derives
from
the
Greek
ἄναρχος,
"anarchos",
meaning
"without
rulers",
from
the
prefix
ἀν-
("an-",
"without")
+
ἀρχή
("archê",
"sovereignty,
realm,
magistracy")
+
-ισμός
("-ismos",
from
the
suffix
-ιζειν,
"-izein"
"-izing").
There
is
some
ambiguity
with
the
use
of
the
terms
"libertarianism"
and
"libertarian"
in
writings
about
anarchism.
Since
the
1890s
from
France,
the
term
"libertarianism"
has
often
been
used
as
a
synonym
for
anarchism
and
was
used
almost
exclusively
in
this
sense
until
the
1950s
in
the
United
States;
its
use
as
a
synonym
is
still
common
outside
the
United
States.
Accordingly,
"libertarian
socialism"
is
sometimes
used
as
a
synonym
for
socialist
anarchism,
to
distinguish
it
from
"individualist
libertarianism"
(individualist
anarchism).
On
the
other
hand,
some
use
"libertarianism"
to
refer
to
individualistic
free-market
philosophy
only,
referring
to
free-market
anarchism
as
"libertarian
anarchism."
Origins.
Some
claim
anarchist
themes
can
be
found
in
the
works
of
Taoist
sages
Laozi
and
Zhuangzi.
The
latter
has
been
translated,
"There
has
been
such
a
thing
as
letting
mankind
alone;
there
has
never
been
such
a
thing
as
governing
mankind
[with
success],"
and
"A
petty
thief
is
put
in
jail.
A
great
brigand
becomes
a
ruler
of
a
Nation."
Diogenes
of
Sinope
and
the
Cynics,
and
their
contemporary
Zeno
of
Citium,
the
founder
of
Stoicism,
also
introduced
similar
topics.
Modern
anarchism,
however,
sprang
from
the
secular
or
religious
thought
of
the
Enlightenment,
particularly
Jean-Jacques
Rousseau's
arguments
for
the
moral
centrality
of
freedom.
Although
by
the
turn
of
the
19th
century
the
term
"anarchist"
had
lost
its
initial
negative
connotation,
it
first
entered
the
English
language
in
1642
during
the
English
Civil
War
as
a
term
of
abuse
used
by
Royalists
to
damn
those
who
were
fomenting
disorder.
By
the
time
of
the
French
Revolution
some,
such
as
the
"Enragés",
began
to
use
the
term
positively,
in
opposition
to
Jacobin
centralisation
of
power,
seeing
"revolutionary
government"
as
oxymoronic.
From
this
climate
William
Godwin
developed
what
many
consider
the
first
expression
of
modern
anarchist
thought.
Godwin
was,
according
to
Peter
Kropotkin,
"the
first
to
formulate
the
political
and
economical
conceptions
of
anarchism,
even
though
he
did
not
give
that
name
to
the
ideas
developed
in
his
work",
while
Godwin
attached
his
anarchist
ideas
to
an
early
Edmund
Burke.
Benjamin
Tucker
instead
credits
Josiah
Warren,
an
American
who
promoted
stateless
and
voluntary
communities
where
all
goods
and
services
were
private,
with
being
"the
first
man
to
expound
and
formulate
the
doctrine
now
known
as
Anarchism."
The
first
to
describe
himself
as
an
anarchist
was
Pierre-Joseph
Proudhon,
a
French
philosopher
and
politician,
which
led
some
to
call
him
the
founder
of
modern
anarchist
theory.
Social
movement.
Anarchism
as
a
social
movement
has
regularly
endured
fluctuations
in
popularity.
Its
classical
period,
which
scholars
demarcate
as
from
1860
to
1939,
is
associated
with
the
working-class
movements
of
the
nineteenth
century
and
the
Spanish
Civil
War-era
struggles
against
fascism.
Anarchists
were
heavilly
involved
in
the
abolition
of
slavery,
and
continue
to
be
active
in
the
labour
movement,
civil
rights,
women's
liberation,
both
anti-capitalism
and
pro-capitalism
(with
varying
definitions
of
capitalism),
the
anti-war
movement,
LGBT
rights,
both
anti-globalization
and
pro-globalization
(with
varying
definitions
of
globalization),
tax
resistance,
and
other
areas.
The
First
International.
In
Europe,
harsh
reaction
followed
the
revolutions
of
1848,
during
which
ten
countries
had
experienced
brief
or
long-term
social
upheaval
as
groups
carried
out
nationalist
uprisings.
After
most
of
these
attempts
at
systematic
change
ended
in
failure,
conservative
elements
took
advantage
of
the
divided
groups
of
socialists,
anarchists,
liberals,
and
nationalists,
to
prevent
further
revolt.
In
1864
the
International
Workingmen's
Association
(sometimes
called
the
"First
International")
united
diverse
revolutionary
currents
including
French
followers
of
Proudhon,
Blanquists,
Philadelphes,
English
trade
unionists,
socialists
and
social
democrats.
Due
to
its
links
to
active
workers'
movements,
the
International
became
a
significant
organization.
Karl
Marx
became
a
leading
figure
in
the
International
and
a
member
of
its
General
Council.
Proudhon's
followers,
the
mutualists,
opposed
Marx's
state
socialism,
advocating
political
abstentionism
and
small
property
holdings.
In
1868,
following
their
unsuccessful
participation
in
the
League
of
Peace
and
Freedom
(LPF),
Russian
revolutionary
Mikhail
Bakunin
and
his
collectivist
anarchist
associates
and
joined
the
First
International
(which
had
decided
not
to
get
involved
with
the
LPF).
They
allied
themselves
with
the
federalist
socialist
sections
of
the
International,
who
advocated
the
revolutionary
overthrow
of
the
state
and
the
collectivization
of
property.
At
first,
the
collectivists
worked
with
the
Marxists
to
push
the
First
International
in
a
more
revolutionary
socialist
direction.
Subsequently,
the
International
became
polarised
into
two
camps,
with
Marx
and
Bakunin
as
their
respective
figureheads.
Bakunin
characterised
Marx's
ideas
as
centralist
and
predicted
that,
if
a
Marxist
party
came
to
power,
its
leaders
would
simply
take
the
place
of
the
ruling
class
they
had
fought
against.
In
1872,
the
conflict
climaxed
with
a
final
split
between
the
two
groups
at
the
Hague
Congress,
where
Bakunin
and
James
Guillaume
were
expelled
from
the
International
and
its
headquarters
were
transferred
to
New
York.
In
response,
the
federalist
sections
formed
their
own
International
at
the
St.
Imier
Congress,
adopting
a
revolutionary
anarchist
program.
Organised
labour.
The
anti-authoritarian
sections
of
the
First
International
were
the
precursors
of
the
anarcho-syndicalists,
seeking
to
"replace
the
privilege
and
authority
of
the
State"
with
the
"free
and
spontaneous
organization
of
labor."
In
1886,
the
Federation
of
Organized
Trades
and
Labor
Unions
(FOTLU)
of
the
United
States
and
Canada
unanimously
set
1
May
1886,
as
the
date
by
which
the
eight-hour
work
day
would
become
standard.
In
response,
unions
across
America
prepared
a
general
strike
in
support
of
the
event.
On
3
May,
in
Chicago,
a
fight
broke
out
when
strikebreakers
attempted
to
cross
the
picket
line,
and
two
workers
died
when
police
opened
fire
upon
the
crowd.
The
next
day,
4
May,
anarchists
staged
a
rally
at
Chicago's
Haymarket
Square.
A
bomb
was
thrown
by
an
unknown
party
near
the
conclusion
of
the
rally,
killing
an
officer.
In
the
ensuing
panic,
police
opened
fire
on
the
crowd
and
each
other.
Seven
police
officers
and
at
least
four
workers
were
killed.
Eight
anarchists
directly
and
indirectly
related
to
the
organisers
of
the
rally
were
arrested
and
charged
with
the
murder
of
the
deceased
officer.
The
men
became
international
political
celebrities
among
the
labour
movement.
Four
of
the
men
were
executed
and
a
fifth
committed
suicide
prior
to
his
own
execution.
The
incident
became
known
as
the
Haymarket
affair,
and
was
a
setback
for
the
labour
movement
and
the
struggle
for
the
eight
hour
day.
In
1890
a
second
attempt,
this
time
international
in
scope,
to
organise
for
the
eight
hour
day
was
made.
The
event
also
had
the
secondary
purpose
of
memorializing
workers
killed
as
a
result
of
the
Haymarket
affair.
Although
it
had
initially
been
conceived
as
a
once-off
event,
by
the
following
year
the
celebration
of
International
Workers'
Day
on
May
Day
had
become
firmly
established
as
an
international
worker's
holiday.
In
1907,
the
International
Anarchist
Congress
of
Amsterdam
gathered
delegates
from
14
different
countries,
among
which
important
figures
of
the
anarchist
movement,
including
Errico
Malatesta,
Pierre
Monatte,
Luigi
Fabbri,
Benoît
Broutchoux,
Emma
Goldman,
Rudolf
Rocker,
and
Christiaan
Cornelissen.
Various
themes
were
treated
during
the
Congress,
in
particular
concerning
the
organisation
of
the
anarchist
movement,
popular
education
issues,
the
general
strike
or
antimilitarism.
A
central
debate
concerned
the
relation
between
anarchism
and
syndicalism
(or
trade
unionism).
Malatesta
and
Monatte
were
in
particular
disagreement
themselves
on
this
issue,
as
the
latter
thought
that
syndicalism
was
revolutionary
and
would
create
the
conditions
of
a
social
revolution,
while
Malatesta
did
not
consider
syndicalism
by
itself
sufficient.
He
thought
that
the
trade-union
movement
was
reformist
and
even
conservative,
citing
as
essentially
bourgeois
and
anti-worker
the
phenomenon
of
professional
union
officials.
Malatesta
warned
that
the
syndicalists
aims
were
in
perpetuating
syndicalism
itself,
whereas
anarchists
must
always
have
anarchy
as
their
end
and
consequently
refrain
from
committing
to
any
particular
method
of
achieving
it.
The
Spanish
Workers
Federation
in
1881
was
the
first
major
anarcho-syndicalist
movement;
anarchist
trade
union
federations
were
of
special
importance
in
Spain.
The
most
successful
was
the
Confederación
Nacional
del
Trabajo
(National
Confederation
of
Labour:
CNT),
founded
in
1910.
Before
the
1940s,
the
CNT
was
the
major
force
in
Spanish
working
class
politics,
attracting
1.58
million
members
at
one
point
and
playing
a
major
role
in
the
Spanish
Civil
War.
The
CNT
was
affiliated
with
the
International
Workers
Association,
a
federation
of
anarcho-syndicalist
trade
unions
founded
in
1922,
with
delegates
representing
two
million
workers
from
15
countries
in
Europe
and
Latin
America.
The
largest
organised
anarchist
movement
today
is
in
Spain,
in
the
form
of
the
Confederación
General
del
Trabajo
(CGT)
and
the
CNT.
CGT
membership
was
estimated
to
be
around
100,000
for
the
year
2003.
Other
active
syndicalist
movements
include
the
US
Workers
Solidarity
Alliance
and
the
UK
Solidarity
Federation.
The
revolutionary
industrial
unionist
Industrial
Workers
of
the
World,
claiming
2,000
paying
members,
and
the
International
Workers
Association,
an
anarcho-syndicalist
successor
to
the
First
International,
also
remain
active.
Russian
Revolution.
Anarchists
participated
alongside
the
Bolsheviks
in
both
February
and
October
revolutions,
and
were
initially
enthusiastic
about
the
Bolshevik
coup.
However,
the
Bolsheviks
soon
turned
against
the
anarchists
and
other
left-wing
opposition,
a
conflict
that
culminated
in
the
1921
Kronstadt
rebellion
which
the
new
government
repressed.
Anarchists
in
central
Russia
were
either
imprisoned,
driven
underground
or
joined
the
victorious
Bolsheviks;
the
anarchists
from
Petrograd
and
Moscow
fled
to
the
Ukraine.
There,
in
the
Free
Territory,
they
fought
in
the
civil
war
against
the
Whites
(a
Western-backed
grouping
of
monarchists
and
other
opponents
of
the
October
Revolution)
and
then
the
Bolsheviks
as
part
of
the
Revolutionary
Insurrectionary
Army
of
Ukraine
led
by
Nestor
Makhno,
who
established
an
anarchist
society
in
the
region
for
a
number
of
months.
Expelled
American
anarchists
Emma
Goldman
and
Alexander
Berkman
were
amongst
those
agitating
in
response
to
Bolshevik
policy
and
the
suppression
of
the
Kronstadt
uprising,
before
they
left
Russia.
Both
wrote
accounts
of
their
experiences
in
Russia,
criticizing
the
amount
of
control
the
Bolsheviks
exercised.
For
them,
Bakunin's
predictions
about
the
consequences
of
Marxist
rule
that
the
rulers
of
the
new
"socialist”
Marxist
state
would
become
a
new
elite
had
proved
all
too
true.
The
victory
of
the
Bolsheviks
in
the
October
Revolution
and
the
resulting
Russian
Civil
War
did
serious
damage
to
anarchist
movements
internationally.
Many
workers
and
activists
saw
Bolshevik
success
as
setting
an
example;
Communist
parties
grew
at
the
expense
of
anarchism
and
other
socialist
movements.
In
France
and
the
United
States,
for
example,
members
of
the
major
syndicalist
movements
of
the
CGT
and
IWW
left
the
organizations
and
joined
the
Communist
International.
In
Paris,
the
Dielo
Truda
group
of
Russian
anarchist
exiles,
which
included
Nestor
Makhno,
concluded
that
anarchists
needed
to
develop
new
forms
of
organisation
in
response
to
the
structures
of
Bolshevism.
Their
1926
manifesto,
called
the
"Organizational
Platform
of
the
General
Union
of
Anarchists
(Draft)",
was
supported.
Platformist
groups
active
today
include
the
Workers
Solidarity
Movement
in
Ireland
and
the
North
Eastern
Federation
of
Anarchist
Communists
of
North
America.
Fight
against
fascism.
In
the
1920s
and
1930s,
the
rise
of
fascism
in
Europe
transformed
anarchism's
conflict
with
the
state.
Italy
saw
the
first
struggles
between
anarchists
and
fascists.
Italian
anarchists
played
a
key
role
in
the
anti-fascist
organisation
"Arditi
del
Popolo",
which
was
strongest
in
areas
with
anarchist
traditions,
and
achieved
some
success
in
their
activism,
such
as
repelling
Blackshirts
in
the
anarchist
stronghold
of
Parma
in
August
1922.
In
France,
where
the
far
right
leagues
came
close
to
insurrection
in
the
February
1934
riots,
anarchists
divided
over
a
united
front
policy.
In
Spain,
the
CNT
initially
refused
to
join
a
popular
front
electoral
alliance,
and
abstention
by
CNT
supporters
led
to
a
right
wing
election
victory.
But
in
1936,
the
CNT
changed
its
policy
and
anarchist
votes
helped
bring
the
popular
front
back
to
power.
Months
later,
the
former
ruling
class
responded
with
an
attempted
coup
causing
the
Spanish
Civil
War
(1936–1939).
In
response
to
the
army
rebellion,
an
anarchist-inspired
movement
of
peasants
and
workers,
supported
by
armed
militias,
took
control
of
Barcelona
and
of
large
areas
of
rural
Spain
where
they
collectivised
the
land.
But
even
before
the
fascist
victory
in
1939,
the
anarchists
were
losing
ground
in
a
bitter
struggle
with
the
Stalinists,
who
controlled
the
distribution
of
military
aid
to
the
Republican
cause
from
the
Soviet
Union.
Stalinist-led
troops
suppressed
the
collectives
and
persecuted
both
dissident
Marxists
and
anarchists.
Contemporary
anarchism.
A
surge
of
popular
interest
in
anarchism
occurred
during
the
1960s
and
1970s.
In
the
United
Kingdom
this
was
associated
with
the
punk
rock
movement,
as
exemplified
by
bands
such
as
Crass
and
the
Sex
Pistols.
The
housing
and
employment
crisis
in
most
of
Western
Europe
led
to
the
formation
of
communes
and
squatter
movements
like
that
of
Barcelona,
Spain.
In
Denmark,
squatters
occupied
a
disused
military
base
and
declared
the
Freetown
Christiania,
an
autonomous
haven
in
central
Copenhagen.
Since
the
revival
of
anarchism
in
the
mid
20th
century,
a
number
of
new
movements
and
schools
of
thought
emerged.
Although
feminist
tendencies
have
always
been
a
part
of
the
anarchist
movement
in
the
form
of
anarcha-feminism,
they
returned
with
vigour
during
the
second
wave
of
feminism
in
the
1960s.
The
American
Civil
Rights
Movement
and
the
movement
against
the
war
in
Vietnam
also
contributed
to
the
revival
of
North
American
anarchism.
European
anarchism
of
the
late
20th
century
drew
much
of
its
strength
from
the
labour
movement,
and
both
have
incorporated
animal
rights
activism.
Anarchist
anthropologist
David
Graeber
has
posited
a
rupture
between
generations
of
anarchism,
with
those
"who
often
still
have
not
shaken
the
sectarian
habits"
of
the
nineteenth
century
contrasted
with
the
younger
activists
who
are
"much
more
informed,
among
other
elements,
by
indigenous,
feminist,
ecological
and
cultural-critical
ideas",
and
who
by
the
turn
of
the
21st
century
formed
"by
far
the
majority"
of
anarchists.
Around
the
turn
of
the
21st
century,
anarchism
grew
in
popularity
and
influence
as
part
of
the
anti-war,
anti-capitalist,
and
anti-globalisation
movements.
Anarchists
became
known
for
their
involvement
in
protests
against
the
meetings
of
the
World
Trade
Organization
(WTO),
Group
of
Eight,
and
the
World
Economic
Forum.
Some
anarchist
factions
at
these
protests
engaged
in
rioting,
property
destruction,
and
violent
confrontations
with
police,
and
the
confrontations
were
selectively
portrayed
in
mainstream
media
coverage
as
violent
riots.
These
actions
were
precipitated
by
ad
hoc,
leaderless,
anonymous
cadres
known
as
"black
blocs";
other
organisational
tactics
pioneered
in
this
time
include
security
culture,
affinity
groups
and
the
use
of
decentralised
technologies
such
as
the
internet.
A
landmark
struggle
of
this
period
was
the
confrontations
at
WTO
conference
in
Seattle
in
1999.
Anarchist
schools
of
thought.
Anarchist
ideas
have
only
occasionally
inspired
political
movements
of
any
size,
and
"the
tradition
is
mainly
one
of
individual
thinkers,
but
they
have
produced
an
important
body
of
theory."
Anarchist
schools
of
thought
had
been
generally
grouped
in
two
main
historical
traditions,
individualist
anarchism
and
social
anarchism,
which
have
some
different
origins,
values
and
evolution.
The
individualist
wing
of
anarchism
emphasises
negative
liberty,
i.e.
opposition
to
state
or
social
control
over
the
individual,
while
those
in
the
social
wing
emphasise
positive
liberty
to
achieve
one's
potential
and
argue
that
humans
have
needs
that
society
ought
to
fulfill,
"recognizing
equality
of
entitlement".
In
chronological
and
theoretical
sense
there
are
classical
—
those
created
throughout
the
19th
century
—
and
post-classical
anarchist
schools
—
those
created
since
the
mid-20th
century
and
after.
Beyond
the
specific
factions
of
anarchist
thought
is
philosophical
anarchism,
which
embodies
the
theoretical
stance
that
the
State
lacks
moral
legitimacy
without
accepting
the
imperative
of
revolution
to
eliminate
it.
A
component
especially
of
individualist
anarchism
philosophical
anarchism
may
accept
the
existence
of
a
minimal
state
as
unfortunate,
and
usually
temporary,
"necessary
evil"
but
argue
that
citizens
do
not
have
a
moral
obligation
to
obey
the
state
when
its
laws
conflict
with
individual
autonomy.
One
reaction
against
sectarianism
within
the
anarchist
milieu
was
"anarchism
without
adjectives",
a
call
for
toleration
first
adopted
by
Fernando
Tarrida
del
Mármol
in
1889
in
response
to
the
"bitter
debates"
of
anarchist
theory
at
the
time.
In
abandoning
the
hyphenated
anarchisms
(i.e.
collectivist-,
communist-,
mutualist-
and
individualist-anarchism),
it
sought
to
emphasise
the
anti-authoritarian
beliefs
common
to
all
anarchist
schools
of
thought.
Mutualism.
Mutualism
began
in
18th
century
English
and
French
labour
movements
before
taking
an
anarchist
form
associated
with
Pierre-Joseph
Proudhon
in
France
and
others
in
the
United
States.
Proudhon
proposed
spontaneous
order,
whereby
organization
emerges
without
central
authority,
a
"positive
anarchy"
where
order
arises
when
everybody
does
“what
he
wishes
and
only
what
he
wishes"
and
where
"business
transactions
alone
produce
the
social
order."
Mutualist
anarchism
is
concerned
with
reciprocity,
free
association,
voluntary
contract,
federation,
and
credit
and
currency
reform.
According
to
William
Batchelder
Greene,
each
worker
in
the
mutualist
system
would
receive
"just
and
exact
pay
for
his
work;
services
equivalent
in
cost
being
exchangeable
for
services
equivalent
in
cost,
without
profit
or
discount."
Mutualism
has
been
retrospectively
characterised
as
ideologically
situated
between
individualist
and
collectivist
forms
of
anarchism.
Proudhon
first
characterised
his
goal
as
a
"third
form
of
society,
the
synthesis
of
communism
and
property."
Individualist
anarchism.
Individualist
anarchism
refers
to
several
traditions
of
thought
within
the
anarchist
movement
that
emphasise
the
individual
and
their
will
over
any
kinds
of
external
determinants
such
as
groups,
society,
traditions,
and
ideological
systems.
Individualist
anarchism
is
not
a
single
philosophy
but
refers
to
a
group
of
individualistic
philosophies
that
sometimes
are
in
conflict.
In
1793,
William
Godwin,
who
has
often
been
cited
as
the
first
anarchist,
wrote
"Political
Justice",
which
some
consider
to
be
the
first
expression
of
anarchism.
Godwin,
a
philosophical
anarchist,
from
a
rationalist
and
utilitarian
basis
opposed
revolutionary
action
and
saw
a
minimal
state
as
a
present
"necessary
evil"
that
would
become
increasingly
irrelevant
and
powerless
by
the
gradual
spread
of
knowledge.
Godwin
advocated
extreme
individualism,
proposing
that
all
cooperation
in
labour
be
eliminated
on
the
premise
that
this
would
be
most
conducive
with
the
general
good.
Godwin
was
a
utilitarian
who
believed
that
all
individuals
are
not
of
equal
value,
with
some
of
us
"of
more
worth
and
importance"
than
others
depending
on
our
utility
in
bringing
about
social
good.
Therefore
he
does
not
believe
in
equal
rights,
but
the
person's
life
that
should
be
favoured
that
is
most
conducive
to
the
general
good.
Godwin
opposed
government
because
he
saw
it
as
infringing
on
the
individual's
right
to
"private
judgement"
to
determine
which
actions
most
maximise
utility,
but
also
makes
a
critique
of
all
authority
over
the
individual's
judgement.
This
aspect
of
Godwin's
philosophy,
stripped
of
utilitarian
motivations,
was
developed
into
a
more
extreme
form
later
by
Stirner.
The
most
extreme
form
of
individualist
anarchism,
called
"egoism,"
or
egoist
anarchism,
was
expounded
by
one
of
the
earliest
and
best-known
proponents
of
individualist
anarchism,
Max
Stirner.
Stirner's
"The
Ego
and
Its
Own",
published
in
1844,
is
a
founding
text
of
the
philosophy.
According
to
Stirner,
the
only
limitation
on
the
rights
of
the
individual
is
their
power
to
obtain
what
they
desire,
without
regard
for
God,
state,
or
morality.
To
Stirner,
rights
were
"spooks"
in
the
mind,
and
he
held
that
society
does
not
exist
but
"the
individuals
are
its
reality".
Stirner
advocated
self-assertion
and
foresaw
Unions
of
Egoists,
non-systematic
associations
continually
renewed
by
all
parties'
support
through
an
act
of
will,
which
Stirner
proposed
as
a
form
of
organization
in
place
of
the
state.
Egoist
anarchists
claim
that
egoism
will
foster
genuine
and
spontaneous
union
between
individuals.
"Egoism"
has
inspired
many
interpretations
of
Stirner's
philosophy.
It
was
re-discovered
and
promoted
by
German
philosophical
anarchist
and
LGBT
activist
John
Henry
Mackay.
Individualist
anarchism
inspired
by
Stirner
attracted
a
small
following
of
European
bohemian
artists
and
intellectuals
(see
European
individualist
anarchism).
Stirner's
philosophy
has
been
seen
as
a
precedent
of
existentialism
with
other
thinkers
like
Friedrich
Nietzsche
and
Sören
Kierkegaard.
Social
anarchism.
Social
anarchism
calls
for
a
system
with
public
ownership
of
means
of
production
and
democratic
control
of
all
organizations,
without
any
government
authority
or
coercion.
It
is
the
largest
school
of
anarchism.
Social
anarchism
rejects
private
property,
seeing
it
as
a
source
of
social
inequality,
and
emphasises
cooperation
and
mutual
aid.
Collectivist
anarchism,
also
referred
to
as
"revolutionary
socialism"
or
a
form
of
such,
is
a
revolutionary
form
of
anarchism,
commonly
associated
with
Mikhail
Bakunin
and
Johann
Most.
Collectivist
anarchists
oppose
all
private
ownership
of
the
means
of
production,
instead
advocating
that
ownership
be
collectivised.
This
was
to
be
achieved
through
violent
revolution,
first
starting
with
a
small
cohesive
group
through
acts
of
violence,
or
"propaganda
by
the
deed,"
which
would
inspire
the
workers
as
a
whole
to
revolt
and
forcibly
collectivise
the
means
of
production.
However,
collectivization
was
not
to
be
extended
to
the
distribution
of
income,
as
workers
would
be
paid
according
to
time
worked,
rather
than
receiving
goods
being
distributed
"according
to
need"
as
in
anarcho-communism.
This
position
was
criticised
by
anarchist
communists
as
effectively
"uphold[ing]
the
wages
system".
Collectivist
anarchism
arose
contemporaneously
with
Marxism
but
opposed
the
Marxist
dictatorship
of
the
proletariat,
despite
the
stated
Marxist
goal
of
a
collectivist
stateless
society.
Anarchist
communist
and
collectivist
ideas
are
not
mutually
exclusive;
although
the
collectivist
anarchists
advocated
compensation
for
labour,
some
held
out
the
possibility
of
a
post-revolutionary
transition
to
a
communist
system
of
distribution
according
to
need.
Anarchist
communism
proposes
that
the
freest
form
of
social
organisation
would
be
a
society
composed
of
self-managing
communes
with
collective
use
of
the
means
of
production,
organised
democratically,
and
related
to
other
communes
through
federation.
While
some
anarchist
communists
favour
direct
democracy,
others
feel
that
its
majoritarianism
can
impede
individual
liberty
and
favour
consensus
democracy
instead.
In
anarchist
communism,
as
money
would
be
abolished,
individuals
would
not
receive
direct
compensation
for
labour
(through
sharing
of
profits
or
payment)
but
would
have
free
access
to
the
resources
and
surplus
of
the
commune.
Anarchist
communism
does
not
always
have
a
communitarian
philosophy.
Some
forms
of
anarchist
communism
are
egoist
and
strongly
influenced
by
radical
individualism,
believing
that
anarchist
communism
does
not
require
a
communitarian
nature
at
all.
In
the
early
20th
century,
anarcho-syndicalism
arose
as
a
distinct
school
of
thought
within
anarchism.
With
greater
focus
on
the
labour
movement
than
previous
forms
of
anarchism,
syndicalism
posits
radical
trade
unions
as
a
potential
force
for
revolutionary
social
change,
replacing
capitalism
and
the
state
with
a
new
society,
democratically
self-managed
by
the
workers.
It
is
often
combined
with
other
branches
of
anarchism,
and
anarcho-syndicalists
often
subscribe
to
anarchist
communist
or
collectivist
anarchist
economic
systems.
An
early
leading
anarcho-syndicalist
thinker
was
Rudolf
Rocker,
whose
1938
pamphlet
"Anarchosyndicalism"
outlined
a
view
of
the
movement's
origin,
aims
and
importance
to
the
future
of
labour.
Post-classical
currents.
Anarchism
continues
to
generate
many
philosophies
and
movements,
at
times
eclectic,
drawing
upon
various
sources,
and
syncretic,
combining
disparate
and
contrary
concepts
to
create
new
philosophical
approaches.
Since
the
revival
of
anarchism
in
the
United
States
in
the
1960s,
a
number
of
new
movements
and
schools
have
emerged.
Anarcho-capitalism
developed
from
radical
anti-state
libertarianism
and
individualist
anarchism,
drawing
from
Austrian
School
economics,
study
of
law
and
economics
and
public
choice
theory,
while
the
burgeoning
feminist
and
environmentalist
movements
also
produced
anarchist
offshoots.
Anarcha-feminism
developed
as
a
synthesis
of
radical
feminism
and
anarchism
that
views
patriarchy
(male
domination
over
women)
as
a
fundamental
manifestation
of
compulsory
government.
It
was
inspired
by
the
late
19th
century
writings
of
early
feminist
anarchists
such
as
Lucy
Parsons,
Emma
Goldman,
Voltairine
de
Cleyre,
and
Dora
Marsden.
Anarcha-feminists,
like
other
radical
feminists,
criticise
and
advocate
the
abolition
of
traditional
conceptions
of
family,
education
and
gender
roles.
Green
anarchism
(or
eco-anarchism)
is
a
school
of
thought
within
anarchism
which
puts
an
emphasis
on
environmental
issues,
and
whose
main
contemporary
currents
are
anarcho-primitivism
and
social
ecology.
Post-left
anarchy
is
a
tendency
which
seeks
to
distance
itself
from
traditional
left-wing
politics
and
to
escape
the
confines
of
ideology
in
general.
Post-anarchism
is
a
theoretical
move
towards
a
synthesis
of
classical
anarchist
theory
and
poststructuralist
thought
drawing
from
diverse
ideas
including
post-modernism,
autonomist
marxism,
post-left
anarchy,
situationism
and
postcolonialism.
Another
recent
form
of
anarchism
critical
of
formal
anarchist
movements
is
insurrectionary
anarchism,
which
advocates
informal
organization
and
active
resistance
to
the
state;
its
proponents
include
Wolfi
Landstreicher
and
Alfredo
M.
Bonanno.
Topics
of
interest
in
anarchist
theory.
Intersecting
and
overlapping
between
various
schools
of
thought,
certain
topics
of
interest
and
internal
disputes
have
proven
perennial
within
anarchist
theory.
Free
love.
An
important
current
within
anarchism
is
Free
love.
Free
love
advocates
sometimes
traced
their
roots
back
to
Josiah
Warren
and
to
experimental
communities,
viewed
sexual
freedom
as
a
clear,
direct
expression
of
an
individual's
self-ownership.
Free
love
particularly
stressed
women's
rights
since
most
sexual
laws
discriminated
against
women:
for
example,
marriage
laws
and
anti-birth
control
measures.
The
most
important
American
free
love
journal
was
"Lucifer
the
Lightbearer"
(1883–1907)
edited
by
Moses
Harman
and
Lois
Waisbrooker,
but
also
there
existed
Ezra
Heywood
and
Angela
Heywood's
'The
Word'
(1872–1890,
1892–1893).
Also
M.
E.
Lazarus
was
an
important
American
individualist
anarchist
who
promoted
free
love.
In
New
York's
Greenwich
Village,
bohemian
feminists
and
socialists
advocated
self-realisation
and
pleasure
for
women
(and
also
men)
in
the
here
and
now.
They
encouraged
playing
with
sexual
roles
and
sexuality,
and
the
openly
bisexual
radical
Edna
St.
Vincent
Millay
and
the
lesbian
anarchist
Margaret
Anderson
were
prominent
among
them.
Discussion
groups
organised
by
the
Villagers
were
frequented
by
Emma
Goldman,
among
others.
Magnus
Hirschfeld
noted
in
1923
that
Goldman
"has
campaigned
boldly
and
steadfastly
for
individual
rights,
and
especially
for
those
deprived
of
their
rights.
Thus
it
came
about
that
she
was
the
first
and
only
woman,
indeed
the
first
and
only
American,
to
take
up
the
defense
of
homosexual
love
before
the
general
public."
In
fact,
before
Goldman,
heterosexual
anarchist
Robert
Reitzel
(1849–98)
spoke
positively
of
homosexuality
from
the
beginning
of
the
1890s
in
his
Detroit-based
German
language
journal
"Der
arme
Teufel".
In
Europe
the
main
propagandist
of
free
love
within
individualist
anarchism
was
Emile
Armand.
He
proposed
the
concept
of
"la
camaraderie
amoureuse"
to
speak
of
free
love
as
the
possibility
of
voluntary
sexual
encounter
between
consenting
adults.
He
was
also
a
consistent
proponent
of
polyamory.
In
Germany
the
stirnerists
Adolf
Brand
and
John
Henry
Mackay
were
pioneering
campaigners
for
the
acceptance
of
male
bisexuality
and
homosexuality.
More
recently,
the
British
anarcho-pacifist
Alex
Comfort
gained
notoriety
during
the
sexual
revolution
for
writing
the
bestseller
sex
manual
"The
Joy
of
Sex".
The
issue
of
free
love
has
a
dedicated
treatment
in
the
work
of
french
anarcho-hedonist
philosopher
Michel
Onfray
in
such
works
as
"Théorie
du
corps
amoureux:
pour
une
érotique
solaire"
(2000)
and
"L'invention
du
plaisir:
fragments
cyréaniques"
(2002).
Libertarian
education.
In
1901,
Spanish
anarchist
and
free-thinker
Francesc
Ferrer
i
Guàrdia
established
"modern"
or
progressive
schools
in
Barcelona
in
defiance
of
an
educational
system
controlled
by
the
Catholic
Church.
The
schools'
stated
goal
was
to
"educate
the
working
class
in
a
rational,
secular
and
non-coercive
setting".
Fiercely
anti-clerical,
Ferrer
believed
in
"freedom
in
education",
education
free
from
the
authority
of
church
and
state.
Murray
Bookchin
wrote:
"This
period
[1890s]
was
the
heyday
of
libertarian
schools
and
pedagogical
projects
in
all
areas
of
the
country
where
Anarchists
exercised
some
degree
of
influence.
Perhaps
the
best-known
effort
in
this
field
was
Francisco
Ferrer's
Modern
School
(Escuela
Moderna),
a
project
which
exercised
a
considerable
influence
on
Catalan
education
and
on
experimental
techniques
of
teaching
generally."
La
Escuela
Moderna,
and
Ferrer's
ideas
generally,
formed
the
inspiration
for
a
series
of
"Modern
Schools"
in
the
United
States,
Cuba,
South
America
and
London.
The
first
of
these
was
started
in
New
York
City
in
1911.
It
also
inspired
the
Italian
newspaper
"Università
popolare",
founded
in
1901.
Another
libertarian
tradition
is
that
of
unschooling
and
the
free
school
in
which
child-led
activity
replaces
pedagogic
approaches.
Experiments
in
Germany
led
to
A.
S.
Neill
founding
what
became
Summerhill
School
in
1921.
Summerhill
is
often
cited
as
an
example
of
anarchism
in
practice.
However,
although
Summerhill
and
other
free
schools
are
radically
libertarian,
they
differ
in
principle
from
those
of
Ferrer
by
not
advocating
an
overtly-political
class
struggle-approach.
In
addition
to
organizing
schools
according
to
libertarian
principles,
anarchists
have
also
questioned
the
concept
of
schooling
per
se.
The
term
deschooling was
popularized
by
Ivan
Illich,
who
argued
that
the
school
as
an
institution
is
dysfunctional
for
self-determined
learning
and
serves
the
creation
of
a
consumer
society
instead.
Internal
issues
and
debates.
Anarchism
is
a
philosophy
which
embodies
many
diverse
attitudes,
tendencies
and
schools
of
thought;
as
such,
disagreement
over
questions
of
values,
ideology
and
tactics
is
common.
The
compatibility
of
capitalism,
nationalism
and
religion
with
anarchism
is
widely
disputed.
Similarly,
anarchism
enjoys
complex
relationships
with
ideologies
such
as
Marxism,
communism
and
capitalism.
Anarchists
may
be
motivated
by
humanism,
divine
authority,
enlightened
self-interest
or
any
number
of
alternative
ethical
doctrines.
Phenomena
such
as
civilization,
technology
(e.g.
within
anarcho-primitivism
and
insurrectionary
anarchism),
and
the
democratic
process
may
be
sharply
criticised
within
some
anarchist
tendencies
and
simultaneously
lauded
in
others.
Anarchist
attitudes
towards
race,
gender
and
the
environment
have
changed
significantly
since
the
modern
origin
of
the
philosophy
in
the
18th
century.
On
a
tactical
level,
while
propaganda
of
the
deed
was
a
tactic
used
by
anarchists
in
the
19th
century
(e.g.
the
Nihilist
movement),
contemporary
anarchists
espouse
alternative
direct
action
methods
such
as
nonviolence,
counter-economics
and
anti-state
cryptography
to
bring
about
an
anarchist
society.
About
the
scope
of
an
anarchist
society,
some
anarchists
advocate
a
global
one,
while
others
do
so
by
local
ones.
The
diversity
in
anarchism
has
led
to
widely
different
use
of
identical
terms
among
different
anarchist
traditions,
which
has
led
to
many
definitional
concerns
in
anarchist
theory.
---END.OF.DOCUMENT---
Autism.
Autism
is
a
disorder
of
neural
development
characterized
by
impaired
social
interaction
and
communication,
and
by
restricted
and
repetitive
behavior.
These
signs
all
begin
before
a
child
is
three
years
old.
Autism
affects
information
processing
in
the
brain
by
altering
how
nerve
cells
and
their
synapses
connect
and
organize;
how
this
occurs
is
not
well
understood.
The
two
other
autism
spectrum
disorders
(ASD)
are
Asperger
syndrome,
which
lacks
delays
in
cognitive
development
and
language,
and
PDD-NOS,
diagnosed
when
full
criteria
for
the
other
two
disorders
are
not
met.
Autism
has
a
strong
genetic
basis,
although
the
genetics
of
autism
are
complex
and
it
is
unclear
whether
ASD
is
explained
more
by
rare
mutations,
or
by
rare
combinations
of
common
genetic
variants.
In
rare
cases,
autism
is
strongly
associated
with
agents
that
cause
birth
defects.
Controversies
surround
other
proposed
environmental
causes,
such
as
heavy
metals,
pesticides
or
childhood
vaccines;
the
vaccine
hypotheses
are
biologically
implausible
and
lack
convincing
scientific
evidence.
The
prevalence
of
autism
is
about
1–2
per
1,000
people;
the
prevalence
of
ASD
is
about
6
per
1,000,
with
about
four
times
as
many
males
as
females.
The
number
of
people
diagnosed
with
autism
has
increased
dramatically
since
the
1980s,
partly
due
to
changes
in
diagnostic
practice;
the
question
of
whether
actual
prevalence
has
increased
is
unresolved.
Parents
usually
notice
signs
in
the
first
two
years
of
their
child's
life.
The
signs
usually
develop
gradually,
but
some
autistic
children
first
develop
more
normally
and
then
regress.
Although
early
behavioral
or
cognitive
intervention
can
help
autistic
children
gain
self-care,
social,
and
communication
skills,
there
is
no
known
cure.
Not
many
children
with
autism
live
independently
after
reaching
adulthood,
though
some
become
successful.
An
autistic
culture
has
developed,
with
some
individuals
seeking
a
cure
and
others
believing
autism
should
be
tolerated
as
a
difference
and
not
treated
as
a
disorder.
Characteristics.
Autism
is
a
highly
variable
neurodevelopmental
disorder
that
first
appears
during
infancy
or
childhood,
and
generally
follows
a
steady
course
without
remission.
Overt
symptoms
gradually
begin
after
the
age
of
six
months,
become
established
by
age
two
or
three
years,
and
tend
to
continue
through
adulthood,
although
often
in
more
muted
form.
It
is
distinguished
not
by
a
single
symptom,
but
by
a
characteristic
triad
of
symptoms:
impairments
in
social
interaction;
impairments
in
communication;
and
restricted
interests
and
repetitive
behavior.
Other
aspects,
such
as
atypical
eating,
are
also
common
but
are
not
essential
for
diagnosis.
Autism's
individual
symptoms
occur
in
the
general
population
and
appear
not
to
associate
highly,
without
a
sharp
line
separating
pathologically
severe
from
common
traits.
Social
development.
Social
deficits
distinguish
autism
and
the
related
autism
spectrum
disorders
(ASD;
see
"Classification")
from
other
developmental
disorders.
People
with
autism
have
social
impairments
and
often
lack
the
intuition
about
others
that
many
people
take
for
granted.
Noted
autistic
Temple
Grandin
described
her
inability
to
understand
the
social
communication
of
neurotypicals,
or
people
with
normal
neural
development,
as
leaving
her
feeling
"like
an
anthropologist
on
Mars".
Unusual
social
development
becomes
apparent
early
in
childhood.
Autistic
infants
show
less
attention
to
social
stimuli,
smile
and
look
at
others
less
often,
and
respond
less
to
their
own
name.
Autistic
toddlers
differ
more
strikingly
from
social
norms;
for
example,
they
have
less
eye
contact
and
turn
taking,
and
are
more
likely
to
communicate
by
manipulating
another
person's
hand.
Three-
to
five-year-old
autistic
children
are
less
likely
to
exhibit
social
understanding,
approach
others
spontaneously,
imitate
and
respond
to
emotions,
communicate
nonverbally,
and
take
turns
with
others.
However,
they
do
form
attachments
to
their
primary
caregivers.
Most
autistic
children
display
moderately
less
attachment
security
than
non-autistic
children,
although
this
difference
disappears
in
children
with
higher
mental
development
or
less
severe
ASD.
Older
children
and
adults
with
ASD
perform
worse
on
tests
of
face
and
emotion
recognition.
Contrary
to
a
common
belief,
autistic
children
do
not
prefer
being
alone.
Making
and
maintaining
friendships
often
proves
to
be
difficult
for
those
with
autism.
For
them,
the
quality
of
friendships,
not
the
number
of
friends,
predicts
how
lonely
they
feel.
Functional
friendships,
such
as
those
resulting
in
invitations
to
parties,
may
affect
the
quality
of
life
more
deeply.
There
are
many
anecdotal
reports,
but
few
systematic
studies,
of
aggression
and
violence
in
individuals
with
ASD.
The
limited
data
suggest
that,
in
children
with
mental
retardation,
autism
is
associated
with
aggression,
destruction
of
property,
and
tantrums.
A
2007
study
interviewed
parents
of
67
children
with
ASD
and
reported
that
about
two-thirds
of
the
children
had
periods
of
severe
tantrums
and
about
one-third
had
a
history
of
aggression,
with
tantrums
significantly
more
common
than
in
non-autistic
children
with
language
impairments.
A
2008
Swedish
study
found
that,
of
individuals
aged
15
or
older
discharged
from
hospital
with
a
diagnosis
of
ASD,
those
who
committed
violent
crimes
were
significantly
more
likely
to
have
other
psychopathological
conditions
such
as
psychosis.
Communication.
About
a
third
to
a
half
of
individuals
with
autism
do
not
develop
enough
natural
speech
to
meet
their
daily
communication
needs.
Differences
in
communication
may
be
present
from
the
first
year
of
life,
and
may
include
delayed
onset
of
babbling,
unusual
gestures,
diminished
responsiveness,
and
vocal
patterns
that
are
not
synchronized
with
the
caregiver.
In
the
second
and
third
years,
autistic
children
have
less
frequent
and
less
diverse
babbling,
consonants,
words,
and
word
combinations;
their
gestures
are
less
often
integrated
with
words.
Autistic
children
are
less
likely
to
make
requests
or
share
experiences,
and
are
more
likely
to
simply
repeat
others'
words
(echolalia)
or
reverse
pronouns.
Joint
attention
seems
to
be
necessary
for
functional
speech,
and
deficits
in
joint
attention
seem
to
distinguish
infants
with
ASD:
for
example,
they
may
look
at
a
pointing
hand
instead
of
the
pointed-at
object,
and
they
consistently
fail
to
point
at
objects
in
order
to
comment
on
or
share
an
experience.
Autistic
children
may
have
difficulty
with
imaginative
play
and
with
developing
symbols
into
language.
In
a
pair
of
studies,
high-functioning
autistic
children
aged
8–15
performed
equally
well
as,
and
adults
better
than,
individually
matched
controls
at
basic
language
tasks
involving
vocabulary
and
spelling.
Both
autistic
groups
performed
worse
than
controls
at
complex
language
tasks
such
as
figurative
language,
comprehension
and
inference.
As
people
are
often
sized
up
initially
from
their
basic
language
skills,
these
studies
suggest
that
people
speaking
to
autistic
individuals
are
more
likely
to
overestimate
what
their
audience
comprehends.
Repetitive
behavior.
Autistic
individuals
display
many
forms
of
repetitive
or
restricted
behavior,
which
the
Repetitive
Behavior
Scale-Revised
(RBS-R)
categorizes
as
follows.
No
single
repetitive
behavior
seems
to
be
specific
to
autism,
but
only
autism
appears
to
have
an
elevated
pattern
of
occurrence
and
severity
of
these
behaviors.
Other
symptoms.
Autistic
individuals
may
have
symptoms
that
are
independent
of
the
diagnosis,
but
that
can
affect
the
individual
or
the
family.
An
estimated
0.5%
to
10%
of
individuals
with
ASD
show
unusual
abilities,
ranging
from
splinter
skills
such
as
the
memorization
of
trivia
to
the
extraordinarily
rare
talents
of
prodigious
autistic
savants.
Many
individuals
with
ASD
show
superior
skills
in
perception
and
attention,
relative
to
the
general
population.
Sensory
abnormalities
are
found
in
over
90%
of
those
with
autism,
and
are
considered
core
features
by
some,
although
there
is
no
good
evidence
that
sensory
symptoms
differentiate
autism
from
other
developmental
disorders.
Differences
are
greater
for
under-responsivity
(for
example,
walking
into
things)
than
for
over-responsivity
(for
example,
distress
from
loud
noises)
or
for
sensation
seeking
(for
example,
rhythmic
movements).
An
estimated
60%–80%
of
autistic
people
have
motor
signs
that
include
poor
muscle
tone,
poor
motor
planning,
and
toe
walking;;
deficits
in
motor
coordination
are
pervasive
across
ASD
and
are
greater
in
autism
proper.
Unusual
eating
behavior
occurs
in
about
three-quarters
of
children
with
ASD,
to
the
extent
that
it
was
formerly
a
diagnostic
indicator.
Selectivity
is
the
most
common
problem,
although
eating
rituals
and
food
refusal
also
occur;
this
does
not
appear
to
result
in
malnutrition.
Although
some
children
with
autism
also
have
gastrointestinal
(GI)
symptoms,
there
is
a
lack
of
published
rigorous
data
to
support
the
theory
that
autistic
children
have
more
or
different
GI
symptoms
than
usual;
studies
report
conflicting
results,
and
the
relationship
between
GI
problems
and
ASD
is
unclear.
Parents
of
children
with
ASD
have
higher
levels
of
stress.
Siblings
of
children
with
ASD
report
greater
admiration
of
and
less
conflict
with
the
affected
sibling
than
siblings
of
unaffected
children
or
those
with
Down
syndrome;
siblings
of
individuals
with
ASD
have
greater
risk
of
negative
well-being
and
poorer
sibling
relationships
as
adults.
Classification.
Autism
is
one
of
the
five
pervasive
developmental
disorders
(PDD),
which
are
characterized
by
widespread
abnormalities
of
social
interactions
and
communication,
and
severely
restricted
interests
and
highly
repetitive
behavior.
These
symptoms
do
not
imply
sickness,
fragility,
or
emotional
disturbance.
Of
the
five
PDD
forms,
Asperger
syndrome
is
closest
to
autism
in
signs
and
likely
causes;
Rett
syndrome
and
childhood
disintegrative
disorder
share
several
signs
with
autism,
but
may
have
unrelated
causes;
PDD
not
otherwise
specified
(PDD-NOS;
also
called
"atypical
autism")
is
diagnosed
when
the
criteria
are
not
met
for
a
more
specific
disorder.
Unlike
with
autism,
people
with
Asperger
syndrome
have
no
substantial
delay
in
language
development.
The
terminology
of
autism
can
be
bewildering,
with
autism,
Asperger
syndrome
and
PDD-NOS
often
called
the
"autism
spectrum
disorders"
(ASD)
or
sometimes
the
"autistic
disorders",
whereas
autism
itself
is
often
called
"autistic
disorder",
"childhood
autism",
or
"infantile
autism".
In
this
article,
"autism"
refers
to
the
classic
autistic
disorder;
in
clinical
practice,
though,
"autism",
"ASD",
and
"PDD"
are
often
used
interchangeably.
ASD,
in
turn,
is
a
subset
of
the
broader
autism
phenotype,
which
describes
individuals
who
may
not
have
ASD
but
do
have
autistic-like
traits,
such
as
avoiding
eye
contact.
The
manifestations
of
autism
cover
a
wide
spectrum,
ranging
from
individuals
with
severe
impairments—who
may
be
silent,
mentally
disabled,
and
locked
into
hand
flapping
and
rocking—to
high
functioning
individuals
who
may
have
active
but
distinctly
odd
social
approaches,
narrowly
focused
interests,
and
verbose,
pedantic
communication.
Because
the
behavior
spectrum
is
continuous,
boundaries
between
diagnostic
categories
are
necessarily
somewhat
arbitrary.
Sometimes
the
syndrome
is
divided
into
low-,
medium-
or
high-functioning
autism
(LFA,
MFA,
and
HFA),
based
on
IQ
thresholds,
or
on
how
much
support
the
individual
requires
in
daily
life;
these
subdivisions
are
not
standardized
and
are
controversial.
Autism
can
also
be
divided
into
syndromal
and
non-syndromal
autism;
the
syndromal
autism
is
associated
with
severe
or
profound
mental
retardation
or
a
congenital
syndrome
with
physical
symptoms,
such
as
tuberous
sclerosis.
Although
individuals
with
Asperger
syndrome
tend
to
perform
better
cognitively
than
those
with
autism,
the
extent
of
the
overlap
between
Asperger
syndrome,
HFA,
and
non-syndromal
autism
is
unclear.
Some
studies
have
reported
diagnoses
of
autism
in
children
due
to
a
loss
of
language
or
social
skills,
as
opposed
to
a
failure
to
make
progress,
typically
from
15
to
30
months
of
age.
The
validity
of
this
distinction
remains
controversial;
it
is
possible
that
regressive
autism
is
a
specific
subtype,
or
that
there
is
a
continuum
of
behaviors
between
autism
with
and
without
regression.
Research
into
causes
has
been
hampered
by
the
inability
to
identify
biologically
meaningful
subpopulations
and
by
the
traditional
boundaries
between
the
disciplines
of
psychiatry,
psychology,
neurology
and
pediatrics.
Newer
technologies
such
as
fMRI
and
diffusion
tensor
imaging
can
help
identify
biologically
relevant
phenotypes
(observable
traits)
that
can
be
viewed
on
brain
scans,
to
help
further
neurogenetic
studies
of
autism;
one
example
is
lowered
activity
in
the
fusiform
face
area
of
the
brain,
which
is
associated
with
impaired
perception
of
people
versus
objects.
It
has
been
proposed
to
classify
autism
using
genetics
as
well
as
behavior.
Causes.
It
has
long
been
presumed
that
there
is
a
common
cause
at
the
genetic,
cognitive,
and
neural
levels
for
autism's
characteristic
triad
of
symptoms.
However,
there
is
increasing
suspicion
that
autism
is
instead
a
complex
disorder
whose
core
aspects
have
distinct
causes
that
often
co-occur.
Autism
has
a
strong
genetic
basis,
although
the
genetics
of
autism
are
complex
and
it
is
unclear
whether
ASD
is
explained
more
by
rare
mutations
with
major
effects,
or
by
rare
multigene
interactions
of
common
genetic
variants.
Complexity
arises
due
to
interactions
among
multiple
genes,
the
environment,
and
epigenetic
factors
which
do
not
change
DNA
but
are
heritable
and
influence
gene
expression.
Studies
of
twins
suggest
that
heritability
is
0.7
for
autism
and
as
high
as
0.9
for
ASD,
and
siblings
of
those
with
autism
are
about
25
times
more
likely
to
be
autistic
than
the
general
population.
However,
most
of
the
mutations
that
increase
autism
risk
have
not
been
identified.
Typically,
autism
cannot
be
traced
to
a
Mendelian
(single-gene)
mutation
or
to
a
single
chromosome
abnormality
like
fragile
X
syndrome,
and
none
of
the
genetic
syndromes
associated
with
ASDs
has
been
shown
to
selectively
cause
ASD.
Numerous
candidate
genes
have
been
located,
with
only
small
effects
attributable
to
any
particular
gene.
The
large
number
of
autistic
individuals
with
unaffected
family
members
may
result
from
copy
number
variations—spontaneous
deletions
or
duplications
in
genetic
material
during
meiosis.
Hence,
a
substantial
fraction
of
autism
cases
may
be
traceable
to
genetic
causes
that
are
highly
heritable
but
not
inherited:
that
is,
the
mutation
that
causes
the
autism
is
not
present
in
the
parental
genome.
Several
lines
of
evidence
point
to
synaptic
dysfunction
as
a
cause
of
autism.
Some
rare
mutations
may
lead
to
autism
by
disrupting
some
synaptic
pathways,
such
as
those
involved
with
cell
adhesion.
Gene
replacement
studies
in
mice
suggest
that
autistic
symptoms
are
closely
related
to
later
developmental
steps
that
depend
on
activity
in
synapses
and
on
activity-dependent
changes.
All
known
teratogens
(agents
that
cause
birth
defects)
related
to
the
risk
of
autism
appear
to
act
during
the
first
eight
weeks
from
conception,
and
though
this
does
not
exclude
the
possibility
that
autism
can
be
initiated
or
affected
later,
it
is
strong
evidence
that
autism
arises
very
early
in
development.
Although
evidence
for
other
environmental
causes
is
anecdotal
and
has
not
been
confirmed
by
reliable
studies,
extensive
searches
are
underway.
Environmental
factors
that
have
been
claimed
to
contribute
to
or
exacerbate
autism,
or
may
be
important
in
future
research,
include
certain
foods,
infectious
disease,
heavy
metals,
solvents,
diesel
exhaust,
PCBs,
phthalates
and
phenols
used
in
plastic
products,
pesticides,
brominated
flame
retardants,
alcohol,
smoking,
illicit
drugs,
vaccines,
and
prenatal
stress.
Parents
may
first
become
aware
of
autistic
symptoms
in
their
child
around
the
time
of
a
routine
vaccination,
and
this
has
given
rise
to
theories
that
vaccines
or
their
preservatives
cause
autism.
Although
these
theories
lack
convincing
scientific
evidence
and
are
biologically
implausible,
parental
concern
about
autism
has
led
to
lower
rates
of
childhood
immunizations
and
higher
likelihood
of
measles
outbreaks.
Mechanism.
Autism's
symptoms
result
from
maturation-related
changes
in
various
systems
of
the
brain.
How
autism
occurs
is
not
well
understood.
Its
mechanism
can
be
divided
into
two
areas:
the
pathophysiology
of
brain
structures
and
processes
associated
with
autism,
and
the
neuropsychological
linkages
between
brain
structures
and
behaviors.
The
behaviors
appear
to
have
multiple
pathophysiologies.
Pathophysiology.
Interactions
between
the
immune
system
and
the
nervous
system
begin
early
during
the
embryonic
stage
of
life,
and
successful
neurodevelopment
depends
on
a
balanced
immune
response.
It
is
possible
that
aberrant
immune
activity
during
critical
periods
of
neurodevelopment
is
part
of
the
mechanism
of
some
forms
of
ASD.
Although
some
abnormalities
in
the
immune
system
have
been
found
in
specific
subgroups
of
autistic
individuals,
it
is
not
known
whether
these
abnormalities
are
relevant
to
or
secondary
to
autism's
disease
processes.
As
autoantibodies
are
found
in
conditions
other
than
ASD,
and
are
not
always
present
in
ASD,
the
relationship
between
immune
disturbances
and
autism
remains
unclear
and
controversial.
The
relationship
of
neurochemicals
to
autism
is
not
well
understood;
several
have
been
investigated,
with
the
most
evidence
for
the
role
of
serotonin
and
of
genetic
differences
in
its
transport.
Some
data
suggest
an
increase
in
several
growth
hormones;
other
data
argue
for
diminished
growth
factors.
Also,
some
inborn
errors
of
metabolism
are
associated
with
autism
but
probably
account
for
less
than
5%
of
cases.
The
mirror
neuron
system
(MNS)
theory
of
autism
hypothesizes
that
distortion
in
the
development
of
the
MNS
interferes
with
imitation
and
leads
to
autism's
core
features
of
social
impairment
and
communication
difficulties.
The
MNS
operates
when
an
animal
performs
an
action
or
observes
another
animal
perform
the
same
action.
The
MNS
may
contribute
to
an
individual's
understanding
of
other
people
by
enabling
the
modeling
of
their
behavior
via
embodied
simulation
of
their
actions,
intentions,
and
emotions.
Several
studies
have
tested
this
hypothesis
by
demonstrating
structural
abnormalities
in
MNS
regions
of
individuals
with
ASD,
delay
in
the
activation
in
the
core
circuit
for
imitation
in
individuals
with
Asperger
syndrome,
and
a
correlation
between
reduced
MNS
activity
and
severity
of
the
syndrome
in
children
with
ASD.
However,
individuals
with
autism
also
have
abnormal
brain
activation
in
many
circuits
outside
the
MNS
and
the
MNS
theory
does
not
explain
the
normal
performance
of
autistic
children
on
imitation
tasks
that
involve
a
goal
or
object.
ASD-related
patterns
of
low
function
and
aberrant
activation
in
the
brain
differ
depending
on
whether
the
brain
is
doing
social
or
nonsocial
tasks.
In
autism
there
is
evidence
for
reduced
functional
connectivity
of
the
default
network,
a
large-scale
brain
network
involved
in
social
and
emotional
processing,
with
intact
connectivity
of
the
task-positive
network,
used
in
sustained
attention
and
goal-directed
thinking.
In
people
with
autism
the
two
networks
are
not
negatively
correlated
in
time,
suggesting
an
imbalance
in
toggling
between
the
two
networks,
possibly
reflecting
a
disturbance
of
self-referential
thought.
A
2008
brain-imaging
study
found
a
specific
pattern
of
signals
in
the
cingulate
cortex
which
differs
in
individuals
with
ASD.
The
underconnectivity
theory
of
autism
hypothesizes
that
autism
is
marked
by
underfunctioning
high-level
neural
connections
and
synchronization,
along
with
an
excess
of
low-level
processes.
Evidence
for
this
theory
has
been
found
in
functional
neuroimaging
studies
on
autistic
individuals
and
by
a
brain
wave
study
that
suggested
that
adults
with
ASD
have
local
overconnectivity
in
the
cortex
and
weak
functional
connections
between
the
frontal
lobe
and
the
rest
of
the
cortex.
Other
evidence
suggests
the
underconnectivity
is
mainly
within
each
hemisphere
of
the
cortex
and
that
autism
is
a
disorder
of
the
association
cortex.
From
studies
based
on
event-related
potentials,
transient
changes
to
the
brain's
electrical
activity
in
response
to
stimuli,
there
is
considerable
evidence
for
differences
in
autistic
individuals
with
respect
to
attention,
orientiation
to
auditory
and
visual
stimuli,
novelty
detection,
language
and
face
processing,
and
information
storage;
several
studies
have
found
a
preference
for
non-social
stimuli.
For
example,
magnetoencephalography
studies
have
found
evidence
in
autistic
children
of
delayed
responses
in
the
brain's
processing
of
auditory
signals.
Neuropsychology.
Two
major
categories
of
cognitive
theories
have
been
proposed
about
the
links
between
autistic
brains
and
behavior.
The
first
category
focuses
on
deficits
in
social
cognition.
The
empathizing–systemizing
theory
postulates
that
autistic
individuals
can
systemize—that
is,
they
can
develop
internal
rules
of
operation
to
handle
events
inside
the
brain—but
are
less
effective
at
empathizing
by
handling
events
generated
by
other
agents.
An
extension,
the
extreme
male
brain
theory,
hypothesizes
that
autism
is
an
extreme
case
of
the
male
brain,
defined
psychometrically
as
individuals
in
whom
systemizing
is
better
than
empathizing;
this
extension
is
controversial,
as
many
studies
contradict
the
idea
that
baby
boys
and
girls
respond
differently
to
people
and
objects.
These
theories
are
somewhat
related
to
the
earlier
theory
of
mind
approach,
which
hypothesizes
that
autistic
behavior
arises
from
an
inability
to
ascribe
mental
states
to
oneself
and
others.
The
theory
of
mind
hypothesis
is
supported
by
autistic
children's
atypical
responses
to
the
Sally–Anne
test
for
reasoning
about
others'
motivations,
and
the
mirror
neuron
system
theory
of
autism
described
in
"Pathophysiology"
maps
well
to
the
hypothesis.
However,
most
studies
have
found
no
evidence
of
impairment
in
autistic
individuals'
ability
to
understand
other
people's
basic
intentions
or
goals;
instead,
data
suggests
that
impairments
are
found
in
understanding
more
complex
social
emotions
or
in
considering
others'
viewpoints.
The
second
category
focuses
on
nonsocial
or
general
processing.
Executive
dysfunction
hypothesizes
that
autistic
behavior
results
in
part
from
deficits
in
working
memory,
planning,
inhibition,
and
other
forms
of
executive
function.
Tests
of
core
executive
processes
such
as
eye
movement
tasks
indicate
improvement
from
late
childhood
to
adolescence,
but
performance
never
reaches
typical
adult
levels.
A
strength
of
the
theory
is
predicting
stereotyped
behavior
and
narrow
interests;
two
weaknesses
are
that
executive
function
is
hard
to
measure
and
that
executive
function
deficits
have
not
been
found
in
young
autistic
children.
Weak
central
coherence
theory
hypothesizes
that
a
limited
ability
to
see
the
big
picture
underlies
the
central
disturbance
in
autism.
One
strength
of
this
theory
is
predicting
special
talents
and
peaks
in
performance
in
autistic
people.
A
related
theory—enhanced
perceptual
functioning—focuses
more
on
the
superiority
of
locally
oriented
and
perceptual
operations
in
autistic
individuals.
These
theories
map
well
from
the
underconnectivity
theory
of
autism.
Neither
category
is
satisfactory
on
its
own;
social
cognition
theories
poorly
address
autism's
rigid
and
repetitive
behaviors,
while
the
nonsocial
theories
have
difficulty
explaining
social
impairment
and
communication
difficulties.
A
combined
theory
based
on
multiple
deficits
may
prove
to
be
more
useful.
Screening.
U.S.
and
Japanese
practice
is
to
screen
all
children
for
ASD
at
18
and
24
months,
using
autism-specific
formal
screening
tests.
In
contrast,
in
the
UK,
screening
targets
children
whose
families
or
doctors
recognize
possible
signs
of
autism.
It
is
not
known
which
approach
is
more
effective.
Screening
tools
include
the
Modified
Checklist
for
Autism
in
Toddlers
(M-CHAT),
the
Early
Screening
of
Autistic
Traits
Questionnaire,
and
the
First
Year
Inventory;
initial
data
on
M-CHAT
and
its
predecessor
CHAT
on
children
aged
18–30
months
suggests
that
it
is
best
used
in
a
clinical
setting
and
that
it
has
low
sensitivity
(many
false-negatives)
but
good
specificity
(few
false-positives).
It
may
be
more
accurate
to
precede
these
tests
with
a
broadband
screener
that
does
not
distinguish
ASD
from
other
developmental
disorders.
Screening
tools
designed
for
one
culture's
norms
for
behaviors
like
eye
contact
may
be
inappropriate
for
a
different
culture.
Although
genetic
screening
for
autism
is
generally
still
impractical,
it
can
be
considered
in
some
cases,
such
as
children
with
neurological
symptoms
and
dysmorphic
features.
Diagnosis.
Diagnosis
is
based
on
behavior,
not
cause
or
mechanism.
Autism
is
defined
in
the
DSM-IV-TR
as
exhibiting
at
least
six
symptoms
total,
including
at
least
two
symptoms
of
qualitative
impairment
in
social
interaction,
at
least
one
symptom
of
qualitative
impairment
in
communication,
and
at
least
one
symptom
of
restricted
and
repetitive
behavior.
Sample
symptoms
include
lack
of
social
or
emotional
reciprocity,
stereotyped
and
repetitive
use
of
language
or
idiosyncratic
language,
and
persistent
preoccupation
with
parts
of
objects.
Onset
must
be
prior
to
age
three
years,
with
delays
or
abnormal
functioning
in
either
social
interaction,
language
as
used
in
social
communication,
or
symbolic
or
imaginative
play.
The
disturbance
must
not
be
better
accounted
for
by
Rett
syndrome
or
childhood
disintegrative
disorder.
ICD-10
uses
essentially
the
same
definition.
Several
diagnostic
instruments
are
available.
Two
are
commonly
used
in
autism
research:
the
Autism
Diagnostic
Interview-Revised
(ADI-R)
is
a
semistructured
parent
interview,
and
the
Autism
Diagnostic
Observation
Schedule
(ADOS)
uses
observation
and
interaction
with
the
child.
The
Childhood
Autism
Rating
Scale
(CARS)
is
used
widely
in
clinical
environments
to
assess
severity
of
autism
based
on
observation
of
children.
A
pediatrician
commonly
performs
a
preliminary
investigation
by
taking
developmental
history
and
physically
examining
the
child.
If
warranted,
diagnosis
and
evaluations
are
conducted
with
help
from
ASD
specialists,
observing
and
assessing
cognitive,
communication,
family,
and
other
factors
using
standardized
tools,
and
taking
into
account
any
associated
medical
conditions.
A
pediatric
neuropsychologist
is
often
asked
to
assess
behavior
and
cognitive
skills,
both
to
aid
diagnosis
and
to
help
recommend
educational
interventions.
A
differential
diagnosis
for
ASD
at
this
stage
might
also
consider
mental
retardation,
hearing
impairment,
and
a
specific
language
impairment
such
as
Landau–Kleffner
syndrome.
The
presence
of
autism
can
make
it
harder
to
diagnose
coexisting
psychiatric
disorders
such
as
depression.
Clinical
genetics
evaluations
are
often
done
once
ASD
is
diagnosed,
particularly
when
other
symptoms
already
suggest
a
genetic
cause.
Although
genetic
technology
allows
clinical
geneticists
to
link
an
estimated
40%
of
cases
to
genetic
causes,
consensus
guidelines
in
the
U.S.
and
UK
are
limited
to
high-resolution
chromosome
and
fragile
X
testing.
A
genotype-first
model
of
diagnosis
has
been
proposed,
which
would
routinely
assess
the
genome's
copy
number
variations.
As
new
genetic
tests
are
developed
several
ethical,
legal,
and
social
issues
will
emerge.
Commercial
availability
of
tests
may
precede
adequate
understanding
of
how
to
use
test
results,
given
the
complexity
of
autism's
genetics.
Metabolic
and
neuroimaging
tests
are
sometimes
helpful,
but
are
not
routine.
ASD
can
sometimes
be
diagnosed
by
age
14
months,
although
diagnosis
becomes
increasingly
stable
over
the
first
three
years
of
life:
for
example,
a
one-year-old
who
meets
diagnostic
criteria
for
ASD
is
less
likely
than
a
three-year-old
to
continue
to
do
so
a
few
years
later.
In
the
UK
the
National
Autism
Plan
for
Children
recommends
at
most
30
weeks
from
first
concern
to
completed
diagnosis
and
assessment,
though
few
cases
are
handled
that
quickly
in
practice.
A
2009
U.S.
study
found
the
average
age
of
formal
ASD
diagnosis
was
5.7
years,
far
above
recommendations,
and
that
27%
of
children
remained
undiagnosed
at
age
8
years.
Although
the
symptoms
of
autism
and
ASD
begin
early
in
childhood,
they
are
sometimes
missed;
years
later,
adults
may
seek
diagnoses
to
help
them
or
their
friends
and
family
understand
themselves,
to
help
their
employers
make
adjustments,
or
in
some
locations
to
claim
disability
living
allowances
or
other
benefits.
Underdiagnosis
and
overdiagnosis
are
problems
in
marginal
cases,
and
much
of
the
recent
increase
in
the
number
of
reported
ASD
cases
is
likely
due
to
changes
in
diagnostic
practices.
The
increasing
popularity
of
drug
treatment
options
and
the
expansion
of
benefits
has
given
providers
incentives
to
diagnose
ASD,
resulting
in
some
overdiagnosis
of
children
with
uncertain
symptoms.
Conversely,
the
cost
of
screening
and
diagnosis
and
the
challenge
of
obtaining
payment
can
inhibit
or
delay
diagnosis.
It
is
particularly
hard
to
diagnose
autism
among
the
visually
impaired,
partly
because
some
of
its
diagnostic
criteria
depend
on
vision,
and
partly
because
autistic
symptoms
overlap
with
those
of
common
blindness
syndromes.
Management.
The
main
goals
of
treatment
are
to
lessen
associated
deficits
and
family
distress,
and
to
increase
quality
of
life
and
functional
independence.
No
single
treatment
is
best
and
treatment
is
typically
tailored
to
the
child's
needs.
Families
and
the
educational
system
are
the
main
resources
for
treatment.
Studies
of
interventions
have
methodological
problems
that
prevent
definitive
conclusions
about
efficacy.
Although
many
psychosocial
interventions
have
some
positive
evidence,
suggesting
that
some
form
of
treatment
is
preferable
to
no
treatment,
the
methodological
quality
of
systematic
reviews
of
these
studies
has
generally
been
poor,
their
clinical
results
are
mostly
tentative,
and
there
is
little
evidence
for
the
relative
effectiveness
of
treatment
options.
Intensive,
sustained
special
education
programs
and
behavior
therapy
early
in
life
can
help
children
acquire
self-care,
social,
and
job
skills,
and
often
improve
functioning
and
decrease
symptom
severity
and
maladaptive
behaviors;
claims
that
intervention
by
around
age
three
years
is
crucial
are
not
substantiated.
Available
approaches
include
applied
behavior
analysis
(ABA),
developmental
models,
structured
teaching,
speech
and
language
therapy,
social
skills
therapy,
and
occupational
therapy.
Educational
interventions
have
some
effectiveness
in
children:
intensive
ABA
treatment
has
demonstrated
effectiveness
in
enhancing
global
functioning
in
preschool
children
and
is
well-established
for
improving
intellectual
performance
of
young
children.
Neuropsychological
reports
are
often
poorly
communicated
to
educators,
resulting
in
a
gap
between
what
a
report
recommends
and
what
education
is
provided.
It
is
not
known
whether
treatment
programs
for
children
lead
to
significant
improvements
after
the
children
grow
up,
and
the
limited
research
on
the
effectiveness
of
adult
residential
programs
shows
mixed
results.
Many
medications
are
used
to
treat
ASD
symptoms
that
interfere
with
integrating
a
child
into
home
or
school
when
behavioral
treatment
fails.
More
than
half
of
U.S.
children
diagnosed
with
ASD
are
prescribed
psychoactive
drugs
or
anticonvulsants,
with
the
most
common
drug
classes
being
antidepressants,
stimulants,
and
antipsychotics.
Aside
from
antipsychotics,
there
is
scant
reliable
research
about
the
effectiveness
or
safety
of
drug
treatments
for
adolescents
and
adults
with
ASD.
A
person
with
ASD
may
respond
atypically
to
medications,
the
medications
can
have
adverse
effects,
and
no
known
medication
relieves
autism's
core
symptoms
of
social
and
communication
impairments.
Experiments
in
mice
have
reversed
or
reduced
some
symptoms
related
to
autism
by
replacing
or
modulating
gene
function
after
birth,
suggesting
the
possibility
of
targeting
therapies
to
specific
rare
mutations
known
to
cause
autism.
Although
many
alternative
therapies
and
interventions
are
available,
few
are
supported
by
scientific
studies.
Treatment
approaches
have
little
empirical
support
in
quality-of-life
contexts,
and
many
programs
focus
on
success
measures
that
lack
predictive
validity
and
real-world
relevance.
Scientific
evidence
appears
to
matter
less
to
service
providers
than
program
marketing,
training
availability,
and
parent
requests.
Though
most
alternative
treatments,
such
as
melatonin,
have
only
mild
adverse
effects
some
may
place
the
child
at
risk.
A
2008
study
found
that
compared
to
their
peers,
autistic
boys
have
significantly
thinner
bones
if
on
casein-free
diets;
in
2005,
botched
chelation
therapy
killed
a
five-year-old
child
with
autism.
Treatment
is
expensive;
indirect
costs
are
more
so.
For
someone
born
in
2000,
a
U.S.
study
estimated
an
average
lifetime
cost
of
$
(net
present
value
in
dollars,
inflation-adjusted
from
2003
estimate),
with
about
10%
medical
care,
30%
extra
education
and
other
care,
and
60%
lost
economic
productivity.
Publicly
supported
programs
are
often
inadequate
or
inappropriate
for
a
given
child,
and
unreimbursed
out-of-pocket
medical
or
therapy
expenses
are
associated
with
likelihood
of
family
financial
problems;
one
2008
U.S.
study
found
a
14%
average
loss
of
annual
income
in
families
of
children
with
ASD,
and
a
related
study
found
that
ASD
is
associated
with
higher
probability
that
child
care
problems
will
greatly
affect
parental
employment.
U.S.
states
increasingly
require
private
health
insurance
to
cover
autism
services,
shifting
costs
from
publicly
funded
education
programs
to
privately
funded
health
insurance.
After
childhood,
key
treatment
issues
include
residential
care,
job
training
and
placement,
sexuality,
social
skills,
and
estate
planning.
Prognosis.
No
cure
is
known.
Children
recover
occasionally,
so
that
they
lose
their
diagnosis
of
ASD;
this
occurs
sometimes
after
intensive
treatment
and
sometimes
not.
It
is
not
known
how
often
recovery
happens;
reported
rates
in
unselected
samples
of
children
with
ASD
have
ranged
from
3%
to
25%.
A
few
autistic
children
have
acquired
speech
at
age
5
or
older.
Most
children
with
autism
lack
social
support,
meaningful
relationships,
future
employment
opportunities
or
self-determination.
Although
core
difficulties
tend
to
persist,
symptoms
often
become
less
severe
with
age.
Few
high-quality
studies
address
long-term
prognosis.
Some
adults
show
modest
improvement
in
communication
skills,
but
a
few
decline;
no
study
has
focused
on
autism
after
midlife.
Acquiring
language
before
age
six,
having
an
IQ
above
50,
and
having
a
marketable
skill
all
predict
better
outcomes;
independent
living
is
unlikely
with
severe
autism.
A
2004
British
study
of
68
adults
who
were
diagnosed
before
1980
as
autistic
children
with
IQ
above
50
found
that
12%
achieved
a
high
level
of
independence
as
adults,
10%
had
some
friends
and
were
generally
in
work
but
required
some
support,
19%
had
some
independence
but
were
generally
living
at
home
and
needed
considerable
support
and
supervision
in
daily
living,
46%
needed
specialist
residential
provision
from
facilities
specializing
in
ASD
with
a
high
level
of
support
and
very
limited
autonomy,
and
12%
needed
high-level
hospital
care.
A
2005
Swedish
study
of
78
adults
that
did
not
exclude
low
IQ
found
worse
prognosis;
for
example,
only
4%
achieved
independence.
A
2008
Canadian
study
of
48
young
adults
diagnosed
with
ASD
as
preschoolers
found
outcomes
ranging
through
poor
(46%),
fair
(32%),
good
(17%),
and
very
good
(4%);
56%
of
these
young
adults
had
been
employed
at
some
point
during
their
lives,
mostly
in
volunteer,
sheltered
or
part-time
work.
Changes
in
diagnostic
practice
and
increased
availability
of
effective
early
intervention
make
it
unclear
whether
these
findings
can
be
generalized
to
recently
diagnosed
children.
Epidemiology.
Most
recent
reviews
tend
to
estimate
a
prevalence
of
1–2
per
1,000
for
autism
and
close
to
6
per
1,000
for
ASD;
because
of
inadequate
data,
these
numbers
may
underestimate
ASD's
true
prevalence.
PDD-NOS's
prevalence
has
been
estimated
at
3.7
per
1,000,
Asperger
syndrome
at
roughly
0.6
per
1,000,
and
childhood
disintegrative
disorder
at
0.02
per
1,000.
The
number
of
reported
cases
of
autism
increased
dramatically
in
the
1990s
and
early
2000s.
This
increase
is
largely
attributable
to
changes
in
diagnostic
practices,
referral
patterns,
availability
of
services,
age
at
diagnosis,
and
public
awareness,
though
unidentified
environmental
risk
factors
cannot
be
ruled
out.
The
available
evidence
does
not
rule
out
the
possibility
that
autism's
true
prevalence
has
increased;
a
real
increase
would
suggest
directing
more
attention
and
funding
toward
changing
environmental
factors
instead
of
continuing
to
focus
on
genetics.
Boys
are
at
higher
risk
for
ASD
than
girls.
The
sex
ratio
averages
4.3:1
and
is
greatly
modified
by
cognitive
impairment:
it
may
be
close
to
2:1
with
mental
retardation
and
more
than
5.5:1
without.
Although
the
evidence
does
not
implicate
any
single
pregnancy-related
risk
factor
as
a
cause
of
autism,
the
risk
of
autism
is
associated
with
advanced
age
in
either
parent,
and
with
diabetes,
bleeding,
and
use
of
psychiatric
drugs
in
the
mother
during
pregnancy.
The
risk
is
greater
with
older
fathers
than
with
older
mothers;
two
potential
explanations
are
the
known
increase
in
mutation
burden
in
older
sperm,
and
the
hypothesis
that
men
marry
later
if
they
carry
genetic
liability
and
show
some
signs
of
autism.
Most
professionals
believe
that
race,
ethnicity,
and
socioeconomic
background
do
not
affect
the
occurrence
of
autism.
History.
A
few
examples
of
autistic
symptoms
and
treatments
were
described
long
before
autism
was
named.
The
"Table
Talk"
of
Martin
Luther
contains
the
story
of
a
12-year-old
boy
who
may
have
been
severely
autistic.
According
to
Luther's
notetaker
Mathesius,
Luther
thought
the
boy
was
a
soulless
mass
of
flesh
possessed
by
the
devil,
and
suggested
that
he
be
suffocated.
The
earliest
well-documented
case
of
autism
is
that
of
Hugh
Blair
of
Borgue,
as
detailed
in
a
1747
court
case
in
which
his
brother
successfully
petitioned
to
annul
Blair's
marriage
to
gain
Blair's
inheritance.
The
Wild
Boy
of
Aveyron,
a
feral
child
caught
in
1798,
showed
several
signs
of
autism;
the
medical
student
Jean
Itard
treated
him
with
a
behavioral
program
designed
to
help
him
form
social
attachments
and
to
induce
speech
via
imitation.
The
New
Latin
word
"autismus"
(English
translation
"autism")
was
coined
by
the
Swiss
psychiatrist
Eugen
Bleuler
in
1910
as
he
was
defining
symptoms
of
schizophrenia.
He
derived
it
from
the
Greek
word
"autós"
(αὐτός,
meaning
"self"),
and
used
it
to
mean
morbid
self-admiration,
referring
to
"autistic
withdrawal
of
the
patient
to
his
fantasies,
against
which
any
influence
from
outside
becomes
an
intolerable
disturbance".
The
word
"autism"
first
took
its
modern
sense
in
1938
when
Hans
Asperger
of
the
Vienna
University
Hospital
adopted
Bleuler's
terminology
"autistic
psychopaths"
in
a
lecture
in
German
about
child
psychology.
Asperger
was
investigating
an
ASD
now
known
as
Asperger
syndrome,
though
for
various
reasons
it
was
not
widely
recognized
as
a
separate
diagnosis
until
1981.
Leo
Kanner
of
the
Johns
Hopkins
Hospital
first
used
"autism"
in
its
modern
sense
in
English
when
he
introduced
the
label
"early
infantile
autism"
in
a
1943
report
of
11
children
with
striking
behavioral
similarities.
Almost
all
the
characteristics
described
in
Kanner's
first
paper
on
the
subject,
notably
"autistic
aloneness"
and
"insistence
on
sameness",
are
still
regarded
as
typical
of
the
autistic
spectrum
of
disorders.
It
is
not
known
whether
Kanner
derived
the
term
independently
of
Asperger.
Kanner's
reuse
of
"autism"
led
to
decades
of
confused
terminology
like
"infantile
schizophrenia",
and
child
psychiatry's
focus
on
maternal
deprivation
led
to
misconceptions
of
autism
as
an
infant's
response
to
"refrigerator
mothers".
Starting
in
the
late
1960s
autism
was
established
as
a
separate
syndrome
by
demonstrating
that
it
is
lifelong,
distinguishing
it
from
mental
retardation
and
schizophrenia
and
from
other
developmental
disorders,
and
demonstrating
the
benefits
of
involving
parents
in
active
programs
of
therapy.
As
late
as
the
mid-1970s
there
was
little
evidence
of
a
genetic
role
in
autism;
now
it
is
thought
to
be
one
of
the
most
heritable
of
all
psychiatric
conditions.
Although
the
rise
of
parent
organizations
and
the
destigmatization
of
childhood
ASD
have
deeply
affected
how
we
view
ASD,
parents
continue
to
feel
social
stigma
in
situations
where
their
autistic
children's
behaviors
are
perceived
negatively
by
others,
and
many
primary
care
physicians
and
medical
specialists
still
express
some
beliefs
consistent
with
outdated
autism
research.
The
Internet
has
helped
autistic
individuals
bypass
nonverbal
cues
and
emotional
sharing
that
they
find
so
hard
to
deal
with,
and
has
given
them
a
way
to
form
online
communities
and
work
remotely.
Sociological
and
cultural
aspects
of
autism
have
developed:
some
in
the
community
seek
a
cure,
while
others
believe
that
autism
is
simply
another
way
of
being.
---END.OF.DOCUMENT---
Albedo.
The
albedo
of
an
object
is
a
measure
of
how
strongly
it
reflects
light
from
light
sources
such
as
the
Sun.
It
is
therefore
a
more
specific
form
of
the
term
reflectivity.
Albedo
is
defined
as
the
ratio
of
total-reflected
to
incident
electromagnetic
radiation.
It
is
a
unitless
measure
indicative
of
a
surface's
or
body's
diffuse
reflectivity.
The
word
is
derived
from
Latin
"albedo"
"whiteness",
in
turn
from
"albus"
"white",
and
was
introduced
into
optics
by
Johann
Heinrich
Lambert
in
his
1760
work
"Photometria".
The
range
of
possible
values
is
from
0
(dark)
to
1
(bright).
The
albedo
is
an
important
concept
in
climatology
and
astronomy,
as
well
as
in
computer
graphics
and
computer
vision.
In
climatology
it
is
sometimes
expressed
as
a
percentage.
Its
value
depends
on
the
frequency
of
radiation
considered:
unqualified,
it
usually
refers
to
some
appropriate
average
across
the
spectrum
of
visible
light.
In
general,
the
albedo
depends
on
the
direction
and
directional
distribution
of
incoming
radiation.
Exceptions
are
Lambertian
surfaces,
which
scatter
radiation
in
all
directions
in
a
cosine
function,
so
their
albedo
does
not
depend
on
the
incoming
distribution.
In
realistic
cases,
a
bidirectional
reflectance
distribution
function
(BRDF)
is
required
to
characterize
the
scattering
properties
of
a
surface
accurately,
although
albedos
are
a
very
useful
first
approximation.
Terrestrial
albedo.
Albedos
of
typical
materials
in
visible
light
range
from
up
to
0.9
for
fresh
snow,
to
about
0.04
for
charcoal,
one
of
the
darkest
substances.
Deeply
shadowed
cavities
can
achieve
an
effective
albedo
approaching
the
zero
of
a
blackbody.
When
seen
from
a
distance,
the
ocean
surface
has
a
low
albedo,
as
do
most
forests,
while
desert
areas
have
some
of
the
highest
albedos
among
landforms.
Most
land
areas
are
in
an
albedo
range
of
0.1
to
0.4.
The
average
albedo
of
the
Earth
is
about
0.3.
This
is
far
higher
than
for
the
ocean
primarily
because
of
the
contribution
of
clouds.
Human
activities
have
changed
the
albedo
(via
forest
clearance
and
farming,
for
example)
of
various
areas
around
the
globe.
However,
quantification
of
this
effect
on
the
global
scale
is
difficult.
The
classic
example
of
albedo
effect
is
the
snow-temperature
feedback.
If
a
snow-covered
area
warms
and
the
snow
melts,
the
albedo
decreases,
more
sunlight
is
absorbed,
and
the
temperature
tends
to
increase.
The
converse
is
true:
if
snow
forms,
a
cooling
cycle
happens.
The
intensity
of
the
albedo
effect
depends
on
the
size
of
the
change
in
albedo
and
the
amount
of
insolation;
for
this
reason
it
can
be
potentially
very
large
in
the
tropics.
The
Earth's
surface
albedo
is
regularly
estimated
via
Earth
observation
satellite
sensors
such
as
NASA's
MODIS
instruments
onboard
the
Terra
and
Aqua
satellites.
As
the
total
amount
of
reflected
radiation
cannot
be
directly
measured
by
satellite,
a
mathematical
model
of
the
BRDF
is
used
to
translate
a
sample
set
of
satellite
reflectance
measurements
into
estimates
of
directional-hemispherical
reflectance
and
bi-hemispherical
reflectance.
(e. g.,
The
Earth's
average
surface
temperature
due
to
its
albedo
and
the
greenhouse
effect
is
currently
about
15°C.
For
the
frozen
(more
reflective)
planet
the
average
temperature
is
below
-40°C
(If
only
all
continents
being
completely
covered
by
glaciers
-
the
mean
temperature
is
about
0°C).
The
simulation
for
(more
absorptive)
aquaplanet
shows
the
average
temperature
close
to
27°C.
White-sky
and
black-sky
albedo.
It
has
been
shown
that
for
many
applications
involving
terrestrial
albedo,
the
albedo
at
a
particular
solar
zenith
angle
formula_1
can
reasonably
be
approximated
by
the
proportionate
sum
of
two
terms:
the
directional-hemispherical
reflectance
at
that
solar
zenith
angle,
formula_2,
and
the
bi-hemispherical
reflectance,
formula_3
the
proportion
concerned
being
defined
as
the
proportion
of
diffuse
illumination
formula_4.
Directional-hemispherical
reflectance
is
sometimes
referred
to
as
black-sky
albedo
and
bi-hemispherical
reflectance
as
white
sky
albedo.
These
terms
are
important
because
they
allow
the
albedo
to
be
calculated
for
any
given
illumination
conditions
from
a
knowledge
of
the
intrinsic
properties
of
the
surface.
Astronomical
albedo.
The
albedos
of
planets,
satellites
and
asteroids
can
be
used
to
infer
much
about
their
properties.
The
study
of
albedos,
their
dependence
on
wavelength,
lighting
angle
("phase
angle"),
and
variation
in
time
comprises
a
major
part
of
the
astronomical
field
of
photometry.
For
small
and
far
objects
that
cannot
be
resolved
by
telescopes,
much
of
what
we
know
comes
from
the
study
of
their
albedos.
For
example,
the
absolute
albedo
can
indicate
the
surface
ice
content
of
outer
solar
system
objects,
the
variation
of
albedo
with
phase
angle
gives
information
about
regolith
properties,
while
unusually
high
radar
albedo
is
indicative
of
high
metallic
content
in
asteroids.
Enceladus,
a
moon
of
Saturn,
has
one
of
the
highest
known
albedos
of
any
body
in
the
Solar
system,
with
99%
of
EM
radiation
reflected.
Another
notable
high
albedo
body
is
Eris,
with
an
albedo
of
0.86.
Many
small
objects
in
the
outer
solar
system
and
asteroid
belt
have
low
albedos
down
to
about
0.05.
A
typical
comet
nucleus
has
an
albedo
of
0.04.
Such
a
dark
surface
is
thought
to
be
indicative
of
a
primitive
and
heavily
space
weathered
surface
containing
some
organic
compounds.
The
overall
albedo
of
the
Moon
is
around
0.072,
but
it
is
strongly
directional
and
non-Lambertian,
displaying
also
a
strong
opposition
effect.
While
such
reflectance
properties
are
different
from
those
of
any
terrestrial
terrains,
they
are
typical
of
the
regolith
surfaces
of
airless
solar
system
bodies.
Two
common
albedos
that
are
used
in
astronomy
are
the
(V-band)
geometric
albedo
(measuring
brightness
when
illumination
comes
from
directly
behind
the
observer)
and
the
Bond
albedo
(measuring
total
proportion
of
electromagnetic
energy
reflected).
Their
values
can
differ
significantly,
which
is
a
common
source
of
confusion.
In
detailed
studies,
the
directional
reflectance
properties
of
astronomical
bodies
are
often
expressed
in
terms
of
the
five
Hapke
parameters
which
semi-empirically
describe
the
variation
of
albedo
with
phase
angle,
including
a
characterization
of
the
opposition
effect
of
regolith
surfaces.
formula_7,
where
formula_8
is
the
astronomical
albedo,
formula_9
is
the
diameter
in
kilometres,
and
"H"
is
the
absolute
magnitude.
Other
types
of
albedo.
Single
scattering
albedo
is
used
to
define
scattering
of
electromagnetic
waves
on
small
particles.
It
depends
on
properties
of
the
material
(refractive
index);
the
size
of
the
particle
or
particles;
and
the
wavelength
of
the
incoming
radiation.
Albedo
also
refers
to
the
white,
spongy
inner
lining
of
a
citrus
fruit
rind.
According
to
Dr.
Renee
M.
Goodrich,
associate
professor
of
food
science
and
human
nutrition
at
the
University
of
Florida,
the
albedo
is
rich
in
the
soluble
fiber
pectin
and
contains
vitamin
C.
The
tropics.
Although
the
albedo-temperature
effect
is
most
famous
in
colder
regions
of
Earth,
because
more
snow
falls
there,
it
is
actually
much
stronger
in
tropical
regions
because
in
the
tropics
there
is
consistently
more
sunlight.
When
ranchers
cut
down
dark,
tropical
rainforest
trees
to
replace
them
with
even
darker
soil
in
order
to
grow
crops,
the
average
temperature
of
the
area
increases
up
to
3
°C
(5.4
°F)
year-round,
although
part
of
the
effect
is
due
to
changed
evaporation
(latent
heat
flux).
Small
scale
effects.
Albedo
works
on
a
smaller
scale,
too.
People
who
wear
dark
clothes
in
the
summertime
put
themselves
at
a
greater
risk
of
heatstroke
than
those
who
wear
lighter
color
clothes.
Trees.
Because
trees
tend
to
have
a
low
albedo,
removing
forests
would
tend
to
increase
albedo
and
thereby
could
produce
localized
climate
cooling.
Cloud
feedbacks
further
complicate
the
issue.
In
seasonally
snow-covered
zones,
winter
albedos
of
treeless
areas
are
10%
to
50%
higher
than
nearby
forested
areas
because
snow
does
not
cover
the
trees
as
readily.
Deciduous
trees
have
an
albedo
value
of
about
0.15
to
0.18
while
coniferous
trees
have
a
value
of
about
0.09
to
0.15.
The
difference
between
deciduous
and
coniferous
is
because
coniferous
trees
are
darker
in
general
and
have
cone-shaped
crowns.
The
shape
of
these
crowns
trap
radiant
energy
more
effectively
than
deciduous
trees.
Studies
by
the
Hadley
Centre
have
investigated
the
relative
(generally
warming)
effect
of
albedo
change
and
(cooling)
effect
of
carbon
sequestration
on
planting
forests.
They
found
that
new
forests
in
tropical
and
midlatitude
areas
tended
to
cool;
new
forests
in
high
latitudes
(e.g.
Siberia)
were
neutral
or
perhaps
warming.
Snow.
Snow
albedos
can
be
as
high
as
0.9;
this,
however,
is
for
the
ideal
example:
fresh
deep
snow
over
a
featureless
landscape.
Over
Antarctica
they
average
a
little
more
than
0.8.
If
a
marginally
snow-covered
area
warms,
snow
tends
to
melt,
lowering
the
albedo,
and
hence
leading
to
more
snowmelt
(the
ice-albedo
positive
feedback).
Water.
Water
reflects
light
very
differently
from
typical
terrestrial
materials.
The
reflectivity
of
a
water
surface
is
calculated
using
the
Fresnel
equations
(see
graph).
At
the
scale
of
the
wavelength
of
light
even
wavy
water
is
always
smooth
so
the
light
is
reflected
in
a
specular
manner
(not
diffusely).
The
glint
of
light
off
water
is
a
commonplace
effect
of
this.
At
small
angles
of
incident
light,
waviness
results
in
reduced
reflectivity
because
of
the
steepness
of
the
reflectivity-vs.-incident-angle
curve
and
a
locally
increased
average
incident
angle.
Although
the
reflectivity
of
water
is
very
low
at
low
and
medium
angles
of
incident
light,
it
increases
tremendously
at
high
angles
of
incident
light
such
as
occur
on
the
illuminated
side
of
the
Earth
near
the
terminator
(early
morning,
late
afternoon
and
near
the
poles).
However,
as
mentioned
above,
waviness
causes
an
appreciable
reduction.
Since
the
light
specularly
reflected
from
water
does
not
usually
reach
the
viewer,
water
is
usually
considered
to
have
a
very
low
albedo
in
spite
of
its
high
reflectivity
at
high
angles
of
incident
light.
Note
that
white
caps
on
waves
look
white
(and
have
high
albedo)
because
the
water
is
foamed
up
(not
smooth
at
the
scale
of
the
wavelength
of
light)
so
the
Fresnel
equations
do
not
apply.
Fresh
‘black’
ice
exhibits
Fresnel
reflection.
Clouds.
Clouds
are
another
source
of
albedo
that
play
into
the
global
warming
equation.
Different
types
of
clouds
have
different
albedo
values,
theoretically
ranging
from
a
minimum
of
near
0
to
a
maximum
approaching
0.8.
"On
any
given
day,
about
half
of
Earth
is
covered
by
clouds,
which
reflect
more
sunlight
than
land
and
water.
Clouds
keep
Earth
cool
by
reflecting
sunlight,
but
they
can
also
serve
as
blankets
to
trap
warmth."
Albedo
and
climate
in
some
areas
are
already
affected
by
artificial
clouds,
such
as
those
created
by
the
contrails
of
heavy
commercial
airliner
traffic.
A
study
following
the
burning
of
the
Kuwaiti
oil
fields
by
Saddam
Hussein
showed
that
temperatures
under
the
burning
oil
fires
were
as
much
as
10oC
colder
than
temperatures
several
miles
away
under
clear
skies.
Aerosol
effects.
Aerosol
(very
fine
particles/droplets
in
the
atmosphere)
has
two
effects,
direct
and
indirect.
The
direct
(albedo)
effect
is
generally
to
cool
the
planet;
the
indirect
effect
(the
particles
act
as
CCNs
and
thereby
change
cloud
properties)
is
less
certain.
Aerosols
can
modify
the
Earth’s
radiative
balance
through
the
aerosol
direct
and
indirect
Black
carbon.
Another
albedo-related
effect
on
the
climate
is
from
black
carbon
particles.
The
size
of
this
effect
is
difficult
to
quantify:
the
IPCC
say
that
their
"estimate
of
the
global
mean
radiative
forcing
for
BC
aerosols
from
fossil
fuels
is...
+0.2
W
m-2
(from
+0.1
W
m-2
in
the
SAR)
with
a
range
+0.1
to
+0.4
W
m...-2".
---END.OF.DOCUMENT---
A.
The
letter
A
is
the
first
letter
in
the
Latin
alphabet,
a
vowel.
Its
name
in
English
()
is
spelled
‹a›;
the
plural
is
"aes,"
although
this
is
rare.
Origins.
"A"
can
be
traced
to
a
pictogram
of
an
ox
head
in
Egyptian
hieroglyph
or
the
Proto-Sinaitic
alphabet.
In
1600
B.C.
the
Phoenician
alphabet's
letter
had
a
linear
form
that
served
as
the
base
for
some
later
forms.
Its
name
must
have
corresponded
closely
to
the
Hebrew
or
Arabic
aleph.
When
the
Ancient
Greeks
adopted
the
alphabet,
they
had
no
use
for
the
glottal
stop
that
the
letter
had
denoted
in
Phoenician
and
other
Semitic
languages,
so
they
used
the
sign
to
represent
the
vowel,
and
kept
its
name
with
a
minor
change
(alpha).
In
the
earliest
Greek
inscriptions
after
the
Greek
Dark
Ages,
dating
to
the
8th
century
BC,
the
letter
rests
upon
its
side,
but
in
the
Greek
alphabet
of
later
times
it
generally
resembles
the
modern
capital
letter,
although
many
local
varieties
can
be
distinguished
by
the
shortening
of
one
leg,
or
by
the
angle
at
which
the
cross
line
is
set.
The
Etruscans
brought
the
Greek
alphabet
to
their
civilization
in
the
Italian
Peninsula
and
left
the
letter
unchanged.
The
Romans
later
adopted
the
Etruscan
alphabet
to
write
the
Latin
language,
and
the
resulting
letter
was
preserved
in
the
modern
Latin
alphabet
used
to
write
many
languages,
including
English.
The
letter
has
two
minuscule
(lower-case)
forms.
The
form
used
in
most
current
handwriting
consists
of
a
circle
and
vertical
stoke
(),
called
Latin
alpha
or
"script
a".
Most
printed
material
uses
a
form
consisting
of
a
small
loop
with
an
arc
over
it
().
Both
derive
from
the
majuscule
(capital)
form.
In
Greek
handwriting,
it
was
common
to
join
the
left
leg
and
horizontal
stroke
into
a
single
loop,
as
demonstrated
by
the
Uncial
version
shown.
Many
fonts
then
made
the
right
leg
vertical.
In
some
of
these,
the
serif
that
began
the
right
leg
stroke
developed
into
an
arc,
resulting
in
the
printed
form,
while
in
others
it
was
dropped,
resulting
in
the
modern
handwritten
form.
Usage.
In
English,
"a"
by
itself
frequently
denotes
the
near-open
front
unrounded
vowel
()
as
in
"pad",
the
open
back
unrounded
vowel
()
as
in
"father",
or,
in
concert
with
a
later
orthographic
vowel,
the
diphthong
as
in
"ace"
and
"major",
due
to
effects
of
the
great
vowel
shift.
In
most
other
languages
that
use
the
Latin
alphabet,
"a"
denotes
an
open
central
unrounded
vowel
().
In
the
International
Phonetic
Alphabet,
variants
of
"a"
denote
various
vowels.
In
X-SAMPA,
capital
"A"
denotes
the
open
back
unrounded
vowel
and
lowercase
"a"
denotes
the
open
front
unrounded
vowel.
"A"
is
the
third
common
used
letter
in
English,
and
the
second
most
common
in
Spanish
and
French.
In
one
study,
on
average,
about
3.68%
of
letters
used
in
English
tend
to
be
‹a›s,
while
the
number
is
6.22%
in
Spanish
and
3.95%
in
French.
"A"
is
often
used
to
denote
something
or
someone
of
a
better
or
more
prestigious
quality
or
status:
A-,
A
or
A+,
the
best
grade
that
can
be
assigned
by
teachers
for
students'
schoolwork;
A
grade
for
clean
restaurants;
A-List
celebrities,
etc.
Such
associations
can
have
a
motivating
effect
as
exposure
to
the
letter
A
has
been
found
to
improve
performance,
when
compared
with
other
letters.
A
turned
"a"
()
is
used
by
the
International
Phonetic
Alphabet
for
the
near-open
central
vowel,
while
a
turned
capital
"A"
("∀")
is
used
in
predicate
logic
to
specify
universal
quantification.
Codes
for
computing.
In
Unicode
the
capital
"A"
is
codepoint
U+0043
and
the
lower
case
"a"
is
U+0067.
The
ASCII
code
for
capital
"A"
is
65
and
for
lower
case
"a"
is
97;
or
in
binary
01000001
and
01100001,
respectively.
The
EBCDIC
code
for
capital
"A"
is
193
and
for
lowercase
"a"
is
129.
The
numeric
character
references
in
HTML
and
XML
are
"&amp;#65;"
and
"&amp;#97;"
for
upper
and
lower
case,
respectively.
---END.OF.DOCUMENT---
Alabama.
Alabama
is
a
state
located
in
the
southeastern
region
of
the
United
States
of
America.
It
is
bordered
by
Tennessee
to
the
north,
Georgia
to
the
east,
Florida
and
the
Gulf
of
Mexico
to
the
south,
and
Mississippi
to
the
west.
Alabama
ranks
30th
in
total
land
area
and
ranks
second
in
the
size
of
its
inland
waterways.
The
state
ranks
23rd
in
population
with
almost
4.6 million
residents
in
2006.
From
the
American
Civil
War
until
World
War
II,
Alabama,
like
many
Southern
states,
suffered
economic
hardship,
in
part
because
of
continued
dependence
on
agriculture.
White
rural
interests
dominated
the
state
legislature
until
the
1960s,
while
urban
interests
and
African
Americans
were
underrepresented.
Following
World
War
II,
Alabama
experienced
significant
recovery
as
the
economy
of
the
state
transitioned
from
agriculture
to
diversified
interests
in
heavy
manufacturing,
mineral
extraction,
education,
and
technology,
as
well
as
the
establishment
or
expansion
of
multiple
military
installations,
primarily
those
of
the
U.S.
Army
and
U.S.
Air
Force.
The
state
has
heavily
invested
in
aerospace,
education,
health
care,
and
banking,
and
various
heavy
industries
including
automobile
manufacturing,
mineral
extraction,
steel
production
and
fabrication.
Alabama
is
unofficially
nicknamed
the
"Yellowhammer
State",
which
is
also
the
name
of
the
state
bird.
Alabama
is
also
known
as
the
"Heart
of
Dixie".
The
state
tree
is
the
Longleaf
Pine,
the
state
flower
is
the
Camellia.
The
capital
of
Alabama
is
Montgomery,
and
the
largest
city
by
population
is
Birmingham.
The
largest
city
by
total
land
area
is
Huntsville.
The
oldest
city
is
Mobile.
Etymology
of
state
name.
The
Alabama,
a
Muskogean
tribe
whose
members
lived
just
below
the
confluence
of
the
Coosa
and
Tallapoosa
Rivers
on
the
upper
reaches
of
the
Alabama
River,
served
as
the
etymological
source
of
the
names
of
the
river
and
state.
In
the
Alabama
language,
the
word
for
an
Alabama
person
is
"Albaamo"
(or
variously
"Albaama"
or
"Albàamo"
in
different
dialects;
the
plural
form
"Alabama
persons"
is
"Albaamaha").
The
word
"Alabama"
is
believed
to
have
originated
from
the
Choctaw
language
and
was
later
adopted
by
the
Alabama
tribe
as
their
name.
The
spelling
of
the
word
varies
significantly
between
sources.
The
first
usage
appears
in
three
accounts
of
the
Hernando
de
Soto
expedition
of
1540
with
Garcilasso
de
la
Vega
using
"Alibamo"
while
the
Knight
of
Elvas
and
Rodrigo
Ranjel
wrote
"Alibamu"
and
"Limamu",
respectively.
As
early
as
1702,
the
tribe
was
known
to
the
French
as
"Alibamon"
with
French
maps
identifying
the
river
as
"Rivière
des
Alibamons".
Other
spellings
of
the
appellation
have
included
"Alibamu",
"Alabamo",
"Albama",
"Alebamon",
"Alibama",
"Alibamou",
"Alabamu",
and
"Allibamou".
Although
the
origin
of
"Alabama"
was
evident,
the
meaning
of
the
tribe's
name
was
not
always
clear.
An
article
without
a
byline
appearing
in
the
"Jacksonville
Republican"
on
July
27,
1842,
originated
the
idea
that
the
meaning
was
"Here
We
Rest."
This
notion
was
popularized
in
the
1850s
through
the
writings
of
Alexander
Beaufort
Meek.
Experts
in
the
Muskogean
languages
have
been
unable
to
find
any
evidence
that
would
support
this
translation.
It
is
now
generally
accepted
that
the
word
comes
from
the
Choctaw
words
"alba"
(meaning
"plants"
or
"weeds")
and
"amo"
(meaning
"to
cut",
"to
trim",
or
"to
gather").
This
results
in
translations
such
as
"clearers
of
the
thicket"
or
even
"herb
gatherers"
which
may
refer
to
clearing
of
land
for
the
purpose
of
planting
crops
or
to
collection
of
medicinal
plants
by
medicine
men.
History.
Among
the
Native
American
people
once
living
in
the
area
of
present
day
Alabama
were
Alabama
("Alibamu"),
Cherokee,
Chickasaw,
Choctaw,
Creek,
Koasati,
and
Mobile.
Trade
with
the
Northeast
via
the
Ohio
River
began
during
the
Burial
Mound
Period
(1000
BC-700
AD)
and
continued
until
European
contact.
The
agrarian
Mississippian
culture
covered
most
of
the
state
from
1000
to
1600
AD,
with
one
of
its
major
centers
being
at
the
Moundville
Archaeological
Site
in
Moundville,
Alabama.
Artifacts
recovered
from
archaeological
excavations
at
Moundville
were
a
major
component
in
the
formulation
of
the
Southeastern
Ceremonial
Complex.
Contrary
to
popular
belief,
this
development
appears
to
have
no
direct
links
to
Mesoamerica,
but
developed
independently.
This
Ceremonial
Complex
represents
a
major
component
of
the
religion
of
the
Mississippian
peoples,
and
is
one
of
the
primary
means
by
which
their
religion
is
understood.
The
French
founded
the
first
European
settlement
in
the
state
with
the
establishment
of
Mobile
in
1702.
Southern
Alabama
was
French
from
1702
to
1763,
part
of
British
West
Florida
from
1763
to
1780,
and
part
of
Spanish
West
Florida
from
1780
to
1814.
Northern
and
central
Alabama
was
part
of
British
Georgia
from
1763
to
1783
and
part
of
the
American
Mississippi
territory
thereafter.
Its
statehood
was
delayed
by
the
lack
of
a
coastline;
rectified
when
Andrew
Jackson
captured
Spanish
Mobile
in
1814.
Alabama
was
the
twenty-second
state,
admitted
to
the
Union
in
1819.
Its
constitution
provided
for
universal
suffrage
for
white
men.
Alabama
was
part
of
the
new
frontier
in
the
1820s
and
1830s.
Settlers
rapidly
arrived
to
take
advantage
of
its
fertile
soil.
Planters
brought
slaves
with
them,
and
traders
brought
in
more
from
the
Upper
South
as
the
cotton
plantations
expanded.
The
economy
of
the
central
"Black
Belt"
was
built
around
large
cotton
plantations
whose
owners
built
their
wealth
on
slave
labor.
It
was
named
for
the
dark,
productive
soil.
Elsewhere
poor
whites
were
subsistence
farmers.
According
to
the
1860
census,
enslaved
Africans
comprised
45%
of
the
state's
population
of
964,201.
There
were
only
2,690
free
persons
of
color.
In
1861
Alabama
declared
its
secession
from
the
Union
and
joined
the
Confederate
States
of
America.
While
few
battles
were
fought
in
the
state,
Alabama
contributed
about
120,000
soldiers
to
the
Civil
War.
All
the
slaves
were
freed
by
1865.
Following
Reconstruction,
Alabama
was
restored
to
the
Union
in
1868.
After
the
Civil
War,
the
state
was
still
chiefly
rural
and
tied
to
cotton.
Planters
resisted
working
with
free
labor
and
sought
to
re-establish
controls
over
African
Americans.
Whites
used
paramilitary
groups,
Jim
Crow
laws
and
segregation
to
reduce
freedoms
of
African
Americans
and
restore
their
own
dominance.
In
its
new
constitution
of
1901,
the
legislature
effectively
disfranchised
African
Americans
through
voting
restrictions.
While
the
planter
class
had
engaged
poor
whites
in
supporting
these
efforts,
the
new
restrictions
resulted
in
disfranchising
poor
whites
as
well.
By
1941,
a
total
of
more
whites
than
blacks
had
been
disfranchised:
600,000
whites
to
520,000
blacks.
This
was
due
mostly
to
effects
of
the
cumulative
poll
tax.
The
damage
to
the
African-American
community
was
pervasive,
as
nearly
all
its
citizens
lost
the
ability
to
vote.
In
1900,
fourteen
Black
Belt
counties
(which
were
primarily
African
American)
had
more
than
79,000
voters
on
the
rolls.
By
June
1,
1903,
the
number
of
registered
voters
had
dropped
to
1,081.
In
1900,
Alabama
had
more
than
181,000
African
Americans
eligible
to
vote.
By
1903,
only
2,980
had
managed
to
"qualify"
to
register,
although
at
least
74,000
black
voters
were
literate.
The
shut
out
was
long-lasting.
The
disfranchisement
was
ended
only
by
African
Americans
leading
the
Civil
Rights
Movement
and
gaining
Federal
legislation
in
the
mid-1960s
to
protect
their
voting
and
civil
rights.
The
Voting
Rights
Act
of
1965
also
protected
the
suffrage
of
poor
whites.
The
rural-dominated
legislature
continued
to
underfund
schools
and
services
for
African
Americans
in
the
segregated
state,
but
did
not
relieve
them
of
paying
taxes.
Continued
racial
discrimination,
agricultural
depression,
and
the
failure
of
the
cotton
crops
due
to
boll
weevil
infestation
led
tens
of
thousands
of
African
Americans
to
seek
out
opportunities
in
northern
cities.
They
left
Alabama
in
the
early
20th
century
as
part
of
the
Great
Migration
to
industrial
jobs
and
better
futures
in
northern
industrial
cities.
The
population
growth
rate
in
Alabama
(see
"Historical
Populations"
table
below)
dropped
by
nearly
half
from
1910–1920,
reflecting
the
effect
of
outmigration.
At
the
same
time,
many
rural
whites
and
blacks
migrated
to
the
city
of
Birmingham
for
work
in
new
industrial
jobs.
It
experienced
such
rapid
growth
that
it
was
nicknamed
"The
Magic
City".
By
the
1920s,
Birmingham
was
the
19th
largest
city
in
the
U.S.
and
held
more
than
30%
of
the
population
of
the
state.
Heavy
industry
and
mining
were
the
basis
of
the
economy.
Despite
massive
population
changes
in
the
state
from
1901
to
1961,
the
rural-dominated
legislature
refused
to
reapportion
House
and
Senate
seats
based
on
population.
They
held
on
to
old
representation
to
maintain
political
and
economic
power
in
agricultural
areas.
In
addition,
the
state
legislature
gerrymandered
the
few
Birmingham
legislative
seats
to
ensure
election
by
persons
living
outside
of
Birmingham.
One
result
was
that
Jefferson
County,
containing
Birmingham's
industrial
and
economic
powerhouse,
contributed
more
than
one-third
of
all
tax
revenue
to
the
state.
Urban
interests
were
consistently
underrepresented
in
the
legislature.
A
1960
study
noted
that
because
of
rural
domination,
"A
minority
of
about
25
per
cent
of
the
total
state
population
is
in
majority
control
of
the
Alabama
legislature."
African
Americans
were
presumed
partial
to
Republicans
for
historical
reasons,
but
they
were
disenfranchised.
White
Alabamans
still
felt
bitter
towards
the
Republican
Party
in
the
aftermath
of
the
Civil
War
and
Reconstruction.
These
factors
created
a
longstanding
tradition
that
any
candidate
who
wanted
to
be
viable
with
white
voters
had
to
run
as
a
Democrat
regardless
of
political
beliefs.
The
state
continued
as
one-party
Democratic
for
more
than
a
century
after
Reconstruction
ended.
It
produced
a
number
of
national
leaders.
Industrial
development
related
to
the
demands
of
World
War
II
brought
prosperity.
Cotton
faded
in
importance
as
the
state
developed
a
manufacturing
and
service
base.
In
the
1960s
under
Governor
George
Wallace,
many
whites
in
the
state
opposed
integration
efforts.
During
the
Civil
Rights
Movement,
African
Americans
achieved
a
protection
of
voting
and
other
civil
rights
through
the
passage
of
the
national
Civil
Rights
Act
of
1964,
and
the
Voting
Rights
Act
of
1965.
"De
jure"
segregation
ended
in
the
states
as
Jim
Crow
laws
were
invalidated
or
repealed.
Under
the
Voting
Rights
Act
of
1965,
cases
were
filed
in
Federal
courts
to
force
Alabama
to
properly
redistrict
by
population
both
the
state
legislature
House
and
Senate.
In
1972,
for
the
first
time
since
1901,
the
legislature
implemented
the
Alabama
constitution's
provision
for
periodic
redistricting
based
on
population.
This
benefited
the
many
urban
areas
that
had
developed,
and
all
in
the
population
who
had
been
underrepresented
for
more
than
60 years.
After
1972,
the
state's
white
voters
shifted
much
of
their
support
to
Republican
candidates
in
presidential
elections
(as
also
occurred
in
neighboring
southern
states).
Since
1990
the
majority
of
whites
in
the
state
have
also
voted
increasingly
Republican
in
state
elections,
although
Democrats
are
still
the
majority
party
in
both
houses
of
the
legislature.
Geography.
Alabama
is
the
thirtieth
largest
state
in
the
United
States
with
52,423
square
miles
(135,775 km²)
of
total
area:
3.19%
of
the
area
is
water,
making
Alabama
twenty-third
in
the
amount
of
surface
water,
also
giving
it
the
second
largest
inland
waterway
system
in
the
United
States.
About
three-fifths
of
the
land
area
is
a
gentle
plain
with
a
general
descent
towards
the
Mississippi
River
and
the
Gulf
of
Mexico.
The
North
Alabama
region
is
mostly
mountainous,
with
the
Tennessee
River
cutting
a
large
valley
creating
numerous
creeks,
streams,
rivers,
mountains,
and
lakes.
The
states
bordering
Alabama
are
Tennessee
to
the
north;
Georgia
to
the
east;
Florida
to
the
south;
and
Mississippi
to
the
west.
Alabama
has
coastline
at
the
Gulf
of
Mexico,
in
the
extreme
southern
edge
of
the
state.
Alabama
ranges
in
elevation
from
sea
level
at
Mobile
Bay
to
over
1,800 feet
(550 m)
in
the
Appalachian
Mountains
in
the
northeast.
The
highest
point
is
Mount
Cheaha,
at
a
height
of.
Alabama's
land
consists
of
of
forest
or
67%
of
total
land
area.
Suburban
Baldwin
County,
along
the
Gulf
Coast,
is
the
largest
county
in
the
state
in
both
land
area
and
water
area.
Areas
in
Alabama
administered
by
the
National
Park
Service
include
Horseshoe
Bend
National
Military
Park
near
Alexander
City;
Little
River
Canyon
National
Preserve
near
Fort
Payne;
Russell
Cave
National
Monument
in
Bridgeport;
Tuskegee
Airmen
National
Historic
Site
in
Tuskegee;
and
Tuskegee
Institute
National
Historic
Site
near
Tuskegee.
Additionally,
Alabama
has
four
National
Forests
including
Conecuh,
Talladega,
Tuskegee,
and
William
B.
Bankhead.
Alabama
also
contains
the
Natchez
Trace
Parkway,
the
Selma
To
Montgomery
National
Historic
Trail,
and
the
Trail
Of
Tears
National
Historic
Trail.
A
notable
natural
wonder
in
Alabama
is
"Natural
Bridge"
rock,
the
longest
natural
bridge
east
of
the
Rockies,
located
just
south
of
Haleyville,
in
Winston
County.
A
-wide
meteorite
impact
crater
is
located
in
Elmore
County,
just
north
of
Montgomery.
This
is
the
Wetumpka
crater,
which
is
the
site
of
"Alabama's
greatest
natural
disaster".
A
-wide
meteorite
hit
the
area
about
80 million
years
ago.
The
hills
just
east
of
downtown
Wetumpka
showcase
the
eroded
remains
of
the
impact
crater
that
was
blasted
into
the
bedrock,
with
the
area
labeled
the
Wetumpka
crater
or
astrobleme
("star-wound")
because
of
the
concentric
rings
of
fractures
and
zones
of
shattered
rock
that
can
be
found
beneath
the
surface.
In
2002,
Christian
Koeberl
with
the
Institute
of
Geochemistry
University
of
Vienna
published
evidence
and
established
the
site
as
an
internationally
recognized
impact
crater.
Climate.
The
state
is
classified
as
humid
subtropical
("Cfa")
under
the
Koppen
Climate
Classification.
The
average
annual
temperature
is
64 °F
(18 °C).
Temperatures
tend
to
be
warmer
in
the
southern
part
of
the
state
with
its
proximity
to
the
Gulf
of
Mexico,
while
the
northern
parts
of
the
state,
especially
in
the
Appalachian
Mountains
in
the
northeast,
tend
to
be
slightly
cooler.
Generally,
Alabama
has
very
hot
summers
and
mild
winters
with
copious
precipitation
throughout
the
year.
Alabama
receives
an
average
of
of
rainfall
annually
and
enjoys
a
lengthy
growing
season
of
up
to
300 days
in
the
southern
part
of
the
state.
Summers
in
Alabama
are
among
the
hottest
in
the
United
States,
with
high
temperatures
averaging
over
throughout
the
summer
in
some
parts
of
the
state.
Alabama
is
also
prone
to
tropical
storms
and
even
hurricanes.
Areas
of
the
state
far
away
from
the
Gulf
are
not
immune
to
the
effects
of
the
storms,
which
often
dump
tremendous
amounts
of
rain
as
they
move
inland
and
weaken.
South
Alabama
reports
more
thunderstorms
than
any
part
of
the
U.S.
The
Gulf
Coast,
around
Mobile
Bay,
averages
between
70
and
80 days
per
year
with
thunder
reported.
This
activity
decreases
somewhat
further
north
in
the
state,
but
even
the
far
north
of
the
state
reports
thunder
on
about
60 days
per
year.
Occasionally,
thunderstorms
are
severe
with
frequent
lightning
and
large
hail
–
the
central
and
northern
parts
of
the
state
are
most
vulnerable
to
this
type
of
storm.
Alabama
ranks
seventh
in
the
number
of
deaths
from
lightning
and
ninth
in
the
number
of
deaths
from
lightning
strikes
per
capita.
Sometimes
tornadoes
occur
–
these
are
common
throughout
the
state,
although
the
peak
season
for
tornadoes
varies
from
the
northern
to
southern
parts
of
the
state.
Alabama
shares
the
dubious
distinction,
with
Kansas,
of
having
reported
more
EF5
tornadoes
than
any
other
state
–
according
to
statistics
from
the
National
Climatic
Data
Center
for
the
period
January
1,
1950,
to
October
31,
2006.
An
F5
tornado
is
the
most
powerful
of
its
kind.
Several
long
–
tracked
F5
tornadoes
have
contributed
to
Alabama
reporting
more
tornado
fatalities
than
any
other
state
except
for
Texas
and
Mississippi.
The
Super
Outbreak
in
March
1974,
badly
affected
Alabama.
The
northern
part
of
the
state
–
along
the
Tennessee
Valley
–
is
one
of
the
areas
in
the
US
most
vulnerable
to
violent
tornadoes.
The
area
of
Alabama
and
Mississippi
most
affected
by
tornadoes
is
sometimes
referred
to
as
Dixie
Alley,
as
distinct
from
the
Tornado
Alley
of
the
Southern
Plains.
Alabama
is
one
of
the
few
places
in
the
world
that
has
a
secondary
tornado
season
(November
and
December)
along
with
the
spring
severe
weather
season.
Winters
are
generally
mild
in
Alabama,
as
they
are
throughout
most
of
the
southeastern
United
States,
with
average
January
low
temperatures
around
in
Mobile
and
around
in
Birmingham.
Although
snow
is
a
rare
event
in
much
of
Alabama,
areas
of
the
state
north
of
Montgomery
may
receive
a
dusting
of
snow
a
few
times
every
winter,
with
an
occasional
moderately
heavy
snowfall
every
few
years.
For
example,
the
annual
average
snowfall
for
the
Birmingham
area
is
2 inches
per
year.
In
the
southern
Gulf
coast,
snowfall
is
less
frequent,
sometimes
going
several
years
without
any
snowfall.
Demographics.
The
United
States
Census
Bureau,
as
of
July
1,
2008,
estimated
Alabama's
population
at
4,661,900,
which
represents
an
increase
of
214,545,
or
4.8%,
since
the
last
census
in
2000.
This
includes
a
natural
increase
since
the
last
census
of
121,054
people
(that
is
502,457
births
minus
381,403
deaths)
and
an
increase
due
to
net
migration
of
104,991
people
into
the
state.
Immigration
from
outside
the
United
States
resulted
in
a
net
increase
of
31,180
people,
and
migration
within
the
country
produced
a
net
gain
of
73,811
people.
The
state
had
108,000
foreign-born
(2.4%
of
the
state
population),
of
which
an
estimated
22.2%
were
illegal
immigrants
(24,000).
The
center
of
population
of
Alabama
is
located
in
Chilton
County,
outside
of
the
town
of
Jemison,
an
area
known
as
Jemison
Division.
Race
and
ancestry.
The
largest
reported
ancestry
groups
in
Alabama:
African
American
(26.0%),
American
(17.0%),
English
(7.8%),
Irish
(7.7%),
German
(5.7%),
and
Scots-Irish
(2.0%).
'American'
does
not
include
those
reported
as
Native
American.
Religion.
Alabama
is
located
in
the
middle
of
the
Bible
Belt.
In
a
2007
survey,
nearly
70%
of
respondents
could
name
all
four
of
the
Christian
Gospels.
Of
those
who
indicated
a
religious
preference,
59%
said
they
possessed
a
"full
understanding"
of
their
faith
and
needed
no
further
learning.
In
a
2007
poll,
92%
of
Alabamians
reported
having
at
least
some
confidence
in
churches
in
the
state.
The
Mobile
area
is
notable
for
its
large
percentage
of
Catholics,
owing
to
the
area's
unique
early
history
under
French
and
Spanish
rule.
Today,
a
majority
of
Alabamians
identify
themselves
as
Protestants.
In
the
2008
American
Religious
Identification
Survey,
80%
of
Alabama
respondents
reported
their
religion
as
"Other
Christian"
(survey's
label),
6%
as
Catholic,
and
11%
as
having
no
religion
at
all.
Economy.
According
to
the
United
States
Bureau
of
Economic
Analysis,
the
2008
total
gross
state
product
was
$170 billion,
or
$29,411
per
capita.
Alabama's
2008
GDP
increased
0.7%
from
the
previous
year.
The
single
largest
increase
came
in
the
area
of
information.
In
1999,
per
capita
income
for
the
state
was
$18,189.
Alabama's
agricultural
outputs
include
poultry
and
eggs,
cattle,
plant
nursery
items,
peanuts,
cotton,
grains
such
as
corn
and
sorghum,
vegetables,
milk,
soybeans,
and
peaches.
Although
known
as
"The
Cotton
State",
Alabama
ranks
between
eight
and
ten
in
national
cotton
production,
according
to
various
reports,
with
Texas,
Georgia
and
Mississippi
comprising
the
top
three.
Alabama's
industrial
outputs
include
iron
and
steel
products
(including
cast-iron
and
steel
pipe);
paper,
lumber,
and
wood
products;
mining
(mostly
coal);
plastic
products;
cars
and
trucks;
and
apparel.
Also,
Alabama
produces
aerospace
and
electronic
products,
mostly
in
the
Huntsville
area,
location
of
NASA
George
C.
Marshall
Space
Flight
Center
and
the
US
Army
Aviation
and
Missile
Command,
headquartered
at
Redstone
Arsenal.
Alabama
contains
the
largest
industrial
growth
corridor
in
the
nation,
including
the
surrounding
states
of
Tennessee,
Mississippi,
Florida,
and
Georgia.
Most
of
this
growth
is
due
to
Alabama's
rapidly
expanding
automotive
manufacturing
industry.
Headquartered
in
the
state
are
Honda
Manufacturing
of
Alabama,
Hyundai
Motor
Manufacturing
Alabama,
Mercedes-Benz
U.S.
International,
and
Toyota
Motor
Manufacturing
Alabama.
Since
1993,
the
automobile
industry
has
generated
more
than
67,800
new
jobs
in
the
state.
Alabama
currently
ranks
4th
in
the
nation
in
automobile
output.
In
the
1970s
and
1980s,
Birmingham's
economy
was
transformed
by
investments
in
bio-technology
and
medical
research
at
the
University
of
Alabama
at
Birmingham
(UAB)
and
its
adjacent
hospital.
The
UAB
Hospital
is
a
Level
I
trauma
center
providing
health
care
and
breakthrough
medical
research.
UAB
is
now
the
area's
largest
employer
and
the
largest
in
Alabama
with
a
workforce
of
about
20,000.
Health
care
services
provider
HealthSouth
is
also
headquartered
in
the
city.
Birmingham
is
also
a
leading
banking
center,
headquarters
of
the
Regions
Financial
Corporation.
Birmingham-based
Compass
Banchshares
was
acquired
by
Madrid-based
BBVA
in
September
2007;
the
headquarters
of
the
new
BBVA
Compass
Bank
remains
in
Birmingham.
SouthTrust,
another
large
bank
headquartered
in
Birmingham,
was
acquired
by
Wachovia
in
2004.
The
city
still
has
major
operations
as
one
of
the
regional
headquarters
of
Wachovia.
In
November
2006,
Regions
Financial
merged
with
AmSouth
Bancorporation,
which
was
also
headquartered
in
Birmingham.
They
formed
the
eighth
largest
U.S.
bank
based
on
by
total
assets.
Nearly
a
dozen
smaller
banks
are
also
headquartered
in
the
Magic
City,
such
as
Superior
Bank
and
New
South
Federal
Savings
Bank.
Telecommunications
provider
AT&T,
formerly
BellSouth,
has
a
major
presence
with
several
large
offices
in
the
metropolitan
area.
Major
insurance
providers:
Protective
Life,
Infinity
Property
&
Casualty
and
ProAssurance
among
others,
are
headquartered
in
Birmingham
and
employ
a
large
number
of
people
in
Greater
Birmingham.
The
city
is
also
a
powerhouse
of
construction
and
engineering
companies,
including
BE&K
and
B.
L.
Harbert
International
which
routinely
are
included
in
the
Engineering
News-Record
lists
of
top
design
and
international
construction
firms.
Huntsville
is
regarded
for
its
high-technology
driven
economy
and
is
known
as
the
"Rocket
City"
because
of
NASA's
Marshall
Space
Flight
Center
and
the
Redstone
Arsenal.
Huntsville's
main
economic
influence
is
derived
from
aerospace
and
military
technology.
Redstone
Arsenal,
Cummings
Research
Park
(CRP),
The
University
of
Alabama
in
Huntsville
and
NASA's
Marshall
Space
Flight
Center
comprise
the
main
hubs
for
the
area's
technology-driven
economy.
CRP
is
the
second
largest
research
park
in
the
United
States
and
the
fourth
largest
in
the
world,
and
is
over
38
years
old.
Huntsville
has
commercial
technology
companies
such
as
the
network
access
company
ADTRAN,
computer
graphics
company
Intergraph
and
design
and
manufacturer
of
IT
infrastructure
Avocent.
Telecommunications
provider
Deltacom,
Inc.
and
copper
tube
manufacturer
and
distributor
Wolverine
Tube
are
also
based
in
Huntsville.
Cinram
manufactures
and
distributes
20th
Century
Fox
DVDs
and
Blu-ray
Discs
out
of
their
Huntsville
plant.
Sanmina-SCI
also
has
a
large
presence
in
the
area.
Forty-two
Fortune
500
companies
have
operations
in
Huntsville.
In
2005,
Forbes
Magazine
named
the
Huntsville-Decatur
Combined
Statistical
Area
as
6th
best
place
in
the
nation
for
doing
business,
and
number
one
in
terms
of
the
number
of
engineers
per
total
employment.
The
city
of
Mobile,
Alabama's
only
saltwater
port,
is
a
busy
seaport
on
the
Gulf
of
Mexico
with
inland
waterway
access
to
the
Midwest
by
way
of
the
Tennessee-Tombigbee
Waterway.
The
Port
of
Mobile
is
currently
the
9th
largest
by
tonnage
in
the
United
States.
In
May
2007,
a
site
north
of
Mobile
was
selected
by
German
steelmaker
ThyssenKrupp
for
a
$3.7 billion
steel
production
plant,
with
the
promise
of
2,700
permanent
jobs.
Taxes.
Alabama's
tax
structure
is
one
the
most
regressive
in
the
United
States.
Alabama
levies
a
2,
4,
or
5
percent
personal
income
tax,
depending
upon
the
amount
earned
and
filing
status,
though
taxpayers
can
deduct
their
federal
income
tax
from
their
Alabama
state
tax.
The
state's
general
sales
tax
rate
is
4%.
The
collection
rate
could
be
substantially
higher,
depending
upon
additional
city
and
county
sales
taxes.
For
example,
the
total
sales
tax
rate
in
Mobile
is
9%
and
there
is
an
additional
restaurant
tax
of
1%,
which
means
that
a
diner
in
Mobile
would
pay
a
10%
tax
on
a
meal.
Sales
and
excise
taxes
in
Alabama
account
for
51
percent
of
all
state
and
local
revenue,
compared
with
an
average
of
about
36
percent
nationwide.
Alabama
is
also
one
of
the
few
remaining
states
that
levies
a
tax
on
food
and
medicine.
Alabama's
income
tax
on
poor
working
families
is
among
the
nation's
very
highest.
Alabama
is
the
only
state
that
levies
income
tax
on
a
family
of
four
with
income
as
low
as
$4,600,
which
is
barely
one-quarter
of
the
federal
poverty
line.
Alabama's
threshold
is
the
lowest
among
the
41
states
and
the
District
of
Columbia
with
income
taxes.
The
corporate
income
tax
rate
is
currently
6.5%.
The
overall
federal,
state,
and
local
tax
burden
in
Alabama
ranks
the
state
as
the
second
least
tax-burdened
state
in
the
country.
Property
taxes
are
the
lowest
in
the
United
States.
The
current
state
constitution
requires
a
voter
referendum
to
raise
property
taxes.
Since
Alabama's
tax
structure
largely
depends
on
consumer
spending,
it
is
subject
to
high
variable
budget
structure.
For
example,
in
2003
Alabama
had
an
annual
budget
deficit
as
high
as
$670 million.
It
is
one
of
only
a
few
states
to
accomplish
large
surpluses,
with
a
budget
surplus
of
nearly
$1.2 billion
in
2007,
and
estimated
at
more
than
$2.1 billion
for
2008.
However,
the
declining
national
economy
in
2008
has
eliminated
that
surplus
and
the
state
is
again
facing
shortfall,
with
the
governor
declaring
"proration,"
which
will
result
in
an
immediate
education
budget
cut
and
school
layoffs.
Transportation.
Alabama
has
five
major
interstate
roads
that
cross
it:
I-65
runs
north–south
roughly
through
the
middle
of
the
state;
I-59/I-20
travels
from
the
central
west
border
to
Birmingham,
where
I-59
continues
to
the
north-east
corner
of
the
state
and
I-20
continues
east
towards
Atlanta;
I-85
originates
in
Montgomery
and
runs
east-northeast
to
the
Georgia
border,
providing
a
main
thoroughfare
to
Atlanta;
and
I-10
traverses
the
southernmost
portion
of
the
state,
running
from
west
to
east
through
Mobile.
Another
interstate
road,
I-22,
is
currently
under
construction.
When
completed
around
2012
it
will
connect
Birmingham
with
Memphis,
Tennessee.
Several
US
Highways
also
pass
through
the
state,
such
as
US
11,
US
29,
US
31,
US
43,
US
72,
US
78,
US
80,
US
82,
US
84,
US
98,
US
231,
and
US
280.
Major
airports
in
Alabama
include
Birmingham-Shuttlesworth
International
Airport
(BHM),
Huntsville
International
Airport
(HSV),
Dothan
Regional
Airport
(DHN),
Mobile
Regional
Airport
(MOB),
Montgomery
Regional
Airport
(MGM),
Muscle
Shoals
–
Northwest
Alabama
Regional
Airport
(MSL),
Tuscaloosa
Regional
Airport
(TCL),
and
Pryor
Field
Regional
Airport
(DCU).
For
rail
transport,
Amtrak
schedules
the
Crescent,
a
daily
passenger
train,
running
from
New
York
to
New
Orleans
with
stops
at
Anniston,
Birmingham,
and
Tuscaloosa.
State
government.
The
foundational
document
for
Alabama's
government
is
the
Alabama
Constitution,
which
was
ratified
in
1901.
At
almost
800
amendments
and
310,000
words,
it
is
the
world's
longest
constitution
and
is
roughly
forty
times
the
length
of
the
U.S.
Constitution.
There
is
a
significant
movement
to
rewrite
and
modernize
Alabama's
constitution.
This
movement
is
based
upon
the
fact
that
Alabama's
constitution
highly
centralizes
power
in
Montgomery
and
leaves
practically
no
power
in
local
hands.
Any
policy
changes
proposed
around
the
state
must
be
approved
by
the
entire
Alabama
legislature
and,
frequently,
by
state
referendum.
One
criticism
of
the
current
constitution
claims
that
its
complexity
and
length
were
intentional
to
codify
segregation
and
racism.
The
legislative
branch
is
the
Alabama
Legislature,
a
bicameral
assembly
composed
of
the
Alabama
House
of
Representatives,
with
105
members,
and
the
Alabama
Senate,
with
35
members.
The
Legislature
is
responsible
for
writing,
debating,
passing,
or
defeating
state
legislation.
The
executive
branch
is
responsible
for
the
execution
and
oversight
of
laws.
It
is
headed
by
the
Governor
of
Alabama.
Other
members
of
executive
branch
include
the
cabinet,
the
Attorney
General
of
Alabama,
the
Alabama
Secretary
of
State,
the
Alabama
Commissioner
of
Agriculture
and
Industries,
the
Alabama
State
Treasurer,
and
the
Alabama
State
Auditor.
The
judicial
branch
is
responsible
for
interpreting
the
Constitution
and
applying
the
law
in
state
criminal
and
civil
cases.
The
highest
court
is
the
Supreme
Court
of
Alabama.
Local
and
county
government.
Alabama
has
67
counties.
Each
county
has
its
own
elected
legislative
branch,
usually
called
the
County
Commission,
which
usually
also
has
executive
authority
in
the
county.
Because
of
the
restraints
placed
in
the
Alabama
Constitution,
all
but
seven
counties
(Jefferson,
Lee,
Mobile,
Madison,
Montgomery,
Shelby,
and
Tuscaloosa)
in
the
state
have
little
to
no
home
rule.
Instead,
most
counties
in
the
state
must
lobby
the
Local
Legislation
Committee
of
the
state
legislature
to
get
simple
local
policies
such
as
waste
disposal
to
land
use
zoning.
Alabama
is
an
alcoholic
beverage
control
state;
the
government
holds
a
monopoly
on
the
sale
of
alcohol.
However,
counties
can
declare
themselves
"dry";
the
state
does
not
sell
alcohol
in
those
areas.
State
politics.
The
current
governor
of
the
state
is
Republican
Bob
Riley.
The
lieutenant
governor
is
Jim
Folsom
Jr.
The
Chief
Justice
of
the
Alabama
Supreme
Court
is
Democrat
Sue
Bell
Cobb.
The
Democratic
Party
currently
holds
a
large
majority
in
both
houses
of
the
Legislature.
Because
of
the
Legislature's
power
to
override
a
gubernatorial
veto
by
a
mere
simple
majority
(most
state
Legislatures
require
a
two-thirds
majority
to
override
a
veto),
the
relationship
between
the
executive
and
legislative
branches
can
be
easily
strained
when
different
parties
control
the
branches.
During
Reconstruction
following
the
American
Civil
War,
Alabama
was
occupied
by
federal
troops
of
the
Third
Military
District
under
General
John
Pope.
In
1874,
the
political
coalition
known
as
the
Redeemers
took
control
of
the
state
government
from
the
Republicans,
in
part
by
suppressing
the
African
American
vote.
After
1890,
a
coalition
of
whites
passed
laws
to
segregate
and
disenfranchise
black
residents,
a
process
completed
in
provisions
of
the
1901
constitution.
Provisions
which
disfranchised
African
Americans
also
disfranchised
poor
whites,
however.
By
1941
more
whites
than
blacks
had
been
disfranchised:
600,000
to
520,000,
although
the
impact
was
greater
on
the
African-American
community,
as
almost
all
of
its
citizens
were
disfranchised.
From
1901
to
the
1960s,
the
state
legislature
failed
to
perform
redistricting
as
population
grew
and
shifted
within
the
state.
The
result
was
a
rural
minority
that
dominated
state
politics
until
a
series
of
court
cases
required
redistricting
in
1972.
With
the
disfranchisement
of
African
Americans,
the
state
became
part
of
the
"Solid
South",
a
one-party
system
in
which
the
Democratic
Party
became
essentially
the
only
political
party
in
every
Southern
state.
For
nearly
100 years,
local
and
state
elections
in
Alabama
were
decided
in
the
Democratic
Party
primary,
with
generally
only
token
Republican
challengers
running
in
the
General
Election.
In
the
1986
Democratic
primary
election,
the
then-incumbent
Lieutenant
Governor,
Bill
Baxley,
lost
the
Democratic
nomination
for
Governor
in
a
scandal
where
Republicans
were
permitted
to
cast
votes
for
his
opponent,
then
Attorney
General
Charlie
Graddick.
The
state
Democratic
party
invalidated
the
election
and
placed
the
Baxley's
name
on
the
ballot
as
the
Democratic
candidate
instead
of
the
candidate
chosen
in
the
primary.
The
voters
of
the
state
revolted
at
what
they
perceived
as
disenfranchisement
of
their
right
to
vote
and
elected
the
Republican
challenger
Guy
Hunt
as
Governor.
This
was
the
first
Republican
Governor
elected
in
Alabama
since
Reconstruction.
Since
then,
Republicans
have
become
increasingly
competitive
in
Alabama
politics.
They
currently
control
both
seats
in
the
U.S.
Senate,
four
out
of
the
state's
seven
congressional
seats.
Republicans
hold
an
8–1
majority
on
the
Alabama
Supreme
Court
and
have
a
5–2
majority
among
statewide
elected
executive
branch
offices.
However,
Democrats
currently
hold
all
three
seats
on
the
Alabama
Public
Service
Commission
and
they
maintain
control
of
both
houses
of
the
legislature,
holding
approximately
59.4%
of
seats
in
the
Alabama
Senate
and
58.7%
of
seats
in
the
Alabama
House
of
Representatives.
A
majority
of
local
offices
in
the
state
are
still
held
by
Democrats.
Local
elections
in
rural
counties
are
generally
decided
in
the
Democratic
primary
and
local
elections
in
metropolitan
counties
are
decided
in
the
Republican
Primary
although
there
are
exceptions
to
this
rule.
Only
one
Republican
Lt.
Governor
has
been
elected
since
Reconstruction,
Steve
Windom.
Windom
served
as
Lt.
Governor
under
Democratic
Gov.
Don
Siegelman.
The
last
time
that
Alabama
had
a
governor
and
Lt.
governor
of
the
same
party
was
the
period
between
1983
and
1987
when
Wallace
was
serving
his
fourth
term
as
governor
and
Bill
Baxley
was
serving
as
Lt.
Governor,
both
were
Democrats.
An
overwhelming
majority
of
sheriff's
offices
in
Alabama
are
in
Democratic
hands.
However,
most
of
the
Democratic
sheriffs
preside
over
more
rural
and
less
populated
counties
and
the
majority
of
Republicans
preside
over
more
urban/suburban
and
more
populated
counties.
Only
three
Alabama
counties
(Tuscaloosa,
Montgomery
and
Calhoun)
with
a
population
of
over
100,000
have
Democratic
sheriffs
and
only
five
Alabama
counties
with
a
population
of
under
75,000
have
Republican
sheriffs
(Autauga,
Coffee,
Dale,
Coosa,
and
Blount).
Alabama
state
politics
gained
nationwide
and
international
attention
in
the
1950s
and
1960s
during
the
American
Civil
Rights
Movement,
when
majority
whites
bureaucratically,
and
at
times,
violently
resisted
protests
for
electoral
and
social
reform.
George
Wallace,
the
state's
governor,
remains
a
notorious
and
controversial
figure.
Only
with
the
passage
of
the
Civil
Rights
Act
of
1964
and
Voting
Rights
Act
of
1965
did
African
Americans
regain
suffrage
and
other
civil
rights.
In
2007,
the
Alabama
Legislature
passed,
and
the
Governor
signed,
a
resolution
expressing
"profound
regret"
over
slavery
and
its
lingering
impact.
In
a
symbolic
ceremony,
the
bill
was
signed
in
the
Alabama
State
Capitol,
which
housed
Congress
of
the
Confederate
States
of
America.
National
politics.
From
1876
through
1956,
Alabama
supported
only
Democratic
presidential
candidates,
by
large
margins.
In
1960,
the
Democrats
won
with
John
F.
Kennedy
on
the
ballot,
but
the
Democratic
electors
from
Alabama
gave
6
of
their
11
electoral
votes
as
a
protest
to
Harry
Byrd.
In
1964,
Republican
Barry
Goldwater
carried
the
state,
in
part
because
of
his
opposition
to
the
1964
Civil
Rights
Act,
which
restored
the
franchise
for
African
Americans.
In
the
1968
presidential
election,
Alabama
supported
native
son
and
American
Independent
Party
candidate
George
Wallace
over
both
Richard
Nixon
and
Hubert
Humphrey.
Wallace
was
the
official
Democratic
candidate
in
Alabama,
while
Humphrey
was
listed
as
the
"National
Democratic".
In
1976,
Democratic
candidate
Jimmy
Carter
from
Georgia
carried
the
state,
the
region,
and
the
nation,
but
Democratic
control
of
the
region
slipped
after
that.
Since
1980,
conservative
Alabama
voters
have
increasingly
voted
for
Republican
candidates
at
the
Federal
level,
especially
in
Presidential
elections.
By
contrast,
Democratic
candidates
have
been
elected
to
many
state-level
offices
and
comprise
a
longstanding
majority
in
the
Alabama
Legislature;
see
Dixiecrat.
In
2004,
George
W.
Bush
won
Alabama's
nine
electoral
votes
by
a
margin
of
25
percentage
points
with
62.5%
of
the
vote,
mostly
white
voters.
The
eleven
counties
that
voted
Democratic
were
Black
Belt
counties,
where
African
Americans
are
the
majority
racial
group.
The
state's
two
U.S.
senators
are
Jefferson
B.
Sessions
III
and
Richard
C.
Shelby,
both
Republicans.
In
the
U.S.
House
of
Representatives,
the
state
is
represented
by
seven
members,
five
of
whom
are
Republicans:
(Jo
Bonner,
Mike
D.
Rogers,
Robert
Aderholt,
Parker
Griffith,
and
Spencer
Bachus)
and
two
are
Democrats:
(Bobby
Bright
and
Artur
Davis).
Primary
and
secondary
education.
Public
primary
and
secondary
education
in
Alabama
is
under
the
overview
of
the
Alabama
State
Board
of
Education
as
well
as
local
oversight
by
67
county
school
boards
and
60
city
boards
of
education.
Together,
1,541
individual
schools
provide
education
for
743,364
elementary
and
secondary
students.
Public
school
funding
is
appropriated
through
the
Alabama
Legislature
through
the
Education
Trust
Fund.
In
FY
2006–2007,
Alabama
appropriated
$3,775,163,578
for
primary
and
secondary
education.
That
represented
an
increase
of
$444,736,387
over
the
previous
fiscal
year.
In
2007,
over
82
percent
of
schools
made
adequate
yearly
progress
(AYP)
toward
student
proficiency
under
the
National
No
Child
Left
Behind
law,
using
measures
determined
by
the
state
of
Alabama.
In
2004,
23
percent
of
schools
met
AYP.
While
Alabama's
public
education
system
has
improved,
it
lags
behind
in
achievement
compared
to
other
states.
According
to
U.S.
Census
data,
Alabama's
high
school
graduation
rate
–
75%
–
is
the
second
lowest
in
the
United
States
(after
Mississippi).
The
largest
educational
gains
were
among
people
with
some
college
education
but
without
degrees.
Colleges
and
universities.
Alabama's
programs
of
higher
education
include
14
four-year
public
universities,
two-year
community
colleges,
and
17
private,
undergraduate
and
graduate
universities.
In
the
state
are
two
medical
schools
(University
of
Alabama
at
Birmingham
and
University
of
South
Alabama),
two
veterinary
colleges
(Auburn
University
and
Tuskegee
University),
a
dental
school
(University
of
Alabama
at
Birmingham),
an
optometry
college
(University
of
Alabama
at
Birmingham),
two
pharmacy
schools
(Auburn
University
and
Samford
University),
and
five
law
schools
(University
of
Alabama
School
of
Law,
Birmingham
School
of
Law,
Cumberland
School
of
Law,
Miles
Law
School,
and
the
Thomas
Goode
Jones
School
of
Law).
Public,
post-secondary
education
in
Alabama
is
overseen
by
the
Alabama
Commission
on
Higher
Education.
Colleges
and
universities
in
Alabama
offer
degree
programs
from
two-year
associate
degrees
to
16
doctoral
level
programs.
Accreditation
of
academic
programs
is
through
the
Southern
Association
of
Schools
and
Colleges
as
well
as
a
plethora
of
subject
focused
national
and
international
accreditation
agencies.
Professional
sports
teams.
Alabama
has
several
minor
league
teams
including
four
Southern
League
baseball
teams
as
well
as
one
Arena
Football
League
team.
Notable
Alabamians.
Famous
people
from
Alabama
include
Hank
Aaron,
Tommie
Agee,
Tallulah
Bankhead,
William
Brockman
Bankhead,
Jay
Barker,
Charles
Barkley,
Regina
Benjamin,
Hugo
L.
Black,
Frank
Bolling,
Paul
W.
(Bear)
Bryant,
Jimmy
Buffett,
Bo
Bice,
George
Washington
Carver,
William
Christenberry,
Nat
King
Cole,
Jerricho
Cotchery,
Courteney
Cox
Arquette,
Robert
Gibbs,
Mitch
Holleman,
Zelda
Fitzgerald,
Charles
Ghigna,
Winston
Groom,
William
C.
Handy,
Emmylou
Harris,
Taylor
Hicks,
Joe
Hilley,
Bo
Jackson,
Kate
Jackson,
Jamey
Johnson,
Helen
Keller,
Coretta
Scott
King,
William
R.
King,
Harper
Lee,
Joe
Louis,
Heinie
Manush,
William
March,
Willie
Mays,
Willie
McCovey,
Roy
Moore,
John
Hunt
Morgan,
Jim
Nabors,
Randy
Owen,
Jesse
Owens,
Terrell
Owens,
Satchel
Paige,
Jake
Peavy,
Claude
Pepper,
Rosa
Parks,
Wilson
Pickett,
Howell
Raines,
Condoleezza
Rice,
Lionel
Richie,
Rich
Boy,
Philip
Rivers,
JaMarcus
Russell,
Kenny
Stabler,
Ozzie
Smith,
John
Sparkman,
Bart
Starr,
Ruben
Studdard,
Channing
Tatum,
Oscar
W.
Underwood,
Jimmy
Wales,
George
Wallace,
Booker
T.
Washington,
Billy
Williams,
and
Hank
Williams.
---END.OF.DOCUMENT---
Achilles.
In
Greek
mythology,
Achilles
(Ancient
Greek:)
was
a
Greek
hero
of
the
Trojan
War,
the
central
character
and
the
greatest
warrior
of
Homer's
"Iliad".
Achilles
also
has
the
attributes
of
being
the
most
handsome
of
the
heroes
assembled
against
Troy.
Later
legends
(beginning
with
a
poem
by
Statius
in
the
first
century
AD)
state
that
Achilles
was
invulnerable
in
all
of
his
body
except
for
his
heel.
Since
he
died
due
to
an
arrow
shot
into
his
heel,
the
"Achilles'
heel"
has
come
to
mean
a
person's
principal
weakness.
Birth.
Achilles
was
the
son
of
the
nymph
Thetis
and
Peleus,
the
king
of
the
Myrmidons.
Zeus
and
Poseidon
had
been
rivals
for
the
hand
of
Thetis
until
Prometheus,
the
fire-bringer,
warned
Zeus
of
a
prophecy
that
Thetis
would
bear
a
son
greater
than
his
father.
For
this
reason,
the
two
gods
withdrew
their
pursuit,
and
had
her
wed
Peleus.
As
with
most
mythology
there
is
a
tale
which
offers
an
alternative
version
of
these
events:
in
"Argonautica"
(iv.760)
Hera
alludes
to
Thetis's
chaste
resistance
to
the
advances
of
Zeus,
that
Thetis
was
so
loyal
to
Hera's
marriage
bond
that
she
coolly
rejected
him.
Thetis,
although
a
daughter
of
the
sea-god
Nereus,
was
also
brought
up
by
Hera,
further
explaining
her
resistance
to
the
advances
of
Zeus.
According
to
the
"Achilleid",
written
by
Statius
in
the
first
century
AD,
and
to
no
surviving
previous
sources,
when
Achilles
was
born
Thetis
tried
to
make
him
immortal
by
dipping
him
in
the
river
Styx.
However,
he
was
left
vulnerable
at
the
part
of
the
body
she
held
him
by,
his
heel.
(See
Achilles
heel,
Achilles'
tendon.)
It
is
not
clear
if
this
version
of
events
was
known
earlier.
In
another
version
of
this
story,
Thetis
anointed
the
boy
in
ambrosia
and
put
him
on
top
of
a
fire
to
burn
away
the
mortal
parts
of
his
body.
She
was
interrupted
by
Peleus
and
abandoned
both
father
and
son
in
a
rage.
However
none
of
the
sources
before
Statius
makes
any
reference
to
this
general
invulnerability.
To
the
contrary,
in
the
"Iliad"
Homer
mentions
Achilles
being
wounded:
in
Book
21
the
Paeonian
hero
Asteropaeus,
son
of
Pelagon,
challenged
Achilles
by
the
river
Scamander.
He
cast
two
spears
at
once,
one
grazed
Achilles'
elbow,
"drawing
a
spurt
of
blood."
Also
in
the
fragmentary
poems
of
the
Epic
Cycle
in
which
we
can
find
description
of
the
hero's
death,
Kúpria
(unknown
author),
"Aithiopis"
by
Arctinus
of
Miletus,
"Ilias
Mikrá"
by
Lesche
of
Mytilene,
Iliou
pérsis
by
Arctinus
of
Miletus,
there
is
no
trace
of
any
reference
to
his
general
invulnerability
or
his
famous
weakness
(heel);
in
the
later
vase-paintings
presenting
Achilles'
death,
the
arrow
(or
in
many
cases,
arrows)
hit
his
body.
Peleus
entrusted
Achilles
to
Chiron
the
Centaur,
on
Mt.
Pelion,
to
be
raised.
Achilles
in
the
Trojan
War.
Achilles'
consuming
rage
is
at
some
times
wavering,
but
at
other
times
he
cannot
be
cooled.
The
humanization
of
Achilles
by
the
events
of
the
war
is
an
important
theme
of
the
narrative.
Telephus.
When
the
Greeks
left
for
the
Trojan
War,
they
accidentally
stopped
in
Mysia,
ruled
by
King
Telephus.
In
the
resulting
battle,
Achilles
gave
Telephus
a
wound
that
would
not
heal;
Telephus
consulted
an
oracle,
who
stated
that
"he
that
wounded
shall
heal".
Guided
by
the
oracle,
he
arrived
at
Argos,
where
Achilles
heals
him
in
order
that
he
become
their
guide
for
the
voyage
to
Troy.
According
to
other
reports
in
Euripides'
lost
play
about
Telephus,
he
went
to
Aulis
pretending
to
be
a
beggar
and
asked
Achilles
to
heal
his
wound.
Achilles
refused,
claiming
to
have
no
medical
knowledge.
Alternatively,
Telephus
held
Orestes
for
ransom,
the
ransom
being
Achilles'
aid
in
healing
the
wound.
Odysseus
reasoned
that
the
spear
had
inflicted
the
wound;
therefore,
the
spear
must
be
able
to
heal
it.
Pieces
of
the
spear
were
scraped
off
onto
the
wound
and
Telephus
was
healed.
Troilus.
According
to
the
Cypria
(the
part
of
the
Epic
Cycle
that
tells
the
events
of
the
Trojan
War
before
Achilles'
Wrath),
when
the
Achaeans
desired
to
return
home,
they
were
restrained
by
Achilles,
who
afterwards
attacked
the
cattle
of
Aeneas,
sacked
neighboring
cities
and
killed
Troilus.
According
to
Dares
Phrygius'
"Account
of
the
Destruction
of
Troy",
the
Latin
summary
through
which
the
story
of
Achilles
was
transmitted
to
medieval
Europe,
Troilus
was
a
young
Trojan
prince,
the
youngest
of
King
Priam's
(or
sometimes
Apollo)
and
Hecuba's
five
legitimate
sons.
Despite
his
youth,
he
was
one
of
the
main
Trojan
war
leaders.
Prophecies
linked
Troilus'
fate
to
that
of
Troy
and
so
he
was
ambushed
in
an
attempt
to
capture
him.
Yet
Achilles,
struck
by
the
beauty
of
both
Troilus
and
his
sister
Polyxena,
and
overcome
with
lust
directed
his
sexual
attentions
on
the
youth —
who
refusing
to
yield
found
instead
himself
decapitated
upon
an
altar-omphalos
of
Apollo.
Later
versions
of
the
story
suggested
Troilus
was
accidentally
killed
by
Achilles
in
an
over-ardent
lovers'
embrace.
In
this
version
of
the
myth,
Achilles'
death
therefore
came
in
retribution
for
this
sacrilege.
Ancient
writers
treated
Troilus
as
the
epitome
of
a
dead
child
mourned
by
his
parents.
Had
Troilus
lived
to
adulthood,
the
First
Vatican
Mythographer
claimed
Troy
would
have
been
invincible.
In
the
"Iliad".
Homer's
"Iliad"
is
the
most
famous
narrative
of
Achilles'
deeds
in
the
Trojan
War.
The
Homeric
epic
only
covers
a
few
weeks
of
the
war,
and
does
not
narrate
Achilles'
death.
It
begins
with
Achilles'
withdrawal
from
battle
after
he
is
dishonored
by
Agamemnon,
the
commander
of
the
Achaean
forces.
Agamemnon
had
taken
a
woman
named
Chryseis
as
his
slave.
Her
father
Chryses,
a
priest
of
Apollo,
begged
Agamemnon
to
return
her
to
him.
Agamemnon
refused
and
Apollo
sent
a
plague
amongst
the
Greeks.
The
prophet
Calchas
correctly
determined
the
source
of
the
troubles
but
would
not
speak
unless
Achilles
vowed
to
protect
him.
Achilles
did
so
and
Calchas
declared
Chryseis
must
be
returned
to
her
father.
Agamemnon
consented,
but
then
commanded
that
Achilles'
battle
prize
Briseis
be
brought
to
replace
Chryseis.
Angry
at
the
dishonor
(and
as
he
says
later,
because
he
loved
Briseis)
and
at
the
urging
of
Thetis,
Achilles
refused
to
fight
or
lead
his
troops
alongside
the
other
Greek
forces.
As
the
battle
turned
against
the
Greeks,
Nestor
declared
that
the
Trojans
were
winning
because
Agamemnon
had
angered
Achilles,
and
urged
the
king
to
appease
the
warrior.
Agamemnon
agreed
and
sent
Odysseus
and
two
other
chieftains,
Ajax
and
Phoenix,
to
Achilles
with
the
offer
of
the
return
of
Briseis
and
other
gifts.
Achilles
rejected
all
Agamemnon
offered
him,
and
simply
urged
the
Greeks
to
sail
home
as
he
was
planning
to
do.
Eventually,
however,
hoping
to
retain
glory
despite
his
absence
from
the
battle,
Achilles
prayed
to
his
mother
Thetis,
asking
her
to
plead
with
Zeus
to
allow
the
Trojans
to
push
back
the
Greek
forces.
The
Trojans,
led
by
Hector,
subsequently
pushed
the
Greek
army
back
toward
the
beaches
and
assaulted
the
Greek
ships.
With
the
Greek
forces
on
the
verge
of
absolute
destruction,
Patroclus
led
the
Myrmidons
into
battle,
though
Achilles
remained
at
his
camp.
Patroclus
succeeded
in
pushing
the
Trojans
back
from
the
beaches,
but
was
killed
by
Hector
before
he
could
lead
a
proper
assault
on
the
city
of
Troy.
After
receiving
the
news
of
the
death
of
Patroclus
from
Antilochus,
the
son
of
Nestor,
Achilles
grieved
over
his
close
friend's
death
and
held
many
funeral
games
in
his
honor.
His
mother
Thetis
came
to
comfort
the
distraught
Achilles.
She
persuaded
Hephaestus
to
make
new
armor
for
him,
in
place
of
the
armor
that
Patroclus
had
been
wearing
which
was
taken
by
Hector.
The
new
armor
included
the
Shield
of
Achilles,
described
in
great
detail
by
the
poet.
Enraged
over
the
death
of
Patroclus,
Achilles
ended
his
refusal
to
fight
and
took
the
field
killing
many
men
in
his
rage
but
always
seeking
out
Hector.
Achilles
even
engaged
in
battle
with
the
river
god
Scamander
who
became
angry
that
Achilles
was
choking
his
waters
with
all
the
men
he
killed.
The
god
tried
to
drown
Achilles
but
was
stopped
by
Hera
and
Hephaestus.
Zeus
himself
took
note
of
Achilles'
rage
and
sent
the
gods
to
restrain
him
so
that
he
would
not
go
on
to
sack
Troy
itself,
seeming
to
show
that
the
unhindered
rage
of
Achilles
could
defy
fate
itself
as
Troy
was
not
meant
to
be
destroyed
yet.
Finally
Achilles
found
his
prey.
Achilles
chased
Hector
around
the
wall
of
Troy
three
times
before
Athena,
in
the
form
of
Hector's
favorite
and
dearest
brother,
Deiphobus,
persuaded
Hector
to
stop
running
and
fight
Achilles
face
to
face.
After
Hector
realized
the
trick,
he
knew
the
battle
was
inevitable.
Wanting
to
go
down
fighting,
he
charged
at
Achilles
with
his
only
weapon,
his
sword,
but
missed.
Accepting
his
fate,
Hector
begged
Achilles –
not
to
spare
his
life,
but
to
treat
his
body
with
respect
after
killing
him.
Achilles
told
Hector
it
was
hopeless
to
expect
that
of
him,
declaring
that
"my
rage,
my
fury
would
drive
me
now
to
hack
your
flesh
away
and
eat
you
raw —
such
agonies
you
have
caused
me".
Achilles
then
got
his
vengeance,
killing
Hector
with
a
single
blow
to
the
neck
and
tying
the
Trojan's
body
to
his
chariot,
dragging
it
around
the
battlefield
for
nine
days.
With
the
assistance
of
the
god
Hermes,
Hector's
father,
Priam,
went
to
Achilles'
tent
to
plead
with
Achilles
to
permit
him
to
perform
for
Hector
his
funeral
rites.
The
final
passage
in
the
"Iliad"
is
Hector's
funeral,
after
which
the
doom
of
Troy
was
just
a
matter
of
time.
Penthesilea.
Achilles,
after
his
temporary
truce
with
Priam,
fought
and
killed
the
Amazonian
warrior
queen
Penthesilea,
but
later
grieved
over
her
death.
At
first,
he
was
so
distracted
by
her
beauty,
he
did
not
fight
as
intensely
as
usual.
Once
he
realized
that
his
distraction
was
endangering
his
life,
he
refocused,
and
killed
her.
As
he
grieved
over
the
death
of
such
a
rare
beauty,
a
notorious
Greek
jeerer
by
the
name
of
Thersites
laughed
and
mocked
the
great
Achilles.
Annoyed
by
his
insensitivity
and
disrespect,
Achilles
punched
him
in
the
face
and
killed
him
instantly.
Memnon,
and
the
fall
of
Achilles.
Following
the
death
of
Patroclus,
Achilles'
closest
companion
was
Nestor's
son
Antilochus.
When
Memnon,
king
of
Ethiopia
killed
Antilochus,
Achilles
was
once
again
drawn
onto
the
battlefield
to
seek
revenge.
The
fight
between
Achilles
and
Memnon
over
Antilochus
echoes
that
of
Achilles
and
Hector
over
Patroclus,
except
that
Memnon
(unlike
Hector)
was
also
the
son
of
a
goddess.
Many
Homeric
scholars
argued
that
episode
inspired
many
details
in
the
"Iliads
description
of
the
death
of
Patroclus
and
Achilles'
reaction
to
it.
The
episode
then
formed
the
basis
of
the
cyclic
epic
"Aethiopis",
which
was
composed
after
the
"Iliad",
possibly
in
the
7th
century
B.C.
The
"Aethiopis"
is
now
lost,
except
for
scattered
fragments
quoted
by
later
authors.
As
predicted
by
Hector
with
his
dying
breath,
Achilles
was
thereafter
killed
by
Paris
with
an
arrow
(to
the
heel
according
to
Statius).
In
some
versions,
the
god
Apollo
guided
Paris'
arrow.
Some
retellings
also
state
that
Achilles
was
scaling
the
gates
of
Troy
and
was
hit
with
a
poisoned
arrow.
Both
versions
conspicuously
deny
the
killer
any
sort
of
valor
owing
to
the
common
conception
that
Paris
was
a
coward
and
not
the
man
his
brother
Hector
was,
and
Achilles
remained
undefeated
on
the
battlefield.
His
bones
were
mingled
with
those
of
Patroclus,
and
funeral
games
were
held.
He
was
represented
in
the
lost
Trojan
War
epic
of
Arctinus
of
Miletus
as
living
after
his
death
in
the
island
of
Leuke
at
the
mouth
of
the
river
Danube
(see
below).
Another
version
of
Achilles'
death
is
that
he
fell
deeply
in
love
with
one
of
the
Trojan
princesses,
Polyxena,
Achilles
asks
Priam
for
Polyxena's
hand
in
marriage.
Priam
is
willing
because
it
would
mean
the
end
of
the
war
and
an
alliance
with
the
world's
greatest
warrior.
But
while
Priam
is
overseeing
the
private
marriage
of
Polyxena
and
Achilles,
Paris
who
would
have
to
give
up
Helen
if
Achilles
married
his
sister
hides
in
the
bushes
and
shoots
Achilles
with
a
divine
arrow
killing
him.
Achilles
was
cremated
and
his
ashes
buried
in
the
same
urn
as
those
of
Patroclus.
Paris
was
later
killed
by
Philoctetes
using
the
enormous
bow
of
Heracles.
Fate
of
Achilles'
armor.
Achilles'
armor
was
the
object
of
a
feud
between
Odysseus
and
Telamonian
Ajax
(Ajax
the
greater).
They
competed
for
it
by
giving
speeches
on
why
they
were
the
bravest
after
Achilles
to
their
Trojan
prisoners,
who
after
considering
both
men
came
to
a
consensus
in
favor
of
Odysseus.
Furious,
Ajax
cursed
Odysseus,
which
earned
the
ire
of
Athena.
Athena
temporarily
made
Ajax
so
mad
with
grief
and
anguish
that
he
began
killing
sheep,
thinking
they
were
his
comrades.
After
a
while,
when
Athena
lifted
his
madness
and
Ajax
realized
that
he
had
actually
been
killing
sheep,
he
was
so
embarrassed
that
he
committed
suicide.
Odysseus
eventually
gave
the
armor
to
Neoptolemus,
the
son
of
Achilles.
A
relic
claimed
to
be
Achilles'
bronze-headed
spear
was
for
centuries
preserved
in
the
temple
of
Athena
on
the
acropolis
of
Phaselis,
Lycia,
a
port
on
the
Pamphylian
Gulf.
The
city
was
visited
in
333
BC
by
Alexander
the
Great,
who
envisioned
himself
as
the
new
Achilles
and
carried
the
"Iliad"
with
him,
but
his
court
biographers
do
not
mention
the
spear,
which
he
would
indeed
have
touched
with
excitement.
But
it
was
being
shown
in
the
time
of
Pausanias
in
the
second
century
AD.
Achilles
and
Patroclus.
Achilles'
relationship
with
Patroclus
is
a
key
aspect
of
his
myth.
Its
exact
nature
has
been
a
subject
of
dispute
in
both
the
classical
period
and
modern
times.
In
the
"Iliad",
they
appeared
to
be
generally
portrayed
as
a
model
of
deep
and
loyal
friendship.
However,
commentators
from
the
classical
period
to
today
have
tended
to
interpret
the
relationship
through
the
lens
of
their
own
cultures.
Thus,
in
5th
century
BC
Athens
the
relationship
was
commonly
interpreted
as
pederastic.
Contemporary
readers
may
interpret
the
two
heroes
either
as
relatives
or
close
friends,
as
"war
buddies,"
as
being
in
a
teacher/student
relationship,
or
in
love
with
each
other
as
an
egalitarian
homosexual
couple.
Whichever
the
case
may
be,
Achilles
nevertheless
continued
to
have
sexual
relationships
with
women.
The
cult
of
Achilles
in
antiquity.
There
was
an
archaic
heroic
cult
of
Achilles
on
the
White
Island,
"Leuce",
in
the
Black
Sea
off
the
modern
coasts
of
Romania
and
Ukraine,
with
a
temple
and
an
oracle
which
survived
into
the
Roman
period.
In
the
lost
epic
"Aithiopis",
a
continuation
of
the
"Iliad"
attributed
to
Arktinus
of
Miletos,
Achilles’
mother
Thetis
returned
to
mourn
him
and
removed
his
ashes
from
the
pyre
and
took
them
to
Leuce
at
the
mouths
of
the
Danube.
There
the
Achaeans
raised
a
tumulus
for
him
and
celebrated
funeral
games.
Pliny's
Natural
History
(IV.27.1)
mentions
a
tumulus
that
is
no
longer
evident
("Insula
Akchillis
tumulo
eius
viri
clara"),
on
the
island
consecrated
to
him,
located
at
a
distance
of
fifty
Roman
miles
from
Peuce
by
the
Danube
Delta,
and
the
temple
there.
Pausanias
has
been
told
that
the
island
is
"covered
with
forests
and
full
of
animals,
some
wild,
some
tame.
In
this
island
there
is
also
Achilles’
temple
and
his
statue”
(III.19.11).
Ruins
of
a
square
temple
30
meters
to
a
side,
possibly
that
dedicated
to
Achilles,
were
discovered
by
Captain
Kritzikly
in
1823,
but
there
has
been
no
modern
archeological
work
done
on
the
island.
Pomponius
Mela
tells
that
Achilles
is
buried
in
the
island
named
Achillea,
between
Boristhene
and
Ister
("De
situ
orbis",
II,
7).
And
the
Greek
geographer
Dionysius
Periegetus
of
Bithynia,
who
lived
at
the
time
of
Domitian,
writes
that
the
island
was
called
"Leuce"
"because
the
wild
animals
which
live
there
are
white.
It
is
said
that
there,
in
Leuce
island,
reside
the
souls
of
Achilles
and
other
heroes,
and
that
they
wander
through
the
uninhabited
valleys
of
this
island;
this
is
how
Jove
rewarded
the
men
who
had
distinguished
themselves
through
their
virtues,
because
through
virtue
they
had
acquired
everlasting
honor”
("Orbis
descriptio",
v.
541,
quoted
in
Densuşianu
1913).
The
"Periplus
of
the
Euxine
Sea"
gives
the
following
details:
"It
is
said
that
the
goddess
Thetis
raised
this
island
from
the
sea,
for
her
son
Achilles,
who
dwells
there.
Here
is
his
temple
and
his
statue,
an
archaic
work.
This
island
is
not
inhabited,
and
goats
graze
on
it,
not
many,
which
the
people
who
happen
to
arrive
here
with
their
ships,
sacrifice
to
Achilles.
In
this
temple
are
also
deposited
a
great
many
holy
gifts,
craters,
rings
and
precious
stones,
offered
to
Achilles
in
gratitude.
One
can
still
read
inscriptions
in
Greek
and
Latin,
in
which
Achilles
is
praised
and
celebrated.
Some
of
these
are
worded
in
Patroclus’
honor,
because
those
who
wish
to
be
favored
by
Achilles,
honor
Patroclus
at
the
same
time.
There
are
also
in
this
island
countless
numbers
of
sea
birds,
which
look
after
Achilles’
temple.
Every
morning
they
fly
out
to
sea,
wet
their
wings
with
water,
and
return
quickly
to
the
temple
and
sprinkle
it.
And
after
they
finish
the
sprinkling,
they
clean
the
hearth
of
the
temple
with
their
wings.
Other
people
say
still
more,
that
some
of
the
men
who
reach
this
island,
come
here
intentionally.
They
bring
animals
in
their
ships,
destined
to
be
sacrificed.
Some
of
these
animals
they
slaughter,
others
they
set
free
on
the
island,
in
Achilles’
honor.
But
there
are
others,
who
are
forced
to
come
to
this
island
by
sea
storms.
As
they
have
no
sacrificial
animals,
but
wish
to
get
them
from
the
god
of
the
island
himself,
they
consult
Achilles’
oracle.
They
ask
permission
to
slaughter
the
victims
chosen
from
among
the
animals
that
graze
freely
on
the
island,
and
to
deposit
in
exchange
the
price
which
they
consider
fair.
But
in
case
the
oracle
denies
them
permission,
because
there
is
an
oracle
here,
they
add
something
to
the
price
offered,
and
if
the
oracle
refuses
again,
they
add
something
more,
until
at
last,
the
oracle
agrees
that
the
price
is
sufficient.
And
then
the
victim
doesn’t
run
away
any
more,
but
waits
willingly
to
be
caught.
So,
there
is
a
great
quantity
of
silver
there,
consecrated
to
the
hero,
as
price
for
the
sacrificial
victims.
To
some
of
the
people
who
come
to
this
island,
Achilles
appears
in
dreams,
to
others
he
would
appear
even
during
their
navigation,
if
they
were
not
too
far
away,
and
would
instruct
them
as
to
which
part
of
the
island
they
would
better
anchor
their
ships”.
(quoted
in
Densuşianu)
The
heroic
cult
of
Achilles
on
Leuce
island
was
widespread
in
antiquity,
not
only
along
the
sea
lanes
of
the
Pontic
Sea
but
also
in
maritime
cities
whose
economic
interests
were
tightly
connected
to
the
riches
of
the
Black
Sea.
Achilles
from
Leuce
island
was
venerated
as
"Pontarches"
the
lord
and
master
of
the
Pontic
(Black)
Sea,
the
protector
of
sailors
and
navigation.
Sailors
went
out
of
their
way
to
offer
sacrifice.
To
Achilles
of
Leuce
were
dedicated
a
number
of
important
commercial
port
cities
of
the
Greek
waters:
Achilleion
in
Messenia
(Stephanus
Byzantinus),
Achilleios
in
Laconia
(Pausanias,
III.25,4)
Nicolae
Densuşianu
(Densuşianu
1913)
even
thought
he
recognized
Achilles
in
the
name
of
Aquileia
and
in
the
north
arm
of
the
Danube
delta,
the
arm
of
Chilia
("Achileii"),
though
his
conclusion,
that
Leuce
had
sovereign
rights
over
Pontos,
evokes
modern
rather
than
archaic
sea-law."
Leuce
had
also
a
reputation
as
a
place
of
healing.
Pausanias
(III.19,13)
reports
that
the
Delphic
Pythia
sent
a
lord
of
Croton
to
be
cured
of
a
chest
wound.
Ammianus
Marcellinus
(XXII.8)
attributes
the
healing
to
waters
("aquae")
on
the
island.
The
cult
of
Achilles
in
modern
times:
The
Achilleion
in
Corfu.
In
the
region
of
Gastouri
(Γαστούρι)
to
the
south
of
the
city
of
Corfu
Greece,
Empress
of
Austria
Elisabeth
of
Bavaria
also
known
as
Sissi
built
in
1890
a
summer
palace
with
Achilles
as
its
central
theme
and
it
is
a
monument
to
platonic
romanticism.
The
palace,
naturally,
was
named
after
Achilles:
"Achilleion"
(Αχίλλειον).
This
elegant
structure
abounds
with
paintings
and
statues
of
Achilles
both
in
the
main
hall
and
in
the
lavish
gardens
depicting
the
heroic
and
tragic
scenes
of
the
Trojan
war.
The
name
of
Achilles.
Achilles'
name
can
be
analyzed
as
a
combination
of
("akhos")
"grief"
and
("Laos")
"a
people,
tribe,
nation,
etc."
In
other
words,
Achilles
is
an
embodiment
of
the
grief
of
the
people,
grief
being
a
theme
raised
numerous
times
in
the
"Iliad"
(frequently
by
Achilles).
Achilles'
role
as
the
hero
of
grief
forms
an
ironic
juxtaposition
with
the
conventional
view
of
Achilles
as
the
hero
of
"kleos"
(glory,
usually
glory
in
war).
"Laos"
has
been
construed
by
Gregory
Nagy,
following
Leonard
Palmer,
to
mean
"a
corps
of
soldiers",
a
muster.
With
this
derivation,
the
name
would
have
a
double
meaning
in
the
poem:
When
the
hero
is
functioning
rightly,
his
men
bring
grief
to
the
enemy,
but
when
wrongly,
his
men
get
the
grief
of
war.
The
poem
is
in
part
about
the
misdirection
of
anger
on
the
part
of
leadership.
The
name
Achilleus
was
a
common
and
attested
name
among
the
Greeks
early
after
7th
century
BC.
It
was
also
turned
into
the
female
form
of
Ἀχιλλεία,
"Achilleía",
firstly
attested
in
Attica,4th
century
BC,
(IG
II²
1617)
and
Achillia,
as
the
name
of
a
female
gladiator
fighting,
'Amazonia'.
Roman
gladiatorial
games
often
referenced
classical
mythology
and
this
seems
to
reference
Achilles'
fight
with
Penthesilea,
but
give
it
an
extra
twist
of
Achilles
being
'played'
by
a
woman.
Other
stories
about
Achilles.
Some
post-Homeric
sources
claim
that
in
order
to
keep
Achilles
safe
from
the
war,
Thetis
(or,
in
some
versions,
Peleus)
hides
the
young
man
at
the
court
of
Lycomedes,
king
of
Skyros.
There,
Achilles
is
disguised
as
a
girl
and
lives
among
Lycomedes'
daughters,
perhaps
under
the
name
"Pyrrha"
(the
red-haired
girl).
With
Lycomedes'
daughter
Deidamia,
whom
in
the
account
of
Statius
he
rapes,
Achilles
there
fathers
a
son,
Neoptolemus
(also
called
Pyrrhus,
after
his
father's
possible
alias).
According
to
this
story,
Odysseus
learns
from
the
prophet
Calchas
that
the
Achaeans
would
be
unable
to
capture
Troy
without
Achilles'
aid.
Odysseus
goes
to
Skyros
in
the
guise
of
a
peddler
selling
women's
clothes
and
jewelry
and
places
a
shield
and
spear
among
his
goods.
When
Achilles
instantly
takes
up
the
spear,
Odysseus
sees
through
his
disguise
and
convinces
him
to
join
the
Greek
campaign.
In
another
version
of
the
story,
Odysseus
arranges
for
a
trumpet
alarm
to
be
sounded
while
he
was
with
Lycomedes'
women;
while
the
women
flee
in
panic,
Achilles
prepares
to
defend
the
court,
thus
giving
his
identity
away.
In
book
11
of
Homer's
"Odyssey,"
Odysseus
sails
to
the
underworld
and
converses
with
the
shades.
One
of
these
is
Achilles,
who
when
greeted
as
"blessed
in
life,
blessed
in
death",
responds
that
he
would
rather
be
a
slave
to
the
worst
of
masters
than
be
king
of
all
the
dead.
But
Achilles
then
asks
Odysseus
of
his
son's
exploits
in
the
Trojan
war,
and
when
Odysseus
tells
of
Neoptolemus'
heroic
actions,
Achilles
is
filled
with
satisfaction.
This
leaves
the
reader
with
an
ambiguous
understanding
of
how
Achilles
felt
about
the
heroic
life.
Achilles
was
worshipped
as
a
sea-god
in
many
of
the
Greek
colonies
on
the
Black
Sea,
the
location
of
the
mythical
"White
Island"
which
he
was
said
to
inhabit
after
his
death,
together
with
many
other
heroes.
The
kings
of
the
Epirus
claimed
to
be
descended
from
Achilles
through
his
son,
Neoptolemus.
Alexander
the
Great,
son
of
the
Epiran
princess
Olympias,
could
therefore
also
claim
this
descent,
and
in
many
ways
strove
to
be
like
his
great
ancestor;
he
is
said
to
have
visited
his
tomb
while
passing
Troy.
Achilles
fought
and
killed
the
Amazon
Helene.
Some
also
said
he
married
Medea,
and
that
after
both
their
deaths
they
were
united
in
the
Elysian
Fields
of
Hades —
as
Hera
promised
Thetis
in
Apollonius'
Argonautica.
In
some
versions
of
the
myth,
Achilles
has
a
relationship
with
his
captive
Briseis.
Achilles
in
Greek
tragedy.
The
Greek
tragedian
Aeschylus
wrote
a
trilogy
of
plays
about
Achilles,
given
the
title
"Achilleis"
by
modern
scholars.
The
tragedies
relate
the
deeds
of
Achilles
during
the
Trojan
War,
including
his
defeat
of
Hector
and
eventual
death
when
an
arrow
shot
by
Paris
and
guided
by
Apollo
punctures
his
heel.
Extant
fragments
of
the
"Achilleis"
and
other
Aeschylean
fragments
have
been
assembled
to
produce
a
workable
modern
play.
The
first
part
of
the
"Achilleis"
trilogy,
"The
Myrmidons",
focused
on
the
relationship
between
Achilles
and
chorus,
who
represent
the
Achaean
army
and
try
to
convince
Achilles
to
give
up
his
quarrel
with
Agamemnon;
only
a
few
lines
survive
today.
The
tragedian
Sophocles
also
wrote
a
play
with
Achilles
as
the
main
character,
"The
Lovers
of
Achilles".
Only
a
few
fragments
survive.
Achilles
in
Greek
philosophy.
The
philosopher
Zeno
of
Elea
centered
one
of
his
paradoxes
on
an
imaginary
footrace
between
"swift-footed"
Achilles
and
a
tortoise,
by
which
he
attempted
to
show
that
Achilles
could
not
catch
up
to
a
tortoise
with
a
head
start,
and
therefore
that
motion
and
change
were
impossible.
As
a
student
of
the
monist
Parmenides
and
a
member
of
the
Eleatic
school,
Zeno
believed
time
and
motion
to
be
illusions.
Music.
Achilles
has
frequently
been
mentioned
in
music.
Quotes.
"If
Achilles
was
anything,
he
was
a
man
who
believed
his
own
press
releases."
—Roger
Ebert,
commenting
on
the
classical
depiction
of
Achilles'
character
and
personality.
Bibliography.
Thomas
Bullfinch,
Myths
of
Greek
and
Rome
---END.OF.DOCUMENT---
Abraham
Lincoln.
Abraham
Lincoln
(February
12,
1809
–
April
15,
1865)
served
as
the
16th
President
of
the
United
States
from
March
1861
until
his
assassination
in
April
1865.
He
successfully
led
his
country
through
its
greatest
internal
crisis,
the
American
Civil
War,
preserving
the
Union
and
ending
slavery.
Before
his
election
in
1860
as
the
first
Republican
president,
Lincoln
had
been
a
country
lawyer,
an
Illinois
state
legislator,
a
member
of
the
United
States
House
of
Representatives,
and
twice
an
unsuccessful
candidate
for
election
to
the
U.S.
Senate.
As
an
outspoken
opponent
of
the
expansion
of
slavery
in
the
United
States,
Lincoln
won
the
Republican
Party
nomination
in
1860
and
was
elected
president
later
that
year.
His
tenure
in
office
was
occupied
primarily
with
the
defeat
of
the
secessionist
Confederate
States
of
America
in
the
American
Civil
War.
He
introduced
measures
that
resulted
in
the
abolition
of
slavery,
issuing
his
Emancipation
Proclamation
in
1863
and
promoting
the
passage
of
the
Thirteenth
Amendment
to
the
Constitution.
Six
days
after
the
large-scale
surrender
of
Confederate
forces
under
General
Robert
E.
Lee,
Lincoln
became
the
first
American
president
to
be
assassinated.
Lincoln
had
closely
supervised
the
victorious
war
effort,
especially
the
selection
of
top
generals,
including
Ulysses
S.
Grant.
Historians
have
concluded
that
he
handled
the
factions
of
the
Republican
Party
well,
bringing
leaders
of
each
faction
into
his
cabinet
and
forcing
them
to
cooperate.
Lincoln
successfully
defused
the
"Trent"
affair,
a
war
scare
with
Britain
late
in
1861.
Under
his
leadership,
the
Union
took
control
of
the
border
slave
states
at
the
start
of
the
war.
Additionally,
he
managed
his
own
reelection
in
the
1864
presidential
election.
Copperheads
and
other
opponents
of
the
war
criticized
Lincoln
for
refusing
to
compromise
on
the
slavery
issue.
Conversely,
the
Radical
Republicans,
an
abolitionist
faction
of
the
Republican
Party,
criticized
him
for
moving
too
slowly
in
abolishing
slavery.
Even
with
these
opponents,
Lincoln
successfully
rallied
public
opinion
through
his
rhetoric
and
speeches;
his
Gettysburg
Address
(1863)
became
an
iconic
symbol
of
the
nation's
duty.
At
the
close
of
the
war,
Lincoln
held
a
moderate
view
of
Reconstruction,
seeking
to
speedily
reunite
the
nation
through
a
policy
of
generous
reconciliation.
Lincoln
has
consistently
been
ranked
by
scholars
as
one
of
the
greatest
of
all
U.S.
Presidents.
Childhood
and
education.
Abraham
Lincoln
was
born
on
February
12,
1809,
to
Thomas
Lincoln
and
Nancy
Hanks,
two
farmers,
in
a
one-room
log
cabin
on
the
Sinking
Spring
Farm,
in
southeast
Hardin
County,
Kentucky
(now
part
of
LaRue
County),
making
him
the
first
president
born
in
the
west.
Lincoln
was
not
given
a
middle
name.
His
ancestor
Samuel
Lincoln
had
arrived
in
Hingham,
Massachusetts
from
England
in
the
17th
century.
His
grandfather,
also
named
Abraham
Lincoln,
had
moved
to
Kentucky,
where
he
owned
over,
and
was
ambushed
and
killed
by
an
Indian
raid
in
1786.
Thomas
Lincoln
was
a
respected
citizen
of
rural
Kentucky.
He
owned
several
farms,
including
the
Sinking
Spring
Farm,
although
he
was
not
wealthy.
The
family
belonged
to
a
Separate
Baptists
church,
which
had
high
moral
standards
frowning
on
alcohol
consumption
and
dancing,
and
many
church
members
were
opposed
to
slavery.
Abraham
himself
never
joined
their
church,
or
any
other
church.
In
1816,
the
Lincoln
family
left
Kentucky
to
avoid
the
expense
of
fighting
for
one
of
their
properties
in
court,
and
made
a
new
start
in
Perry
County,
Indiana
(now
in
Spencer
County).
Lincoln
later
noted
that
this
move
was
"partly
on
account
of
slavery",
and
partly
because
of
difficulties
with
land
deeds
in
Kentucky.
Abraham's
father
disapproved
of
slavery
on
religious
grounds
and
it
was
hard
to
compete
economically
with
farms
operated
by
slaves.
Unlike
land
in
the
Northwest
Territory,
Kentucky
never
had
a
proper
U.S.
survey,
and
farmers
often
had
difficulties
proving
title
to
their
property.
When
Lincoln
was
nine,
his
mother,
then
34
years
old,
died
of
milk
sickness.
Soon
afterwards,
his
father
remarried,
to
Sarah
Bush
Johnston.
Lincoln
and
his
stepmother
were
close;
he
called
her
"Mother"
for
the
rest
of
his
life,
but
he
became
increasingly
distant
from
his
father.
Abraham
felt
his
father
was
not
a
success,
and
did
not
want
to
be
like
him.
In
later
years,
he
would
occasionally
lend
his
father
money.
In
1830,
fearing
a
milk
sickness
outbreak,
the
family
settled
on
public
land
in
Macon
County,
Illinois.
The
next
year,
when
his
father
relocated
the
family
to
a
new
homestead
in
Coles
County,
Illinois,
22-year-old
Lincoln
struck
out
on
his
own,
canoeing
down
the
Sangamon
River
to
the
village
of
New
Salem
in
Sangamon
County.
Later
that
year,
hired
by
New
Salem
businessman
Denton
Offutt
and
accompanied
by
friends,
he
took
goods
from
New
Salem
to
New
Orleans
via
flatboat
on
the
Sangamon,
Illinois
and
Mississippi
rivers.
Lincoln's
formal
education
consisted
of
about
18
months
of
schooling;
but
he
was
an
avid
reader
and
largely
self-educated.
He
was
also
skilled
with
an
axe
and
a
talented
local
wrestler,
the
latter
of
which
helped
give
him
self-confidence.
Lincoln
avoided
hunting
and
fishing
because
he
did
not
like
killing
animals,
even
for
food.
Marriage
and
family.
Lincoln's
first
love
was
Ann
Rutledge.
He
met
her
when
he
first
moved
to
New
Salem,
and
by
1835
they
had
reached
a
romantic
understanding.
Rutledge,
however,
died
on
August
25,
probably
of
typhoid
fever.
Earlier,
in
either
1833
or
1834,
he
had
met
Mary
Owens,
the
sister
of
his
friend
Elizabeth
Abell,
when
she
was
visiting
from
her
home
in
Kentucky.
Late
in
1836,
Lincoln
agreed
to
a
match
proposed
by
Elizabeth
between
him
and
her
sister,
if
Mary
ever
returned
to
New
Salem.
Mary
did
return
in
November
1836
and
Lincoln
courted
her
for
a
time;
however
they
both
had
second
thoughts
about
their
relationship.
On
August
16,
1837,
Lincoln
wrote
Mary
a
letter
from
Springfield,
to
which
he
had
moved
that
April
to
begin
his
law
practice,
suggesting
he
would
not
blame
her
if
she
ended
the
relationship.
She
never
replied,
and
the
courtship
was
over.
In
1840,
Lincoln
became
engaged
to
Mary
Todd,
from
a
wealthy
slaveholding
family
based
in
Lexington,
Kentucky.
They
met
in
Springfield
in
December
1839,
and
were
engaged
sometime
around
that
Christmas.
A
wedding
was
set
for
January
1,
1841,
but
the
couple
split
as
the
wedding
approached.
They
later
met
at
a
party,
and
then
married
on
November
4,
1842,
in
the
Springfield
mansion
of
Mary's
married
sister.
In
1844,
the
couple
bought
a
house
on
Eighth
and
Jackson
in
Springfield,
near
Lincoln's
law
office.
The
Lincolns
soon
had
a
budding
family,
with
the
birth
of
son
Robert
Todd
Lincoln
in
Springfield,
Illinois
on
August
1,
1843,
and
second
son
Edward
Baker
Lincoln
on
March
10,
1846,
also
in
Springfield.
According
to
a
house
girl,
Abraham
"was
remarkably
fond
of
children".
The
Lincolns
did
not
believe
in
strict
rules
and
tight
boundaries
when
it
came
to
their
children.
Robert,
however,
would
be
the
only
one
of
the
Lincolns'
children
to
survive
into
adulthood.
Edward
Lincoln
died
on
February
1,
1850
in
Springfield,
likely
of
tuberculosis.
The
Lincolns'
grief
over
this
loss
was
somewhat
assuaged
by
the
birth
of
William
"Willie"
Wallace
Lincoln
nearly
eleven
months
later,
on
December
21.
But
Willie
himself
died
of
a
fever
at
the
age
of
eleven
on
February
20,
1862,
in
Washington,
D.C.,
during
President
Lincoln's
first
term.
The
Lincolns'
fourth
son
Thomas
"Tad"
Lincoln
was
born
on
April
4,
1853,
and,
although
he
outlived
his
father,
died
at
the
age
of
eighteen
on
July
16,
1871
in
Chicago.
Robert
Lincoln
eventually
went
on
to
attend
Phillips
Exeter
Academy
and
Harvard
College.
His
(and
by
extension,
his
father's)
last
known
lineal
descendant,
Robert
Todd
Lincoln
Beckwith,
died
December
24,
1985.
The
death
of
the
Lincolns'
sons
had
profound
effects
on
both
Abraham
and
Mary.
Later
in
life,
Mary
Todd
Lincoln
found
herself
unable
to
cope
with
the
stresses
of
losing
her
husband
and
sons,
and
this
(in
conjunction
with
what
some
historians
consider
to
have
been
pre-existing
bipolar
disorder
)
eventually
led
Robert
Lincoln
to
involuntarily
commit
her
to
a
mental
health
asylum
in
1875.
Abraham
Lincoln
himself
was
contemporaneously
described
as
suffering
from
"melancholy"
throughout
his
legal
and
political
life,
a
condition
which
modern
mental
health
professionals
would
now
typically
characterize
as
clinical
depression.
Early
political
career
and
military
service.
Lincoln
began
his
political
career
in
March
1832
at
age
23
when
he
announced
his
candidacy
for
the
Illinois
General
Assembly.
He
was
esteemed
by
the
residents
of
New
Salem,
but
he
didn't
have
an
education,
powerful
friends,
or
money.
The
centerpiece
of
his
platform
was
the
undertaking
of
navigational
improvements
on
the
Sangamon
River.
Before
the
election
he
served
as
a
captain
in
a
company
of
the
Illinois
militia
during
the
Black
Hawk
War,
although
he
never
saw
combat.
Lincoln
returned
from
the
militia
after
a
few
months
and
was
able
to
campaign
throughout
the
county
before
the
August
6
election.
At,
he
was
tall
and
"strong
enough
to
intimidate
any
rival."
At
his
first
political
speech,
he
grabbed
a
man
accosting
a
supporter
by
his
"neck
and
the
seat
of
his
trousers",
and
threw
him.
When
the
votes
were
counted,
Lincoln
finished
eighth
out
of
thirteen
candidates
(only
the
top
four
were
elected),
but
he
did
manage
to
secure
277
out
of
the
300
votes
cast
in
the
New
Salem
precinct.
In
1834,
he
won
an
election
to
the
state
legislature.
He
was
labeled
a
Whig,
but
ran
a
bipartisan
campaign.
He
then
decided
to
become
a
lawyer,
and
began
teaching
himself
law
by
reading
"Commentaries
on
the
Laws
of
England".
Admitted
to
the
bar
in
1837,
he
moved
to
Springfield,
Illinois,
that
April,
and
began
to
practice
law
with
John
T.
Stuart,
Mary
Todd's
cousin,
who
let
Lincoln
have
the
run
of
his
law
library
while
studying
to
be
a
lawyer.
With
a
reputation
as
a
formidable
adversary
during
cross-examinations
and
closing
arguments,
Lincoln
became
an
able
and
successful
lawyer.
In
1841,
Lincoln
entered
law
practice
with
William
Herndon,
whom
Lincoln
thought
"a
studious
young
man".
He
served
four
successive
terms
in
the
Illinois
House
of
Representatives
as
a
representative
from
Sangamon
County,
affiliated
with
the
Whig
party.
In
1837,
he
and
another
legislator
declared
that
slavery
was
"founded
on
both
injustice
and
bad
policy"
the
first
time
he
had
publicly
opposed
slavery.
In
the
1835–1836
legislative
session
he'd
voted
to
restrict
suffrage
to
whites
only.
He
would
later
say
that
he
had
been
against
slavery
since
he
was
a
boy,
but
being
labelled
an
abolitionist
was
"political
suicide"
in
Sangamon
County
in
those
years,
and
so
he
chose
his
words
carefully
when
discussing
the
issue
publicly.
National
politics.
Lincoln
was
a
Whig,
and
since
the
early
1830s
had
strongly
admired
the
policies
and
leadership
of
Henry
Clay.
"I
have
always
been
an
old-line
Henry
Clay
Whig"
he
professed
to
friends
in
1861.
The
party
favored
economic
expansion
such
as
improving
roads
and
increasing
trade.
In
1846,
Lincoln
was
elected
to
the
U.S.
House
of
Representatives,
where
he
served
one
two-year
term.
As
a
House
member,
Lincoln
was
a
dedicated
Whig,
showing
up
for
most
votes
and
giving
speeches
that
echoed
the
party
line.
He
used
his
office
as
an
opportunity
to
speak
out
against
the
Mexican–American
War,
which
he
attributed
to
President
Polk's
desire
for
"military
glory
—
that
attractive
rainbow,
that
rises
in
showers
of
blood".
Lincoln's
main
stand
against
Polk
occurred
in
his
Spot
Resolutions:
The
war
had
begun
with
a
violent
confrontation
on
territory
disputed
by
Mexico
and
Texas,
but
as
Lincoln
pointed
out,
Polk
had
insisted
that
Mexican
soldiers
had
"invaded
"our
territory"
and
shed
the
blood
of
our
fellow-citizens
on
our
"own
soil".
Lincoln
demanded
that
Polk
show
Congress
the
exact
spot
on
which
blood
had
been
shed,
and
proof
that
that
spot
was
on
American
soil.
Congress
never
enacted
the
resolution
or
even
debated
it,
and
its
introduction
resulted
in
a
loss
of
political
support
for
Lincoln
in
his
district;
one
Illinois
newspaper
derisively
nicknamed
him
"spotty
Lincoln."
Despite
his
admiration
for
Henry
Clay,
Lincoln
was
a
key
early
supporter
of
Zachary
Taylor's
candidacy
for
the
1848
presidential
election.
When
Lincoln's
term
ended,
the
incoming
Taylor
administration
offered
him
the
governorship
of
the
Oregon
Territory.
The
territory
leaned
heavily
Democratic,
and
Lincoln
doubted
they
would
elect
him
as
governor
or
as
a
senator
after
they
were
admitted
to
the
union,
so
he
returned
to
Springfield.
Prairie
lawyer.
Back
in
Springfield,
Lincoln
turned
most
of
his
energies
to
making
a
living
practicing
law,
handling
"every
kind
of
business
that
could
come
before
a
prairie
lawyer."
He
"rode
the
circuit"--that
is,
appeared
in
county
seats
in
the
mid-state
region
when
the
county
courts
were
in
session.
His
reputation
grew
and
he
appeared
before
the
Supreme
Court
of
the
United
States,
arguing
a
case
involving
a
canal
boat
that
sank
after
hitting
a
bridge.
Lincoln
represented
numerous
transportation
interests,
such
as
the
river
barges
and
the
railroads.
As
a
riverboat
man,
Lincoln
had
initially
favored
riverboat
interests,
but
ultimately
he
represented
whoever
hired
him.
In
1849,
he
had
received
a
patent
for
a
"device
to
buoy
vessels
over
shoals".
Lincoln's
goal
had
been
to
lessen
the
draft
of
a
river
craft
by
pushing
horizontal
floats
into
the
water
alongside
the
hull.
The
floats
would
have
served
as
temporary
ballast
tanks.
The
idea
was
never
commercialized,
but
Lincoln
is
still
the
only
person
to
hold
a
patent
and
serve
as
President
of
the
United
States.
In
1851,
he
represented
the
Alton
&
Sangamon
Railroad
in
a
dispute
with
one
of
its
shareholders,
James
A.
Barret,
who
had
refused
to
pay
the
balance
on
his
pledge
to
the
railroad
on
the
grounds
that
it
had
changed
its
originally
planned
route.
Lincoln
argued
that
as
a
matter
of
law
a
corporation
is
not
bound
by
its
original
charter
when
that
charter
can
be
amended
in
the
public
interest,
that
the
newer
proposed
Alton
&
Sangamon
route
was
superior
and
less
expensive,
and
that
accordingly
the
corporation
had
a
right
to
sue
Mr.
Barret
for
his
delinquent
payment.
He
won
this
case,
and
the
decision
by
the
Illinois
Supreme
Court
was
eventually
cited
by
25
other
courts
throughout
the
United
States.
Lincoln
appeared
in
front
of
the
Illinois
Supreme
Court
175
times,
51
times
as
sole
counsel,
of
which,
31
were
decided
in
his
favor.
Lincoln's
most
notable
criminal
trial
came
in
1858
when
he
defended
William
"Duff"
Armstrong,
who
was
on
trial
for
the
murder
of
James
Preston
Metzker.
The
case
is
famous
for
Lincoln's
use
of
judicial
notice
to
show
an
eyewitness
had
lied
on
the
stand.
After
the
witness
testified
to
having
seen
the
crime
in
the
moonlight,
Lincoln
produced
a
Farmers'
Almanac
to
show
that
the
moon
on
that
date
was
at
such
a
low
angle
it
could
not
have
produced
enough
illumination
to
see
anything
clearly.
Based
on
this
evidence,
Armstrong
was
acquitted.
Republican
politics
1854–1860.
Lincoln
returned
to
politics
in
response
to
the
Kansas-Nebraska
Act
(1854),
which
expressly
repealed
the
limits
on
slavery's
extent
as
established
by
the
Missouri
Compromise
(1820).
Illinois
Democrat
Stephen
A.
Douglas,
the
most
powerful
man
in
the
Senate,
proposed
popular
sovereignty
as
the
solution
to
the
slavery
impasse,
and
incorporated
it
into
the
Kansas–Nebraska
Act.
Douglas
argued
that
in
a
democracy
the
people
should
have
the
right
to
decide
whether
to
allow
slavery
in
their
territory,
rather
than
have
such
a
decision
imposed
on
them
by
the
national
Congress.
In
the
October
16,
1854,
"Peoria
Speech",
Lincoln
outlined
his
position
on
slavery
that
he
would
repeat
over
the
next
six
years
on
the
route
to
the
presidency.
According
to
a
newspaper
account
of
the
speech,
Lincoln
spoke
with
"a
thin
high-pitched
falsetto
voice
of
much
carrying
power,
that
could
be
heard
a
long
distance
in
spite
of
the
hustle
and
bustle
of
the
crowd...
[with]
the
accent
and
pronunciation
peculiar
to
his
native
state,
Kentucky."
In
late
1854,
Lincoln
decided
to
run
for
the
United
States
Senate
as
a
Whig.
Despite
leading
in
the
first
six
rounds
of
voting
in
the
state
legislature,
Lincoln
instructed
his
backers
to
vote
for
Lyman
Trumbull
to
prevent
pro-Nebraska
candidate
Joel
Aldrich
Matteson
from
winning.
Trumbull
beat
Matteson
in
the
tenth
round
of
voting.
The
Whigs
had
been
irreparably
split
by
the
Kansas-Nebraska
Act.
"I
think
I
am
a
Whig,
but
others
say
there
are
not
Whigs,
and
I
am
an
abolitionist,
even
though
I
do
no
more
than
oppose
the
expansion
of
slavery"
he
said.
Drawing
on
remnants
of
the
old
Whig
party,
and
on
disenchanted
Free
Soil,
Liberty,
and
Democratic
party
members,
he
was
instrumental
in
forging
the
shape
of
the
new
Republican
Party.
At
the
Republican
convention
in
1856,
Lincoln
placed
second
in
the
contest
to
become
the
party's
candidate
for
Vice-President.
In
1857–58,
Douglas
broke
with
President
Buchanan,
leading
to
a
fight
for
control
of
the
Democratic
Party.
Some
eastern
Republicans
even
favored
the
reelection
of
Douglas
in
1858,
since
he
had
led
the
opposition
to
the
Lecompton
Constitution,
which
would
have
admitted
Kansas
as
a
slave
state.
Accepting
the
Republican
nomination
for
Senate
in
1858,
Lincoln
delivered
his
famous
speech:
"'A
house
divided
against
itself
cannot
stand.'(Mark
3:25)
I
believe
this
government
cannot
endure
permanently
half
slave
and
half
free.
I
do
not
expect
the
Union
to
be
dissolved
—
I
do
not
expect
the
house
to
fall
—
but
I
do
expect
it
will
cease
to
be
divided.
It
will
become
all
one
thing,
or
all
the
other."
The
speech
created
an
evocative
image
of
the
danger
of
disunion
caused
by
the
slavery
debate,
and
rallied
Republicans
across
the
north.
As
a
part
of
his
1860
presidential
campaign
strategy
Lincoln
acquired
through
banker
Jacob
Bunn,
in
May,
1859,
the
Illinois
Staats-Anzeiger,
a
German-language
newspaper
of
Springfield,
Illinois,
to
further
the
cause
of
Republican
Party
politics
among
the
German-speaking
community
of
the
region..
Lincoln–Douglas
debates
of
1858.
The
1858
campaign
featured
the
Lincoln–Douglas
debates,
generally
considered
the
most
famous
political
debate
in
American
history.
Lincoln
warned
that
"The
Slave
Power"
was
threatening
the
values
of
republicanism,
while
Stephen
A.
Douglas
emphasized
the
supremacy
of
democracy,
as
set
forth
in
his
Freeport
Doctrine,
which
said
that
local
settlers
should
be
free
to
choose
whether
to
allow
slavery
or
not
and
could
overrule
the
Supreme
Courts
Dred
Scott
v.
Sandford
decision.
Though
the
Republican
legislative
candidates
won
more
popular
votes,
the
Democrats
won
more
seats,
and
the
legislature
reelected
Douglas
to
the
Senate.
Nevertheless,
Lincoln's
definition
of
the
issues
gave
him
a
national
political
reputation.<ref»Carwardine,
p.
89-90
On
February
27,
1860,
New
York
party
leaders
invited
Lincoln
to
give
a
speech
at
Cooper
Union
to
group
of
powerful
Republicans.
In
one
of
the
most
important
speeches
of
his
career,
Lincoln
showed
that
he
was
a
contender
for
the
Republican's
presidential
nomination.
Journalist
Noah
Brooks
reported,
"No
man
ever
before
made
such
an
impression
on
his
first
appeal
to
a
New
York
audience."
1860
Presidential
election.
On
May
9–10,
1860,
the
Illinois
Republican
State
Convention
was
held
in
Decatur.
At
this
convention,
Lincoln
received
his
first
endorsement
to
run
for
the
presidency.
On
May
18,
at
the
1860
Republican
National
Convention
in
Chicago,
Lincoln
emerged
as
the
Republican
candidate
on
the
third
ballot,
beating
candidates
such
as
William
H.
Seward
and
Salmon
P.
Chase.
Why
Lincoln
won
the
nomination
has
been
subject
of
much
debate.
His
expressed
views
on
slavery
were
seen
as
more
moderate
than
those
of
rivals
Seward
and
Chase.
Some
feel
that
Seward
lost
more
than
Lincoln
won,
including
Seward
himself.
Others
attribute
it
to
luck,
and
the
fact
that
the
convention
was
held
in
Lincoln's
home
state.
Historian
Doris
Kearns
Goodwin
believes
the
real
reason
was
Lincoln's
skill
as
a
politician.
Most
Republicans
agreed
with
Lincoln
that
the
North
was
the
aggrieved
party
as
the
Slave
Power
tightened
its
grasp
on
the
national
government
with
the
Dred
Scott
decision
and
the
presidency
of
James
Buchanan.
Throughout
the
1850s
Lincoln
denied
that
there
would
ever
be
a
civil
war,
and
his
supporters
repeatedly
rejected
claims
that
his
election
would
incite
secession.
Meanwhile,
Douglas
was
selected
as
the
candidate
of
the
northern
Democrats,
with
Herschel
Vespasian
Johnson
as
the
vice-presidential
candidate.
Delegates
from
eleven
slave
states
walked
out
of
the
Democrat's
convention,
disagreeing
with
Douglas's
position
on
Popular
sovereignty,
and
ultimately
selected
John
C.
Breckinridge
as
their
candidate.
As
Douglas
stumped
the
country,
Lincoln
was
the
only
one
of
the
four
major
candidates
to
give
no
speeches
whatever.
Instead
he
monitored
the
campaign
closely
but
relied
on
the
enthusiasm
of
the
Republican
Party.
It
did
the
leg
work
that
produced
majorities
across
the
North.
It
produced
tons
of
campaign
posters
and
leaflets,
and
thousands
of
newspaper
editorials.
There
were
thousands
of
Republican
speakers
who
focused
first
on
the
party
platform,
and
second
on
Lincoln's
life
story,
emphasizing
his
childhood
poverty.
The
goal
was
to
demonstrate
the
superior
power
of
"free
labor",
whereby
a
common
farm
boy
could
work
his
way
to
the
top
by
his
own
efforts.
The
Republican
Party's
production
of
campaign
literature
dwarfed
the
combined
opposition.
A
"Chicago
Tribune"
writer
produced
a
pamphlet
that
detailed
Lincoln's
life,
and
sold
one
million
copies.
On
November
6,
1860,
Lincoln
was
elected
as
the
16th
President
of
the
United
States,
beating
Democrat
Stephen
A.
Douglas,
John
C.
Breckinridge
of
the
Southern
Democrats,
and
John
Bell
of
the
new
Constitutional
Union
Party.
He
was
the
first
Republican
president,
winning
entirely
on
the
strength
of
his
support
in
the
North:
he
was
not
even
on
the
ballot
in
ten
states
in
the
South,
and
won
only
two
of
996
counties
in
all
the
Southern
states.
Lincoln
received
1,866,452
votes,
Douglas
1,376,957
votes,
Breckinridge
849,781
votes,
and
Bell
588,789
votes.
The
electoral
vote
was
decisive:
Lincoln
had
180
and
his
opponents
added
together
had
only
123.
Turnout
was
82.2%,
with
Lincoln
winning
the
free
northern
states.
Douglas
won
Missouri,
and
split
New
Jersey
with
Lincoln.
Bell
won
Virginia,
Tennessee,
and
Kentucky,
and
Breckinridge
won
the
rest
of
the
South.
There
were
fusion
tickets
in
which
all
of
Lincoln's
opponents
combined
to
form
one
ticket
in
New
York,
New
Jersey
and
Rhode
Island,
but
even
if
the
anti-Lincoln
vote
had
been
combined
in
every
state,
Lincoln
still
would
have
won
because
he
would
still
have
had
a
majority
in
the
electoral
college.
Presidency
and
the
Civil
War.
With
the
emergence
of
the
Republicans
as
the
nation's
first
major
sectional
party
by
the
mid-1850s,
the
old
Second
Party
System
collapsed
and
a
realignment
created
the
Third
Party
System.
It
became
the
stage
on
which
sectional
tensions
were
played
out.
Although
little
of
the
West–the
focal
point
of
sectional
tensions–
was
fit
for
cotton
cultivation,
Southern
secessionists
read
the
political
fallout
as
a
sign
that
their
power
in
national
politics
was
rapidly
weakening.
The
slave
system
had
been
buttressed
by
the
Democratic
Party,
which
was
increasingly
seen
by
anti-slavery
elements
as
representing
a
more
pro-Southern
position
that
unfairly
permitted
the
Slave
Power
to
prevail
in
the
nation's
territories
and
to
dominate
national
policy
before
the
Civil
War.
Yet
the
Democrats
suffered
a
significant
reverse
in
the
electoral
realignment
of
the
mid-1850s;
they
lost
the
dominance
they
had
achieved
over
the
Whig
Party
and,
indeed,
were
the
minority
party
in
most
of
the
northern
states.
The
1854
election
was
a
Realigning
election
or
"critical
election"
that
saw
a
realignment
of
voting
patterns.
Abraham
Lincoln's
election
was
a
watershed
in
the
balance
of
power
of
competing
national
and
parochial
interests
and
affiliations.
Secession
winter
1860–1861.
As
Lincoln's
election
became
more
likely,
secessionists
made
clear
their
intent
to
leave
the
Union.
On
December
20,
1860,
South
Carolina
took
the
lead;
by
February
1,
1861,
Florida,
Mississippi,
Alabama,
Georgia,
Louisiana,
The
seven
states
soon
declared
themselves
to
be
a
new
nation,
the
Confederate
States
of
America.
The
upper
South
(Delaware,
Maryland,
Virginia,
North
Carolina,
Tennessee,
Kentucky,
Missouri,
and
Arkansas)
listened
to,
but
initially
rejected,
the
secessionist
appeal.
President
Buchanan
and
President-elect
Lincoln
refused
to
recognize
the
Confederacy.
Attempts
at
compromise,
such
as
the
Crittenden
Compromise
which
would
have
extended
the
Missouri
line
of
1820,
were
discussed.
Despite
support
for
the
Crittenden
Compromise
among
some
Republicans,
Lincoln
denounced
it
in
private
letters,
saying
"either
the
Missouri
line
extended,
or...
Pop.
Sov.
would
lose
us
everything
we
gained
in
the
election;
that
filibustering
for
all
South
of
us,
and
making
slave
states
of
it,
would
follow
in
spite
of
us,
under
either
plan",
while
other
Republicans
publicly
stated
it
"would
amount
to
a
perpetual
covenant
of
war
against
every
people,
tribe,
and
state
owning
a
foot
of
land
between
here
and
Tierra
del
Fuego."
The
Confederate
States
of
America
selected
Jefferson
Davis
on
February
9,
1861,
as
their
provisional
President.
President-elect
Lincoln
evaded
possible
assassins
in
Baltimore,
and
on
February
23,
1861,
arrived
in
disguise
in
Washington,
D.C.
At
his
inauguration
on
March
4,
1861,
sharpshooters
watched
the
inaugural
platform,
while
soldiers
on
horseback
patrolled
the
surrounding
area.
In
his
first
inaugural
address,
Lincoln
declared,
"I
hold
that
in
contemplation
of
universal
law
and
of
the
Constitution
the
Union
of
these
States
is
perpetual.
Perpetuity
is
implied,
if
not
expressed,
in
the
fundamental
law
of
all
national
governments,"
arguing
further
that
the
purpose
of
the
United
States
Constitution
was
"to
form
a
more
perfect
union"
than
the
Articles
of
Confederation
which
were
"explicitly"
perpetual,
thus
the
Constitution
too
was
perpetual.
He
asked
rhetorically
that
even
were
the
Constitution
a
simple
contract,
would
it
not
require
the
agreement
of
all
parties
to
rescind
it?
Also
in
his
inaugural
address,
in
a
final
attempt
to
reunite
the
states
and
prevent
certain
war,
Lincoln
supported
the
pending
Corwin
Amendment
to
the
Constitution,
which
had
passed
Congress
the
previous
day.
This
amendment,
which
explicitly
protected
slavery
in
those
states
in
which
it
already
existed,
was
considered
by
Lincoln
to
be
a
possible
way
to
stave
off
secession.
A
few
short
weeks
before
the
war
he
went
so
far
as
to
pen
a
letter
to
every
governor
asking
for
their
support
in
ratifying
the
Corwin
Amendment.
By
the
time
Lincoln
took
office,
the
Confederacy
was
an
established
fact,
and
no
leaders
of
the
insurrection
proposed
rejoining
the
Union
on
any
terms.
The
failure
of
the
Peace
Conference
of
1861
rendered
legislative
compromise
virtually
impossible.
Buchanan
might
have
allowed
the
southern
states
to
secede,
and
some
members
of
his
cabinet
recommended
that.
However,
conservative
Democratic
nationalists,
such
as
Jeremiah
S.
Black,
Joseph
Holt,
and
Edwin
M.
Stanton
had
taken
control
of
Buchanan's
cabinet
in
early
January,
and
refused
to
accept
secession.
Lincoln
and
nearly
every
Republican
leader
adopted
this
position
by
March
1861:
the
Union
could
not
be
dismantled.
Believing
that
a
peaceful
solution
was
still
possible,
Lincoln
decided
to
not
take
any
action
against
the
South
unless
the
Unionists
themselves
were
attacked
first.
This
finally
happened
in
April
1861.
Historian
Allan
Nevins
argues
that
Lincoln
made
three
miscalculations
in
believing
that
he
could
preserve
the
Union,
hold
government
property,
and
still
avoid
war.
He
"temporarily
underrated
the
gravity
of
the
crisis",
overestimated
the
strength
of
Unionist
sentiment
in
the
South
and
border
states,
and
misunderstood
the
conditional
support
of
Unionists
in
the
border
states.
Fighting
begins.
On
April
12,
1861,
Union
troops
at
Fort
Sumter
were
fired
upon
and
forced
to
surrender.
On
April
15,
Lincoln
called
on
the
states
to
send
detachments
totaling
75,000
troops,
to
recapture
forts,
protect
the
capital,
and
"preserve
the
Union",
which
in
his
view
still
existed
intact
despite
the
actions
of
the
seceding
states.
These
events
forced
the
states
to
choose
sides.
Virginia
declared
its
secession,
after
which
the
Confederate
capital
was
moved
from
Montgomery
to
Richmond.
North
Carolina,
Tennessee,
and
Arkansas
also
voted
for
secession
over
the
next
two
months.
Missouri,
Kentucky
and
Maryland
threatened
secession,
but
neither
they
nor
the
slave
state
of
Delaware
seceded.
Lincoln
urgently
negotiated
with
state
leaders
there,
promising
not
to
interfere
with
slavery.
Troops
headed
south
towards
Washington,
D.C.
to
protect
the
capital
in
response
to
Lincoln's
call.
On
April
19,
angry
secessionist
mobs
in
Baltimore,
a
Maryland
city
to
the
north
of
Washington
that
controlled
the
rail
links,
attacked
Union
troops
traveling
to
the
capital.
George
William
Brown,
the
Mayor
of
Baltimore,
and
other
suspect
Maryland
politicians
were
arrested
and
imprisoned
at
Fort
McHenry.
Rebel
leaders
were
also
arrested
in
other
border
areas
and
held
in
military
prisons
without
trial.
Over
18,000
were
arrested.
One,
Clement
Vallandigham,
was
exiled,
but
the
remainder
were
released,
usually
after
two
or
three
months
("see":
Ex
parte
Merryman).
Conducting
the
war
effort.
The
war
was
a
source
of
constant
frustration
for
the
president
and
occupied
nearly
all
of
his
time.
He
had
a
contentious
relationship
with
General
McClellan,
who
became
general-in-chief
of
all
the
Union
armies
in
the
wake
of
the
embarrassing
Union
defeat
at
the
First
Battle
of
Bull
Run
and
after
the
retirement
of
Winfield
Scott
in
late
1861.
Despite
his
inexperience
in
military
affairs,
Lincoln
immediately
took
an
active
part
in
determining
war
strategy.
His
priorities
were
twofold:
to
ensure
that
Washington
was
well
defended;
and
to
conduct
an
aggressive
war
effort
that
would
satisfy
the
demand
in
the
North
for
prompt,
decisive
victory.
McClellan,
a
youthful
West
Point
graduate
and
railroad
executive
called
back
to
active
military
service,
He
took
several
months
to
plan
and
execute
his
Peninsula
Campaign,
with
the
objective
of
capturing
Richmond
by
moving
the
Army
of
the
Potomac
by
boat
to
the
peninsula
and
then
traveling
by
land
to
Richmond.
McClellan's
delay
concerned
Lincoln,
as
did
his
insistence
that
no
troops
were
needed
to
defend
Washington,
Lincoln
insisted
on
holding
some
of
McClellan's
troops
to
defend
the
capital,
a
decision
McClellan
blamed
for
the
ultimate
failure
of
the
Peninsula
Campaign.
McClellan,
a
conservative
Democrat,
was
passed
over
for
general-in-chief
(that
is,
chief
strategist)
in
favor
of
Henry
Wager
Halleck,
after
giving
Lincoln
his
"Harrison's
Landing
Letter",
where
he
offered
unsolicited
political
advice
to
Lincoln
urging
caution
in
the
war
effort.
McClellan's
letter
incensed
Radical
Republicans,
who
successfully
pressured
Lincoln
to
appoint
John
Pope,
a
Republican,
as
head
of
the
new
Army
of
Virginia.
Pope
complied
with
Lincoln's
strategic
desire
to
move
toward
Richmond
from
the
north,
thus
protecting
the
capital
from
attack.
However,
Pope
was
soundly
defeated
at
the
Second
Battle
of
Bull
Run
in
the
summer
of
1862,
forcing
the
Army
of
the
Potomac
to
defend
Washington
for
a
second
time.
In
response
to
his
failure,
Pope
was
sent
to
Minnesota
to
fight
the
Sioux.
Despite
his
dissatisfaction
with
McClellan's
failure
to
reinforce
Pope,
Lincoln
restored
him
to
command
of
all
forces
around
Washington,
to
the
dismay
of
his
cabinet
(all
save
Seward),
who
wished
McClellan
gone.
Two
days
after
McClellan's
return
to
command,
General
Lee's
forces
crossed
the
Potomac
River
into
Maryland,
leading
to
the
Battle
of
Antietam
(September
1862).
The
ensuing
Union
victory,
one
of
the
bloodiest
in
American
history,
enabled
Lincoln
to
give
notice
that
he
would
issue
an
Emancipation
Proclamation
in
January,
but
he
relieved
McClellan
of
his
command
after
waiting
for
the
conclusion
of
the
1862
midterm
elections
and
appointed
Republican
Ambrose
Burnside
to
head
the
Army
of
the
Potomac.
Burnside
was
politically
neutral,
which
Lincoln
desired,
and
for
the
most
part
supported
the
President's
aims.
Burnside
had
promised
to
follow
through
on
Lincoln's
strategic
vision
for
a
strong
offensive
against
Lee
and
Richmond.
After
Burnside
was
stunningly
defeated
at
Fredericksburg
in
December,
Joseph
Hooker
took
command,
despite
his
history
of
"loose
talk"
and
criticizing
former
commanders.
Hooker
was
routed
by
Lee
at
the
Battle
of
Chancellorsville
in
May,
1863,
but
continued
to
command
his
troops
for
roughly
two
months.
Hooker
did
not
agree
with
Lincoln's
desire
to
divide
his
troops,
and
possibly
force
Lee
to
do
the
same,
and
tendered
his
resignation,
which
was
accepted.
During
the
Gettysburg
Campaign
he
was
replaced
by
George
Meade.
Using
black
troops
and
former
slaves
was
official
government
policy
after
the
issuance
of
the
Emancipation
Proclamation.
At
first
Lincoln
was
reluctant
to
fully
implement
this
program,
but
by
the
spring
of
1863
he
was
ready
to
initiate
"a
massive
recruitment
of
Negro
troops."
In
a
letter
to
Andrew
Johnson,
the
military
governor
of
Tennessee,
encouraging
him
to
lead
the
way
in
raising
black
troops,
Lincoln
wrote,
"The
bare
sight
of
fifty
thousand
armed,
and
drilled
black
soldiers
on
the
banks
of
the
Mississippi
would
end
the
rebellion
at
once."
By
the
end
of
1863,
at
Lincoln's
direction,
General
Lorenzo
Thomas
had
recruited
twenty
regiments
of
African
Americans
from
the
Mississippi
Valley.
Grant.
After
the
Union
victory
at
Gettysburg,
Meade's
failure
to
pursue
Lee
and
months
of
inactivity
for
the
Army
of
the
Potomac
persuaded
Lincoln
that
a
change
was
needed.
McClellan
was
seeking
the
Democratic
nomination
for
President,
and
Lincoln
worried
that
Grant
might
also
have
political
aspirations.
Lincoln
convinced
himself
that
Grant
didn't
have
political
aspirations,
in
the
immediate
at
least,
and
made
Ulysses
S.
Grant
commander
of
the
Union
Army.
Grant
already
had
a
solid
string
of
victories
in
the
Western
Theater,
including
the
battles
of
Vicksburg
and
Chattanooga.
Responding
to
criticism
of
Grant,
Lincoln
replied,
"I
can't
spare
this
man.
He
fights."
Grant
waged
his
bloody
Overland
Campaign
in
1864
with
a
strategy
of
a
war
of
attrition,
characterized
by
high
Union
losses
at
battles
such
as
the
Wilderness
and
Cold
Harbor,
but
by
proportionately
higher
Confederate
losses.
The
high
casualty
figures
alarmed
the
nation,
and,
after
Grant
lost
a
third
of
his
army,
Lincoln
asked
what
Grant's
plans
were.
"I
propose
to
fight
it
out
on
this
line
if
it
takes
all
summer,"
replied
Grant.
Lincoln
and
the
Republican
party
mobilized
support
throughout
the
North,
backed
Grant
to
the
hilt,
and
replaced
his
losses.
The
Confederacy
was
out
of
replacements,
so
Lee's
army
shrank
with
every
battle,
forcing
it
back
to
trenches
outside
Petersburg.
In
April
1865,
Lee's
army
finally
crumbled
under
Grant's
pounding,
and
Richmond
fell.
Lincoln
authorized
Grant
to
target
the
Confederate
infrastructure
–
such
as
plantations,
railroads,
and
bridges
–
hoping
to
destroy
the
South's
morale
and
weaken
its
economic
ability
to
continue
fighting.
This
strategy
allowed
Generals
Sherman
and
Sheridan
to
destroy
plantations
and
towns
in
the
Shenandoah
Valley,
Georgia,
and
South
Carolina.
The
damage
caused
by
Sherman's
March
to
the
Sea
through
Georgia
totaled
more
than
$100
million
by
Sherman's
own
estimate.
Lincoln
grasped
the
need
to
control
strategic
points
(such
as
the
Mississippi
River
and
the
fortress
city
of
Vicksburg)
and
understood
the
importance
of
defeating
the
enemy's
army,
rather
than
simply
capturing
territory.
He
had,
however,
limited
success
in
motivating
his
commanders
to
adopt
his
strategies
until
late
1863,
when
he
found
a
man
who
shared
his
vision
of
the
war
in
Ulysses
S.
Grant.
Only
then
could
he
relentlessly
pursue
a
series
of
coordinated
offensives
in
multiple
theaters,
and
have
a
top
commander
who
agreed
on
the
use
of
black
troops.
Two
days
a
week,
Lincoln
would
meet
with
his
cabinet
in
the
afternoon,
and
occasionally
his
wife
would
force
him
to
take
a
carriage
ride
because
she
was
concerned
he
was
working
too
hard.
Throughout
the
war,
Lincoln
showed
an
intense
interest
with
the
military
campaigns.
He
spent
hours
at
the
War
Department
telegraph
office,
reading
dispatches
from
the
field.
He
visited
battle
sites
frequently,
and
seemed
fascinated
by
scenes
of
war.
During
Jubal
Anderson
Early's
raid
on
Washington,
D.C.
in
1864,
Lincoln
was
watching
the
combat
from
an
exposed
position;
captain
Oliver
Wendell
Holmes,
Jr.
shouted
at
him,
"Get
down,
you
damn
fool,
before
you
get
shot!"
Emancipation
Proclamation.
Lincoln
maintained
that
the
powers
of
his
administration
to
end
slavery
were
limited
by
the
Constitution.
He
expected
to
cause
the
eventual
extinction
of
slavery
by
stopping
its
further
expansion
into
any
U.S.
territory,
and
by
persuading
states
to
accept
compensated
emancipation
if
the
state
would
outlaw
slavery
(an
offer
that
took
effect
only
in
Washington,
D.C.).
Guelzo
says
Lincoln
believed
that
shrinking
slavery
in
this
way
would
make
it
uneconomical,
and
place
it
back
on
the
road
to
eventual
extinction
that
the
Founders
had
envisioned.
In
July
1862,
Congress
passed
the
Second
Confiscation
Act,
which
freed
the
slaves
of
anyone
convicted
of
aiding
the
rebellion.
Although
Lincoln
believed
it
wasn't
in
Congress's
remit
to
free
any
slaves,
he
approved
the
bill.
He
felt
freeing
the
slaves
could
only
be
done
by
the
Commander
in
Chief
during
wartime,
and
that
signing
the
bill
would
help
placate
those
in
Congress
who
wanted
to
do
it
through
legislation.
In
that
month,
Lincoln
discussed
a
draft
of
the
Emancipation
Proclamation
with
his
cabinet.
In
it,
he
stated
that
"as
a
fit
and
necessary
military
measure"
(and
according
to
Donald
not
for
moral
reasons)
on
January
1,
1863,
"all
persons
held
as
a
slaves"
in
the
Confederate
states
will
"
thenceforward,
and
forever,
be
free."
The
Emancipation
Proclamation,
announced
on
September
22,
1862
and
put
into
effect
on
January
1,
1863,
freed
slaves
in
territories
not
already
under
Union
control.
As
Union
armies
advanced
south,
more
slaves
were
liberated
until
all
of
them
in
Confederate
territory
(over
three
million)
were
freed.
Lincoln
later
said:
"I
never,
in
my
life,
felt
more
certain
that
I
was
doing
right,
than
I
do
in
signing
this
paper."
The
proclamation
made
the
abolition
of
slavery
in
the
rebel
states
an
official
war
goal.
Lincoln
then
threw
his
energies
into
passage
of
the
Thirteenth
Amendment
to
permanently
abolish
slavery
throughout
the
nation.
He
personally
lobbied
individual
Congressmen
for
the
Amendment,
which
was
passed
by
the
Congress
in
early
1865,
shortly
before
his
death.
A
few
days
after
the
Emancipation
was
announced,
thirteen
Republican
governors
met
at
the
War
Governors'
Conference;
they
supported
the
president's
Proclamation,
but
suggested
the
removal
of
General
George
B.
McClellan
as
commander
of
the
Union's
Army
of
the
Potomac.
For
some
time,
Lincoln
continued
earlier
plans
to
set
up
colonies
for
the
newly
freed
slaves.
He
commented
favorably
on
colonization
in
the
Emancipation
Proclamation,
but
all
attempts
at
such
a
massive
undertaking
failed.
As
Frederick
Douglass
observed,
Lincoln
was,
"The
first
great
man
that
I
talked
with
in
the
United
States
freely
who
in
no
single
instance
reminded
me
of
the
difference
between
himself
and
myself,
of
the
difference
of
color."
Gettysburg
Address.
Although
the
Battle
of
Gettysburg
was
a
Union
victory,
it
was
also
the
bloodiest
battle
of
the
war
and
dealt
a
blow
to
Lincoln's
war
effort.
As
the
Union
Army
decreased
in
numbers
due
to
casualties,
more
soldiers
were
needed
to
replace
the
ranks.
Lincoln's
1863
military
drafts
were
considered
"odious"
among
many
in
the
north,
particularly
immigrants.
The
New
York
Draft
Riots
of
July
1863
were
the
most
notable
manifestation
of
this
discontent.
"If
the
election
were
to
occur
now,
the
result
would
be
extremely
doubtful,
and
although
most
of
our
discreet
friends
are
sanguine
of
the
result,
my
impression
is,
the
chances
would
be
against
us.
The
draft
is
very
odious
in
the
State...
the
Democratic
leaders
have
succeeded
in
exciting
prejudice
and
passion,
and
have
infused
their
poison
into
the
minds
of
the
people
to
a
very
large
extent,
and
the
changes
are
against
us."
Therefore,
in
the
fall
of
1863,
Lincoln's
principal
aim
was
to
sustain
public
support
for
the
war
effort.
This
goal
became
the
focus
of
his
address
at
the
Gettysburg
battlefield
cemetery
on
November
19.
The
"Gettysburg
Address"
is
one
of
the
most
quoted
speeches
in
United
States
history.
It
was
delivered
at
the
dedication
of
the
Soldiers'
National
Cemetery
in
Gettysburg,
Pennsylvania,
on
the
afternoon
of
Thursday,
November
19,
1863,
during
the
American
Civil
War,
four
and
a
half
months
after
the
Union
armies
defeated
those
of
the
Confederacy
at
the
decisive
Battle
of
Gettysburg.
Abraham
Lincoln's
carefully
crafted
address,
secondary
to
other
presentations
that
day,
came
to
be
regarded
as
one
of
the
greatest
speeches
in
American
history.
In
just
over
two
minutes,
Lincoln
invoked
the
principles
of
human
equality
espoused
by
the
Declaration
of
Independence
and
redefined
the
Civil
War
as
a
struggle
not
merely
for
the
Union,
but
as
"a
new
birth
of
freedom"
that
would
bring
true
equality
to
all
of
its
citizens,
and
that
would
also
create
a
unified
nation
in
which
states'
rights
were
no
longer
dominant.
Beginning
with
the
now-iconic
phrase,
"Four
score
and
seven
years
ago...",
Lincoln
referred
to
the
events
of
the
Civil
War
and
described
the
ceremony
at
Gettysburg
as
an
opportunity
not
only
to
consecrate
the
grounds
of
a
cemetery,
but
also
to
dedicate
the
living
to
the
struggle
to
ensure
that
"government
of
the
people,
by
the
people,
for
the
people,
shall
not
perish
from
the
earth".
1864
election.
After
Union
victories
at
Gettysburg,
Vicksburg,
and
Chattanooga
in
1863,
overall
victory
seemed
at
hand,
and
Lincoln
promoted
Ulysses
S.
Grant
General-in-Chief
on
March
12,
1864.
When
the
spring
campaigns
turned
into
bloody
stalemates,
Lincoln
supported
Grant's
strategy
of
wearing
down
Lee's
Confederate
army
at
the
cost
of
heavy
Union
casualties.
With
an
election
looming,
he
easily
defeated
efforts
to
deny
his
renomination.
At
the
Convention,
the
Republican
Party
selected
Andrew
Johnson,
a
War
Democrat
from
the
Southern
state
of
Tennessee,
as
his
running
mate
to
form
a
broader
coalition.
They
ran
on
the
new
Union
Party
ticket
uniting
Republicans
and
War
Democrats.
Lincoln
did
not
show
the
pledge
to
his
cabinet,
but
asked
them
to
sign
the
sealed
envelope.
While
the
Democratic
platform
followed
the
Peace
wing
of
the
party
and
called
the
war
a
"failure,"
their
candidate,
General
George
B.
McClellan,
supported
the
war
and
repudiated
the
platform.
Lincoln
provided
Grant
with
new
replacements
and
mobilized
his
party
to
support
Grant
and
win
local
support
for
the
war
effort.
Sherman's
capture
of
Atlanta
in
September
ended
defeatist
jitters;
the
Democratic
Party
was
deeply
split,
with
some
leaders
and
most
soldiers
openly
for
Lincoln;
the
Union
party
was
united
and
energized,
and
Lincoln
was
easily
reelected
in
a
landslide.
He
won
all
but
three
states,
including
78%
of
the
Union
soldiers'
vote.
Second
Inaugural
Address.
On
March
4,
1865,
Lincoln
delivered
his
second
inaugural
address,
his
favorite
of
all
his
speeches.
At
this
time,
a
victory
over
the
rebels
was
at
hand,
slavery
was
dead,
and
Lincoln
was
looking
to
the
future.
Reconstruction.
Reconstruction
began
during
the
war
as
Lincoln
and
his
associates
pondered
questions
of
how
to
reintegrate
the
Southern
states
and
what
to
do
with
Confederate
leaders
and
the
freed
slaves.
Lincoln
led
the
"moderates"
regarding
Reconstruction
policy,
and
was
usually
opposed
by
the
Radical
Republicans,
under
Thaddeus
Stevens
in
the
House
and
Charles
Sumner
and
Benjamin
Wade
in
the
Senate
(though
he
cooperated
with
these
men
on
most
other
issues).
Determined
to
find
a
course
that
would
reunite
the
nation
and
not
alienate
the
South,
Lincoln
urged
that
speedy
elections
under
generous
terms
be
held
throughout
the
war
in
areas
behind
Union
lines.
His
Amnesty
Proclamation
of
December
8,
1863,
offered
pardons
to
those
who
had
not
held
a
Confederate
civil
office,
had
not
mistreated
Union
prisoners,
and
would
sign
an
oath
of
allegiance.
Critical
decisions
had
to
be
made
as
state
after
state
was
reconquered.
Of
special
importance
were
Tennessee,
where
Lincoln
appointed
Andrew
Johnson
as
governor,
and
Louisiana,
where
Lincoln
attempted
a
plan
that
would
restore
statehood
when
10%
of
the
voters
agreed
to
it.
The
Radicals
thought
this
policy
too
lenient,
and
passed
their
own
plan,
the
Wade-Davis
Bill,
in
1864.
When
Lincoln
pocket
vetoed
the
bill,
the
Radicals
retaliated
by
refusing
to
seat
representatives
elected
from
Louisiana,
Arkansas,
and
Tennessee.
Near
the
end
of
the
war,
Lincoln
made
an
extended
visit
to
Grant's
headquarters
at
City
Point,
Virginia.
This
allowed
the
president
to
confer
in
person
with
Grant
and
Sherman
about
ending
hostilities
(as
Sherman
managed
a
hasty
visit
to
Grant
from
his
forces
in
North
Carolina
at
the
same
time).
Lincoln
also
was
able
to
visit
Richmond
after
it
was
taken
by
the
Union
forces
and
to
make
a
public
gesture
of
sitting
at
Jefferson
Davis'
own
desk,
symbolically
saying
to
the
nation
that
the
President
of
the
United
States
held
authority
over
the
entire
land.
He
was
greeted
at
the
city
as
a
conquering
hero
by
freed
slaves,
whose
sentiments
were
epitomized
by
one
admirer's
quote,
"I
know
I
am
free
for
I
have
seen
the
face
of
Father
Abraham
and
have
felt
him."
When
a
general
asked
Lincoln
how
the
defeated
Confederates
should
be
treated,
Lincoln
replied,
"Let
'em
up
easy."
Lincoln
arrived
back
in
Washington
on
the
evening
of
April
9,
1865,
the
day
Lee
surrendered
at
Appomattox
Court
House
in
Virginia.
The
war
was
effectively
over.
The
other
rebel
armies
surrendered
soon
after,
and
there
was
no
subsequent
guerrilla
warfare.
Redefining
Republicanism.
Lincoln's
rhetoric
defined
the
issues
of
the
war
for
the
nation,
the
world,
and
posterity.
The
Gettysburg
Address
defied
Lincoln's
own
prediction
that
"the
world
will
little
note,
nor
long
remember
what
we
say
here."
His
second
inaugural
address
is
also
greatly
admired
and
often
quoted.
In
recent
years,
historians
have
stressed
Lincoln's
use
of
and
redefinition
of
republican
values.
As
early
as
the
1850s,
a
time
when
most
political
rhetoric
focused
on
the
sanctity
of
the
Constitution,
Lincoln
shifted
emphasis
to
the
Declaration
of
Independence
as
the
foundation
of
American
political
values—what
he
called
the
"sheet
anchor"
of
republicanism.
The
Declaration's
emphasis
on
freedom
and
equality
for
all,
rather
than
the
Constitution's
tolerance
of
slavers,
shifted
the
debate.
As
Diggins
concludes
regarding
the
highly
influential
Cooper
Union
speech,
"Lincoln
presented
Americans
a
theory
of
history
that
offers
a
profound
contribution
to
the
theory
and
destiny
of
republicanism
itself."
His
position
gained
strength
because
he
highlighted
the
moral
basis
of
republicanism,
rather
than
its
legalisms.
Nevertheless,
in
1861
Lincoln
justified
the
war
in
terms
of
legalisms
(the
Constitution
was
a
contract,
and
for
one
party
to
get
out
of
a
contract
all
the
other
parties
had
to
agree),
and
then
in
terms
of
the
national
duty
to
guarantee
a
"republican
form
of
government"
in
every
state.
That
duty
was
also
the
principle
underlying
federal
intervention
in
Reconstruction.
In
his
Gettysburg
Address
Lincoln
redefined
the
American
nation,
arguing
that
it
was
born
not
in
1789
but
in
1776,
"conceived
in
Liberty,
and
dedicated
to
the
proposition
that
all
men
are
created
equal."
He
declared
that
the
sacrifices
of
battle
had
rededicated
the
nation
to
the
propositions
of
democracy
and
equality,
"that
this
nation
shall
have
a
new
birth
of
freedom
—
and
that
government
of
the
people,
by
the
people,
for
the
people,
shall
not
perish
from
the
earth."
By
emphasizing
the
centrality
of
the
nation,
he
rebuffed
the
claims
of
state
sovereignty.
While
some
critics
say
Lincoln
moved
too
far
and
too
fast,
they
agree
that
he
dedicated
the
nation
to
values
that
marked
"a
new
founding
of
the
nation."
Civil
liberties
suspended.
During
the
Civil
War,
Lincoln
appropriated
powers
no
previous
President
had
wielded:
he
used
his
war
powers
to
proclaim
a
blockade,
suspended
the
writ
of
habeas
corpus,
spent
money
before
Congress
appropriated
it,
and
imprisoned
between
15,000
and
18,000
suspected
Confederate
sympathizers
without
trial.
Domestic
measures.
Lincoln
believed
in
the
Whig
theory
of
the
presidency,
which
left
Congress
to
write
the
laws
while
he
signed
them;
Lincoln
exercised
his
veto
power
only
four
times,
the
only
significant
instance
being
his
pocket
veto
of
the
Wade-Davis
Bill.
Thus,
he
signed
the
Homestead
Act
in
1862,
making
millions
of
acres
of
government-held
land
in
the
West
available
for
purchase
at
very
low
cost.
The
Morrill
Land-Grant
Colleges
Act,
also
signed
in
1862,
provided
government
grants
for
state
agricultural
colleges
in
each
state.
The
Pacific
Railway
Acts
of
1862
and
1864
granted
federal
support
for
the
construction
of
the
United
States'
First
Transcontinental
Railroad,
which
was
completed
in
1869.
The
passage
of
the
Homestead
Act
and
the
Pacific
Railway
Acts
was
made
possible
by
the
absence
of
Southern
congressmen
and
senators
who
had
opposed
the
measures
in
the
1850s.
Other
important
legislation
involved
two
measures
to
raise
revenues
for
the
Federal
government:
tariffs
(a
policy
with
long
precedent),
and
a
Federal
income
tax
(which
was
new).
In
1861,
Lincoln
signed
the
second
and
third
Morrill
Tariff
(the
first
had
become
law
under
James
Buchanan).
In
1861,
Lincoln
signed
the
Revenue
Act
of
1861
creating
the
first
U.S.
income
tax.
This
created
a
flat
tax
of
3%
on
incomes
above
$800
($
in
current
dollars),
which
was
later
changed
by
the
Revenue
Act
of
1862
Lincoln
also
presided
over
the
expansion
of
the
federal
government's
economic
influence
in
several
other
areas.
The
creation
of
the
system
of
national
banks
by
the
National
Banking
Acts
of
1863,
1864,
and
1865
allowed
the
creation
of
a
strong
national
financial
system.
In
1862,
Congress
created,
with
Lincoln's
approval,
the
Department
of
Agriculture,
although
that
institution
would
not
become
a
Cabinet-level
department
until
1889.
The
Legal
Tender
Act
of
1862
established
the
United
States
Note,
the
first
paper
currency
in
United
States
history
since
the
Continentals
that
were
issued
during
the
Revolution.
This
was
done
to
increase
the
money
supply
to
pay
for
fighting
the
war.
In
1862,
Lincoln
sent
a
senior
general,
John
Pope,
to
put
down
the
"Sioux
Uprising"
in
Minnesota.
Presented
with
303
death
warrants
for
convicted
Santee
Dakota
who
were
accused
of
killing
innocent
farmers,
Lincoln
ordered
a
personal
review
of
these
warrants,
eventually
approving
39
of
these
for
execution
(one
was
later
reprieved).
Abraham
Lincoln
is
largely
responsible
for
the
institution
of
the
Thanksgiving
holiday
in
the
United
States.
Prior
to
Lincoln's
presidency,
Thanksgiving,
while
a
regional
holiday
in
New
England
since
the
17th
century,
had
only
been
proclaimed
by
the
federal
government
sporadically,
and
on
irregular
dates.
The
last
such
proclamation
was
during
James
Madison's
presidency
fifty
years
before.
In
1863,
Lincoln
declared
the
final
Thursday
in
November
to
be
a
day
of
Thanksgiving,
and
the
holiday
has
been
celebrated
annually
then
ever
since.
Assassination.
Originally,
John
Wilkes
Booth,
a
well-known
actor
and
a
Confederate
spy
from
Maryland,
had
formulated
a
plan
to
kidnap
Lincoln
in
exchange
for
the
release
of
Confederate
prisoners.
After
attending
an
April
11
speech
in
which
Lincoln
promoted
voting
rights
for
blacks,
an
incensed
Booth
changed
his
plans
and
determined
to
assassinate
the
president.
Learning
that
the
President
and
First
Lady
would
be
attending
Ford's
Theatre,
he
laid
his
plans,
assigning
his
co-conspirators
to
assassinate
Vice
President
Andrew
Johnson
and
Secretary
of
State
William
H.
Seward.
Without
his
main
bodyguard
Ward
Hill
Lamon,
to
whom
he
related
his
famous
dream
regarding
his
own
assassination,
Lincoln
left
to
attend
the
play
"Our
American
Cousin"
on
April
14,
1865.
As
a
lone
bodyguard
wandered,
and
Lincoln
sat
in
his
state
box
(Box
7)
in
the
balcony,
Booth
crept
up
behind
the
President
and
waited
for
what
he
thought
would
be
the
funniest
line
of
the
play
("You
sock-dologizing
old
man-trap"),
hoping
the
laughter
would
muffle
the
noise
of
the
gunshot.
When
the
laughter
began,
Booth
jumped
into
the
box
and
aimed
a
single-shot,
round-ball.44
caliber
(11 mm)
Deringer
at
his
head,
firing
at
point-blank
range.
Major
Henry
Rathbone
momentarily
grappled
with
Booth
but
was
cut
by
Booth's
knife.
Booth
then
leaped
to
the
stage
and
shouted
"Sic
semper
tyrannis!"
()
and
escaped,
despite
suffering
a
broken
leg
in
the
leap.
A
twelve-day
manhunt
ensued,
in
which
Booth
was
chased
by
Federal
agents
(under
the
direction
of
Secretary
of
War
Edwin
M.
Stanton).
He
was
eventually
cornered
in
a
Virginia
barn
house
and
shot,
dying
of
his
wounds
soon
after.
An
army
surgeon,
Doctor
Charles
Leale,
initially
assessed
Lincoln's
wound
as
mortal.
The
President
was
taken
across
the
street
from
the
theater
to
the
Petersen
House,
where
he
lay
in
a
coma
for
nine
hours
before
dying.
Several
physicians
attended
Lincoln,
including
U.S.
Army
Surgeon
General
Joseph
K.
Barnes
of
the
Army
Medical
Museum.
Using
a
probe,
Barnes
located
some
fragments
of
Lincoln's
skull
and
the
ball
lodged
inside
his
brain.
Lincoln
never
regained
consciousness
and
was
pronounced
dead
at
7:22:10
a.m.
April
15,
1865.
He
was
the
first
president
to
be
assassinated
or
to
lie
in
state.
Lincoln's
body
was
carried
by
train
in
a
grand
funeral
procession
through
several
states
on
its
way
back
to
Illinois.
While
much
of
the
nation
mourned
him
as
the
savior
of
the
United
States,
Copperheads
celebrated
the
death
of
a
man
they
considered
a
tyrant.
The
Lincoln
Tomb
in
Oak
Ridge
Cemetery
in
Springfield,
is
tall
and,
by
1874,
was
surmounted
with
several
bronze
statues
of
Lincoln.
To
prevent
repeated
attempts
to
steal
Lincoln's
body
and
hold
it
for
ransom,
Robert
Todd
Lincoln
had
it
exhumed
and
reinterred
in
concrete
several
feet
thick
in
1901.
Religious
and
philosophical
beliefs.
In
March
1860
in
a
speech
in
New
Haven,
Connecticut,
Lincoln
said,
regarding
slavery,
"Whenever
this
question
shall
be
settled,
it
must
be
settled
on
some
philosophical
basis.
No
policy
that
does
not
rest
upon
some
philosophical
public
opinion
can
be
permanently
maintained."
The
philosophical
basis
for
Lincoln's
beliefs
regarding
slavery
and
other
issues
of
the
day
require
that
Lincoln
be
examined
"seriously
as
a
man
of
ideas."
Lincoln
was
a
strong
supporter
of
the
American
Whig
version
of
liberal
capitalism
who,
more
than
most
politicians
of
the
time,
was
able
to
express
his
ideas
within
the
context
of
Nineteenth
Century
religious
beliefs.
There
were
few
people
who
strongly
or
directly
influenced
Lincoln's
moral
and
intellectual
development
and
perspectives.
There
was
no
teacher,
mentor,
church
leader,
community
leader,
or
peer
that
Lincoln
would
credit
in
later
years
as
a
strong
influence
on
his
intellectual
development.
Lacking
a
formal
education,
Lincoln's
personal
philosophy
was
shaped
by
"an
amazingly
retentive
memory
and
a
passion
for
reading
and
learning."
It
was
Lincoln's
reading,
rather
than
his
relationships,
that
were
most
influential
in
shaping
his
personal
beliefs.
Even
as
a
child,
Lincoln
largely
rejected
organized
religion,
but
the
Calvinistic
"doctrine
of
necessity"
would
remain
a
factor
throughout
his
life.
In
1846
Lincoln
described
the
effect
of
this
doctrine
as
"that
the
human
mind
is
impelled
to
action,
or
held
in
rest
by
some
power,
over
which
the
mind
itself
has
no
control."
In
April
1864,
in
justifying
his
actions
regarding
Emancipation,
Lincoln
wrote,
"I
claim
not
to
have
controlled
events,
but
confess
plainly
that
events
have
controlled
me.
Now,
at
the
end
of
three
years
struggle
the
nation's
condition
is
not
what
either
party,
or
any
man
devised,
or
expected.
God
alone
can
claim
it."
As
Lincoln
matured,
and
especially
during
his
term
as
president,
the
idea
of
a
divine
will
somehow
interacting
with
human
affairs
increasingly
influenced
his
public
expressions.
On
a
personal
level,
the
death
of
his
son
Willie
in
February
1862
may
have
caused
Lincoln
to
look
towards
religion
for
answers
and
solace.
Lincoln's
religious
skepticism
was
fueled
by
his
exposure
to
the
ideas
of
the
Lockean
Enlightenment
and
classical
liberalism,
especially
economic
liberalism.
Consistent
with
the
common
practice
of
the
Whig
party,
Lincoln
would
often
use
the
Declaration
of
Independence
as
the
philosophical
and
moral
expression
of
these
two
philosophies.
In
a
February
22,
1861
speech
at
Independence
Hall
in
Philadelphia
Lincoln
said,
He
found
in
the
Declaration
justification
for
Whig
economic
policy
and
opposition
to
territorial
expansion
and
the
nativist
platform
of
the
Know
Nothings.
In
claiming
that
all
men
were
created
free,
Lincoln
and
the
Whigs
argued
that
this
freedom
required
economic
advancement,
expanded
education,
territory
to
grow,
and
the
ability
of
the
nation
to
absorb
the
growing
immigrant
population.
It
was
the
Declaration
of
Independence,
rather
than
the
Bible,
that
Lincoln
most
relied
on
to
oppose
any
further
territorial
expansion
of
slavery.
He
saw
the
Declaration
as
more
than
a
political
document.
To
him,
as
well
as
to
many
abolitionists
and
other
antislavery
leaders,
it
was,
foremost,
a
moral
document
that
had
forever
determined
valuable
principles
for
the
future
shaping
of
the
nation.
Legacy
and
memorials.
Lincoln's
death
made
the
President
a
national
martyr,
regarded
by
historians
in
numerous
polls
as
among
the
greatest
presidents
in
U.S.
history,
usually
in
the
top
three,
along
with
George
Washington
and
Franklin
D.
Roosevelt.
A
study
published
in
2004,
found
that
scholars
in
the
fields
of
history
and
politics
ranked
Lincoln
number
one,
while
law
scholars
placed
him
second
after
Washington.
Among
contemporary
admirers,
Lincoln
is
usually
seen
as
personifying
classical
values
of
honesty
and
integrity,
as
well
as
respect
for
individual
and
minority
rights,
and
human
freedom
in
general.
Many
American
organizations
of
all
purposes
and
agendas
continue
to
cite
his
name
and
image,
with
interests
ranging
from
the
gay
rights-supporting
Log
Cabin
Republicans
to
the
insurance
corporation
Lincoln
National
Corporation.
The
Lincoln
automobile
brand
is
also
named
after
him.
The
ballistic
missile
submarine
"Abraham
Lincoln"
(SSBN-602)
and
the
aircraft
carrier
"Abraham
Lincoln"
(CVN-72)
were
named
in
his
honor.
During
the
Spanish
Civil
War,
the
American
faction
of
the
International
Brigades
named
themselves
the
Abraham
Lincoln
Brigade.
Lincoln
has
been
memorialized
in
many
town,
city,
and
county
names,
Lincoln,
Illinois,
is
the
only
city
to
be
named
for
Abraham
Lincoln
before
he
became
President.
Lincoln's
name
and
image
appear
in
numerous
places.
These
include
the
Lincoln
Memorial
in
Washington,
D.C.,
the
U.S.
Lincoln
$5
bill
and
the
Lincoln
cent,
and
Lincoln's
sculpture
on
Mount
Rushmore.
Abraham
Lincoln
Birthplace
National
Historical
Park
in
Hodgenville,
Kentucky,
Lincoln
Boyhood
National
Memorial
in
Lincoln
City,
Indiana,
and
Lincoln
Home
National
Historic
Site
in
Springfield,
Illinois,
In
addition,
New
Salem,
Illinois
(a
reconstruction
of
Lincoln's
early
adult
hometown),
Ford's
Theatre,
and
Petersen
House
(where
he
died)
are
all
preserved
as
museums.
The
state
nickname
for
Illinois
is
"Land
of
Lincoln";
the
slogan
has
appeared
continuously
on
nearly
all
Illinois
license
plates
issued
since
1954.
Abraham
Lincoln's
birthday,
February
12,
was
never
a
national
holiday,
but
it
was
observed
by
30
states.
In
1971,
Presidents
Day
became
a
national
holiday,
combining
Lincoln's
and
Washington's
birthdays,
and
replacing
most
states'
celebration
of
his
birthday.
As
of
2005,
Lincoln's
Birthday
is
a
legal
holiday
in
10
states.
The
Abraham
Lincoln
Association
was
formed
in
1908
to
commemorate
the
centennial
of
Lincoln's
birth.
The
Association
is
now
the
oldest
group
dedicated
to
the
study
of
Lincoln.
To
commemorate
his
200th
birthday
in
February
2009,
Congress
established
the
Abraham
Lincoln
Bicentennial
Commission
(ALBC)
in
2000
to
honor
Lincoln.
The
Abraham
Lincoln
Presidential
Library
and
Museum
is
located
in
Springfield
and
is
run
by
the
State
of
Illinois.
Lincoln
owned
a
model
1857
Waltham
William
Ellery
watch,
with
serial
number
67613.
This
watch
is
now
in
the
custody
of
the
Smithsonian
Museum.
On
March
11,
2009,
the
National
Museum
of
American
History
found
a
message
engraved
inside
Lincoln's
watch
by
a
watchmaker
named
Jonathan
Dillon
who
was
repairing
it
at
the
outbreak
of
the
American
Civil
War.
The
engraving
reads
(in
part):
"Fort
Sumpter
was
attacked
by
the
rebels"
and
"thank
God
we
have
a
government."
---END.OF.DOCUMENT---
Aristotle.
Aristotle
(,
"Aristotélēs")
(384
BC
–
322
BC)
was
a
Greek
philosopher,
a
student
of
Plato
and
teacher
of
Alexander
the
Great.
His
writings
cover
many
subjects,
including
physics,
metaphysics,
poetry,
theater,
music,
logic,
rhetoric,
politics,
government,
ethics,
biology,
and
zoology.
Together
with
Plato
and
Socrates
(Plato's
teacher),
Aristotle
is
one
of
the
most
important
founding
figures
in
Western
philosophy.
Aristotle's
writings
constitute
a
first
at
creating
a
comprehensive
system
of
Western
philosophy,
encompassing
morality
and
aesthetics,
logic
and
science,
politics
and
metaphysics.
Aristotle's
views
on
the
physical
sciences
profoundly
shaped
medieval
scholarship,
and
their
influence
extended
well
into
the
Renaissance,
although
they
were
ultimately
replaced
by
Newtonian
physics.
In
the
biological
sciences,
some
of
his
observations
were
confirmed
to
be
accurate
only
in
the
nineteenth
century.
His
works
contain
the
earliest
known
formal
study
of
logic,
which
was
incorporated
in
the
late
nineteenth
century
into
modern
formal
logic.
In
metaphysics,
Aristotelianism
had
a
profound
influence
on
philosophical
and
theological
thinking
in
the
Islamic
and
Jewish
traditions
in
the
Middle
Ages,
and
it
continues
to
influence
Christian
theology,
especially
Eastern
Orthodox
theology,
and
the
scholastic
tradition
of
the
Catholic
Church.
His
ethics,
though
always
influential,
gained
renewed
interest
with
the
modern
advent
of
virtue
ethics.
All
aspects
of
Aristotle's
philosophy
continue
to
be
the
object
of
active
academic
study
today.
Though
Aristotle
wrote
many
elegant
treatises
and
dialogues
(Cicero
described
his
literary
style
as
"a
river
of
gold"),
it
is
thought
that
the
majority
of
his
writings
are
now
lost
and
only
about
one-third
of
the
original
works
have
survived.
Despite
the
far-reaching
appeal
that
Aristotle's
works
have
traditionally
enjoyed,
today
modern
scholarship
questions
a
substantial
portion
of
the
Aristotelian
corpus
as
authentically
Aristotle's
own.
Life.
Aristotle
was
born
in
Stageira,
Chalcidice,
in
384
BC,
about
east
of
modern-day
Thessaloniki.
His
father
Nicomachus
was
the
personal
physician
to
King
Amyntas
of
Macedon.
Aristotle
was
trained
and
educated
as
a
member
of
the
aristocracy.
At
about
the
age
of
eighteen,
he
went
to
Athens
to
continue
his
education
at
Plato's
Academy.
Aristotle
remained
at
the
academy
for
nearly
twenty
years,
not
leaving
until
after
Plato's
death
in
347
BC.
He
then
traveled
with
Xenocrates
to
the
court
of
his
friend
Hermias
of
Atarneus
in
Asia
Minor.
While
in
Asia,
Aristotle
traveled
with
Theophrastus
to
the
island
of
Lesbos,
where
together
they
researched
the
botany
and
zoology
of
the
island.
Aristotle
married
Hermias's
adoptive
daughter
(or
niece)
Pythias.
She
bore
him
a
daughter,
whom
they
named
Pythias.
Soon
after
Hermias'
death,
Aristotle
was
invited
by
Philip
II
of
Macedon
to
become
the
tutor
to
his
son
Alexander
the
Great
in
343
B.C.
Aristotle
was
appointed
as
the
head
of
the
royal
academy
of
Macedon.
During
that
time
he
gave
lessons
not
only
to
Alexander,
but
also
to
two
other
future
kings:
Ptolemy
and
Cassander.
In
his
"Politics",
Aristotle
states
that
only
one
thing
could
justify
monarchy,
and
that
was
if
the
virtue
of
the
king
and
his
family
were
greater
than
the
virtue
of
the
rest
of
the
citizens
put
together.
Tactfully,
he
included
the
young
prince
and
his
father
in
that
category.
Aristotle
encouraged
Alexander
toward
eastern
conquest,
and
his
attitude
towards
Persia
was
unabashedly
ethnocentric.
In
one
famous
example,
he
counsels
Alexander
to
be
'a
leader
to
the
Greeks
and
a
despot
to
the
barbarians,
to
look
after
the
former
as
after
friends
and
relatives,
and
to
deal
with
the
latter
as
with
beasts
or
plants'.
By
335
BC
he
had
returned
to
Athens,
establishing
his
own
school
there
known
as
the
Lyceum.
Aristotle
conducted
courses
at
the
school
for
the
next
twelve
years.
While
in
Athens,
his
wife
Pythias
died
and
Aristotle
became
involved
with
Herpyllis
of
Stageira,
who
bore
him
a
son
whom
he
named
after
his
father,
Nicomachus.
According
to
the
Suda,
he
also
had
an
eromenos,
Palaephatus
of
Abydus.
It
is
during
this
period
in
Athens
from
335
to
323
BC
when
Aristotle
is
believed
to
have
composed
many
of
his
works.
Aristotle
wrote
many
dialogues,
only
fragments
of
which
survived.
The
works
that
have
survived
are
in
treatise
form
and
were
not,
for
the
most
part,
intended
for
widespread
publication,
as
they
are
generally
thought
to
be
lecture
aids
for
his
students.
His
most
important
treatises
include
"Physics",
"Metaphysics",
"Nicomachean
Ethics",
"Politics",
"De
Anima
(On
the
Soul)"
and
"Poetics".
Aristotle
not
only
studied
almost
every
subject
possible
at
the
time,
but
made
significant
contributions
to
most
of
them.
In
physical
science,
Aristotle
studied
anatomy,
astronomy,
embryology,
geography,
geology,
meteorology,
physics
and
zoology.
In
philosophy,
he
wrote
on
aesthetics,
ethics,
government,
metaphysics,
politics,
economics,
psychology,
rhetoric
and
theology.
He
also
studied
education,
foreign
customs,
literature
and
poetry.
His
combined
works
constitute
a
virtual
encyclopedia
of
Greek
knowledge.
It
has
been
suggested
that
Aristotle
was
probably
the
last
person
to
know
everything
there
was
to
be
known
in
his
own
time.
Near
the
end
of
Alexander's
life,
Alexander
began
to
suspect
plots
against
himself,
and
threatened
Aristotle
in
letters.
Aristotle
had
made
no
secret
of
his
contempt
for
Alexander's
pretense
of
divinity,
and
the
king
had
executed
Aristotle's
grandnephew
Callisthenes
as
a
traitor.
A
widespread
tradition
in
antiquity
suspected
Aristotle
of
playing
a
role
in
Alexander's
death,
but
there
is
little
evidence
for
this.
Upon
Alexander's
death,
anti-Macedonian
sentiment
in
Athens
once
again
flared.
Eurymedon
the
hierophant
denounced
Aristotle
for
not
holding
the
gods
in
honor.
Aristotle
fled
the
city
to
his
mother's
family
estate
in
Chalcis,
explaining,
"I
will
not
allow
the
Athenians
to
sin
twice
against
philosophy,"
a
reference
to
Athens's
prior
trial
and
execution
of
Socrates.
However,
he
died
in
Euboea
of
natural
causes
within
the
year
(in
322
BC).
Aristotle
named
chief
executor
his
student
Antipater
and
left
a
will
in
which
he
asked
to
be
buried
next
to
his
wife.
Logic.
With
the
"Prior
Analytics",
Aristotle
is
credited
with
the
earliest
study
of
formal
logic,
and
his
conception
of
it
was
the
dominant
form
of
Western
logic
until
19th
century
advances
in
mathematical
logic.
Kant
stated
in
the
"Critique
of
Pure
Reason"
that
Aristotle's
theory
of
logic
completely
accounted
for
the
core
of
deductive
inference.
History.
Aristotle
"says
that
'on
the
subject
of
reasoning'
he
'had
nothing
else
on
an
earlier
date
to
speak
of'".
However,
Plato
reports
that
syntax
was
devised
before
him,
by
Prodicus
of
Ceos,
who
was
concerned
by
the
correct
use
of
words.
Logic
seems
to
have
emerged
from
dialectics;
the
earlier
philosophers
made
frequent
use
of
concepts
like
"reductio
ad
absurdum"
in
their
discussions,
but
never
truly
understood
the
logical
implications.
Even
Plato
had
difficulties
with
logic;
although
he
had
a
reasonable
conception
of
a
deducting
system,
he
could
never
actually
construct
one
and
relied
instead
on
his
dialectic.
Plato
believed
that
deduction
would
simply
follow
from
premises,
hence
he
focused
on
maintaining
solid
premises
so
that
the
conclusion
would
logically
follow.
Consequently,
Plato
realized
that
a
method
for
obtaining
conclusions
would
be
most
beneficial.
He
never
succeeded
in
devising
such
a
method,
but
his
best
attempt
was
published
in
his
book
"Sophist",
where
he
introduced
his
division
method.
Analytics
and
the
"Organon".
The
order
of
the
books
(or
the
teachings
from
which
they
are
composed)
is
not
certain,
but
this
list
was
derived
from
analysis
of
Aristotle's
writings.
It
goes
from
the
basics,
the
analysis
of
simple
terms
in
the
"Categories,"
the
analysis
of
propositions
and
their
elementary
relations
in
"On
Interpretation",
to
the
study
of
more
complex
forms,
namely,
syllogisms
(in
the
"Analytics")
and
dialectics
(in
the
"Topics"
and
"Sophistical
Refutations").
The
first
three
treatises
form
the
core
of
the
logical
theory
"stricto
sensu":
the
grammar
of
the
language
of
logic
and
the
correctness
rules
of
reasoning.
There
is
one
volume
of
Aristotle's
concerning
logic
not
found
in
the
"Organon",
namely
the
fourth
book
of
"Metaphysics.".
Aristotle's
scientific
method.
Like
his
teacher
Plato,
Aristotle's
philosophy
aims
at
the
universal.
Aristotle,
however,
found
the
universal
in
particular
things,
which
he
called
the
essence
of
things,
while
Plato
finds
that
the
universal
exists
apart
from
particular
things,
and
is
related
to
them
as
their
prototype
or
exemplar.
For
Aristotle,
therefore,
philosophic
method
implies
the
ascent
from
the
study
of
particular
phenomena
to
the
knowledge
of
essences,
while
for
Plato
philosophic
method
means
the
descent
from
a
knowledge
of
universal
Forms
(or
ideas)
to
a
contemplation
of
particular
imitations
of
these.
For
Aristotle,
"form"
still
refers
to
the
unconditional
basis
of
phenomena
but
is
"instantiated"
in
a
particular
substance
(see
"Universals
and
particulars",
below).
In
a
certain
sense,
Aristotle's
method
is
both
inductive
and
deductive,
while
Plato's
is
essentially
deductive
from
"a
priori"
principles.
In
Aristotle's
terminology,
"natural
philosophy"
is
a
branch
of
philosophy
examining
the
phenomena
of
the
natural
world,
and
includes
fields
that
would
be
regarded
today
as
physics,
biology
and
other
natural
sciences.
In
modern
times,
the
scope
of
"philosophy"
has
become
limited
to
more
generic
or
abstract
inquiries,
such
as
ethics
and
metaphysics,
in
which
logic
plays
a
major
role.
Today's
philosophy
tends
to
exclude
empirical
study
of
the
natural
world
by
means
of
the
scientific
method.
In
contrast,
Aristotle's
philosophical
endeavors
encompassed
virtually
all
facets
of
intellectual
inquiry.
In
the
larger
sense
of
the
word,
Aristotle
makes
philosophy
coextensive
with
reasoning,
which
he
also
would
describe
as
"science".
Note,
however,
that
his
use
of
the
term
"science"
carries
a
different
meaning
than
that
covered
by
the
term
"scientific
method".
For
Aristotle,
"all
science
("dianoia")
is
either
practical,
poetical
or
theoretical"
("Metaphysics"
1025b25).
By
practical
science,
he
means
ethics
and
politics;
by
poetical
science,
he
means
the
study
of
poetry
and
the
other
fine
arts;
by
theoretical
science,
he
means
physics,
mathematics
and
metaphysics.
If
logic
(or
"analytics")
is
regarded
as
a
study
preliminary
to
philosophy,
the
divisions
of
Aristotelian
philosophy
would
consist
of:
(1)
Logic;
(2)
Theoretical
Philosophy,
including
Metaphysics,
Physics,
Mathematics,
(3)
Practical
Philosophy
and
(4)
Poetical
Philosophy.
In
the
period
between
his
two
stays
in
Athens,
between
his
times
at
the
Academy
and
the
Lyceum,
Aristotle
conducted
most
of
the
scientific
thinking
and
research
for
which
he
is
renowned
today.
In
fact,
most
of
Aristotle's
life
was
devoted
to
the
study
of
the
objects
of
natural
science.
Aristotle's
metaphysics
contains
observations
on
the
nature
of
numbers
but
he
made
no
original
contributions
to
mathematics.
He
did,
however,
perform
original
research
in
the
natural
sciences,
e.g.,
botany,
zoology,
physics,
astronomy,
chemistry,
meteorology,
and
several
other
sciences.
Aristotle's
writings
on
science
are
largely
qualitative,
as
opposed
to
quantitative.
Beginning
in
the
sixteenth
century,
scientists
began
applying
mathematics
to
the
physical
sciences,
and
Aristotle's
work
in
this
area
was
deemed
hopelessly
inadequate.
His
failings
were
largely
due
to
the
absence
of
concepts
like
mass,
velocity,
force
and
temperature.
He
had
a
conception
of
speed
and
temperature,
but
no
quantitative
understanding
of
them,
which
was
partly
due
to
the
absence
of
basic
experimental
devices,
like
clocks
and
thermometers.
His
writings
provide
an
account
of
many
scientific
observations,
a
mixture
of
precocious
accuracy
and
curious
errors.
For
example,
in
his
"History
of
Animals"
he
claimed
that
human
males
have
more
teeth
than
females
and
in
the
"Generation
of
Animals"
he
said
the
female
is
as
it
were
a
deformed
male.
In
a
similar
vein,
John
Philoponus,
and
later
Galileo,
showed
by
simple
experiments
that
Aristotle's
theory
that
a
heavier
object
falls
faster
than
a
lighter
object
is
incorrect.
On
the
other
hand,
Aristotle
refuted
Democritus's
claim
that
the
Milky
Way
was
made
up
of
"those
stars
which
are
shaded
by
the
earth
from
the
sun's
rays,"
pointing
out
(correctly,
even
if
such
reasoning
was
bound
to
be
dismissed
for
a
long
time)
that,
given
"current
astronomical
demonstrations"
that
"the
size
of
the
sun
is
greater
than
that
of
the
earth
and
the
distance
of
the
stars
from
the
earth
many
times
greater
than
that
of
the
sun,
then...the
sun
shines
on
all
the
stars
and
the
earth
screens
none
of
them."
In
places,
Aristotle
goes
too
far
in
deriving
'laws
of
the
universe'
from
simple
observation
and
over-stretched
reason.
Today's
scientific
method
assumes
that
such
thinking
without
sufficient
facts
is
ineffective,
and
that
discerning
the
validity
of
one's
hypothesis
requires
far
more
rigorous
experimentation
than
that
which
Aristotle
used
to
support
his
laws.
Aristotle
also
had
some
scientific
blind
spots.
He
posited
a
geocentric
cosmology
that
we
may
discern
in
selections
of
the
"Metaphysics",
which
was
widely
accepted
up
until
the
1500s.
From
the
3rd
century
to
the
1500s,
the
dominant
view
held
that
the
Earth
was
the
center
of
the
universe
(geocentrism).
Since
he
was
perhaps
the
philosopher
most
respected
by
European
thinkers
during
and
after
the
Renaissance,
these
thinkers
often
took
Aristotle's
erroneous
positions
as
given,
which
held
back
science
in
this
epoch.
However,
Aristotle's
scientific
shortcomings
should
not
mislead
one
into
forgetting
his
great
advances
in
the
many
scientific
fields.
For
instance,
he
founded
logic
as
a
formal
science
and
created
foundations
to
biology
that
were
not
superseded
for
two
millennia.
Moreover,
he
introduced
the
fundamental
notion
that
nature
is
composed
of
things
that
change
and
that
studying
such
changes
can
provide
useful
knowledge
of
underlying
constants.
The
five
elements.
Each
of
the
four
earthly
elements
has
its
natural
place;
the
earth
at
the
centre
of
the
universe,
then
water,
then
air,
then
fire.
When
they
are
out
of
their
natural
place
they
have
natural
motion,
requiring
no
external
cause,
which
is
towards
that
place;
so
bodies
sink
in
water,
air
bubbles
rise
up,
rain
falls,
flame
rises
in
air.
The
heavenly
element
has
perpetual
circular
motion.
Causality,
The
Four
Causes===.
Additionally,
things
can
be
causes
of
one
another,
causing
each
other
reciprocally,
as
hard
work
causes
fitness
and
vice
versa,
although
not
in
the
same
way
or
function,
the
one
is
as
the
beginning
of
change,
the
other
as
the
goal.
(Thus
Aristotle
first
suggested
a
reciprocal
or
circular
causality
as
a
relation
of
mutual
dependence
or
influence
of
cause
upon
effect).
Moreover,
Aristotle
indicated
that
the
same
thing
can
be
the
cause
of
contrary
effects;
its
presence
and
absence
may
result
in
different
outcomes.
Simply
it
is
the
goal
or
purpose
that
brings
about
an
event
(not
necessarily
a
mental
goal).
Taking
our
two
dominos,
it
requires
someone
to
intentionally
knock
the
dominos
over
as
they
cannot
fall
themselves.
Aristotle
marked
two
modes
of
causation:
proper
(prior)
causation
and
accidental
(chance)
causation.
All
causes,
proper
and
incidental,
can
be
spoken
as
potential
or
as
actual,
particular
or
generic.
The
same
language
refers
to
the
effects
of
causes,
so
that
generic
effects
assigned
to
generic
causes,
particular
effects
to
particular
causes,
operating
causes
to
actual
effects.
Essentially,
causality
does
not
suggest
a
temporal
relation
between
the
cause
and
the
effect.
All
further
investigations
of
causality
will
consist
of
imposing
the
favorite
hierarchies
on
the
order
causes,
such
as
final
>
efficient
>
material
>
formal
(Thomas
Aquinas),
or
of
restricting
all
causality
to
the
material
and
efficient
causes
or
to
the
efficient
causality
(deterministic
or
chance)
or
just
to
regular
sequences
and
correlations
of
natural
phenomena
(the
natural
sciences
describing
how
things
happen
instead
of
explaining
the
whys
and
wherefores).
Optics.
Aristotle
held
more
accurate
theories
on
some
optical
concepts
than
other
philosophers
of
his
day.
The
earliest
known
written
evidence
of
a
camera
obscura
can
be
found
in
Aristotle's
documentation
of
such
a
device
in
350
BC
in
"Problemata".
Aristotle's
apparatus
contained
a
dark
chamber
that
had
a
single
small
hole,
or
aperture,
to
allow
for
sunlight
to
enter.
Aristotle
used
the
device
to
make
observations
of
the
sun
and
noted
that
no
matter
what
shape
the
hole
was,
the
sun
would
still
be
correctly
displayed
as
a
round
object.
In
modern
cameras,
this
is
analogous
to
the
diaphragm.
Aristotle
also
made
the
observation
that
when
the
distance
between
the
tiny
hole
and
the
surface
with
the
image
increased,
the
image
was
amplified.
Chance
and
spontaneity.
Spontaneity
and
chance
are
causes
of
effects.
Chance
as
an
incidental
cause
lies
in
the
realm
of
accidental
things.
It
is
"from
what
is
spontaneous"
(but
note
that
what
is
spontaneous
does
not
come
from
chance).
For
a
better
understanding
of
Aristotle's
conception
of
"chance"
it
might
be
better
to
think
of
"coincidence":
Something
takes
place
by
chance
if
a
person
sets
out
with
the
intent
of
having
one
thing
take
place,
but
with
the
result
of
another
thing
(not
intended)
taking
place.
For
example:
A
person
seeks
donations.
That
person
may
find
another
person
willing
to
donate
a
substantial
sum.
However,
if
the
person
seeking
the
donations
met
the
person
donating,
not
for
the
purpose
of
collecting
donations,
but
for
some
other
purpose,
Aristotle
would
call
the
collecting
of
the
donation
by
that
particular
donator
a
result
of
chance.
It
must
be
unusual
that
something
happens
by
chance.
In
other
words,
if
something
happens
all
or
most
of
the
time,
we
cannot
say
that
it
is
by
chance.
There
is
also
more
specific
kind
of
chance,
which
Aristotle
names
"luck",
that
can
only
apply
to
human
beings,
since
it
is
in
the
sphere
of
moral
actions.
According
to
Aristotle,
luck
must
involve
choice
(and
thus
deliberation),
and
only
humans
are
capable
of
deliberation
and
choice.
"What
is
not
capable
of
action
cannot
do
anything
by
chance".
Metaphysics.
Aristotle
defines
metaphysics
as
"the
knowledge
of
immaterial
being,"
or
of
"being
in
the
highest
degree
of
abstraction."
He
refers
to
metaphysics
as
"first
philosophy",
as
well
as
"the
theologic
science."
Substance,
potentiality
and
actuality.
Aristotle
examines
the
concept
of
substance
and
essence
("ousia")
in
his
"Metaphysics",
Book
VII
and
he
concludes
that
a
particular
substance
is
a
combination
of
both
matter
and
form.
As
he
proceeds
to
the
book
VIII,
he
concludes
that
the
matter
of
the
substance
is
the
substratum
or
the
stuff
of
which
it
is
composed,
"e.g."
the
matter
of
the
house
are
the
bricks,
stones,
timbers
etc.,
or
whatever
constitutes
the
"potential"
house.
While
the
form
of
the
substance,
is
the
"actual"
house,
namely
'covering
for
bodies
and
chattels'
or
any
other
differentia
(see
also
predicables).
The
formula
that
gives
the
components
is
the
account
of
the
matter,
and
the
formula
that
gives
the
differentia
is
the
account
of
the
form.
With
regard
to
the
change
("kinesis")
and
its
causes
now,
as
he
defines
in
his
Physics
and
On
Generation
and
Corruption
319b-320a,
he
distinguishes
the
coming
to
be
from:
1)
growth
and
diminution,
which
is
change
in
quantity;
2)
locomotion,
which
is
change
in
space;
and
3)
alteration,
which
is
change
in
quality.
The
coming
to
be
is
a
change
where
nothing
persists
of
which
the
resultant
is
a
property.
In
that
particular
change
he
introduces
the
concept
of
potentiality
("dynamis")
and
actuality
("entelecheia")
in
association
with
the
matter
and
the
form.
Referring
to
potentiality,
this
is
what
a
thing
is
capable
of
doing,
or
being
acted
upon,
if
it
is
not
prevented
by
something
else.
For
example,
the
seed
of
a
plant
in
the
soil
is
potentially
("dynamei")
plant,
and
if
is
not
prevented
by
something,
it
will
become
a
plant.
Potentially
beings
can
either
'act'
("poiein")
or
'be
acted
upon'
("paschein"),
which
can
be
either
innate
or
learned.
For
example,
the
eyes
possess
the
potentiality
of
sight
(innate
–
being
acted
upon),
while
the
capability
of
playing
the
flute
can
be
possessed
by
learning
(exercise
–
acting).
Actuality
is
the
fulfillment
of
the
end
of
the
potentiality.
Because
the
end
("telos")
is
the
principle
of
every
change,
and
for
the
sake
of
the
end
exists
potentiality,
therefore
actuality
is
the
end.
Referring
then
to
our
previous
example,
we
could
say
that
actuality
is
when
the
seed
of
the
plant
becomes
a
plant.
"
For
that
for
the
sake
of
which
a
thing
is,
is
its
principle,
and
the
becoming
is
for
the
sake
of
the
end;
and
the
actuality
is
the
end,
and
it
is
for
the
sake
of
this
that
the
potentiality
is
acquired.
For
animals
do
not
see
in
order
that
they
may
have
sight,
but
they
have
sight
that
they
may
see."
In
conclusion,
the
matter
of
the
house
is
its
potentiality
and
the
form
is
its
actuality.
The
formal
cause
("aitia")
then
of
that
change
from
potential
to
actual
house,
is
the
reason
("logos")
of
the
house
builder
and
the
final
cause
is
the
end,
namely
the
house
itself.
Then
Aristotle
proceeds
and
concludes
that
the
actuality
is
prior
to
potentiality
in
formula,
in
time
and
in
substantiality.
With
this
definition
of
the
particular
substance
(i.e.,
matter
and
form),
Aristotle
tries
to
solve
the
problem
of
the
unity
of
the
beings,
"e.g.",
what
is
that
makes
the
man
one?
Since,
according
to
Plato
there
are
two
Ideas:
animal
and
biped,
how
then
is
man
a
unity?
However,
according
to
Aristotle,
the
potential
being
(matter)
and
the
actual
one
(form)
are
one
and
the
same
thing.
Universals
and
particulars.
Aristotle's
predecessor,
Plato,
argued
that
all
things
have
a
universal
form,
which
could
be
either
a
property,
or
a
relation
to
other
things.
When
we
look
at
an
apple,
for
example,
we
see
an
apple,
and
we
can
also
analyze
a
form
of
an
apple.
In
this
distinction,
there
is
a
particular
apple
and
a
universal
form
of
an
apple.
Moreover,
we
can
place
an
apple
next
to
a
book,
so
that
we
can
speak
of
both
the
book
and
apple
as
being
next
to
each
other.
Plato
argued
that
there
are
some
universal
forms
that
are
not
a
part
of
particular
things.
For
example,
it
is
possible
that
there
is
no
particular
good
in
existence,
but
"good"
is
still
a
proper
universal
form.
Bertrand
Russell
is
a
contemporary
philosopher
that
agreed
with
Plato
on
the
existence
of
"uninstantiated
universals".
Aristotle
disagreed
with
Plato
on
this
point,
arguing
that
all
universals
are
instantiated.
Aristotle
argued
that
there
are
no
universals
that
are
unattached
to
existing
things.
According
to
Aristotle,
if
a
universal
exists,
either
as
a
particular
or
a
relation,
then
there
must
have
been,
must
be
currently,
or
must
be
in
the
future,
something
on
which
the
universal
can
be
predicated.
Consequently,
according
to
Aristotle,
if
it
is
not
the
case
that
some
universal
can
be
predicated
to
an
object
that
exists
at
some
period
of
time,
then
it
does
not
exist.
In
addition,
Aristotle
disagreed
with
Plato
about
the
location
of
universals.
As
Plato
spoke
of
the
world
of
the
forms,
a
location
where
all
universal
forms
subsist,
Aristotle
maintained
that
universals
exist
within
each
thing
on
which
each
universal
is
predicated.
So,
according
to
Aristotle,
the
form
of
apple
exists
within
each
apple,
rather
than
in
the
world
of
the
forms.
Biology
and
medicine.
In
Aristotelian
science,
most
especially
in
biology,
things
he
saw
himself
have
stood
the
test
of
time
better
than
his
retelling
of
the
reports
of
others,
which
contain
error
and
superstition.
He
dissected
animals,
but
not
humans
and
his
ideas
on
how
the
human
body
works
have
been
almost
entirely
superseded.
Empirical
research
program.
Aristotle
is
the
earliest
natural
historian
whose
work
has
survived
in
some
detail.
Aristotle
certainly
did
research
on
the
natural
history
of
Lesbos,
and
the
surrounding
seas
and
neighbouring
areas.
The
works
that
reflect
this
research,
such
as
"History
of
Animals",
"Generation
of
Animals",
and
"Parts
of
Animals",
contain
some
observations
and
interpretations,
along
with
sundry
myths
and
mistakes.
The
most
striking
passages
are
about
the
sea-life
visible
from
observation
on
Lesbos
and
available
from
the
catches
of
fishermen.
His
observations
on
catfish,
electric
fish
("Torpedo")
and
angler-fish
are
detailed,
as
is
his
writing
on
cephalopods,
namely,
"Octopus",
"Sepia"
(cuttlefish)
and
the
paper
nautilus
("Argonauta
argo").
His
description
of
the
hectocotyl
arm
was
about
two
thousand
years
ahead
of
its
time,
and
widely
disbelieved
until
its
rediscovery
in
the
nineteenth
century.
He
separated
the
aquatic
mammals
from
fish,
and
knew
that
sharks
and
rays
were
part
of
the
group
he
called
Selachē
(selachians).
Another
good
example
of
his
methods
comes
from
the
"Generation
of
Animals"
in
which
Aristotle
describes
breaking
open
fertilized
chicken
eggs
at
intervals
to
observe
when
visible
organs
were
generated.
He
gave
accurate
descriptions
of
ruminants'
four-chambered
fore-stomachs,
and
of
the
ovoviviparous
embryological
development
of
the
hound
shark
"Mustelus
mustelus".
Classification
of
living
things.
Aristotle's
classification
of
living
things
contains
some
elements
which
still
existed
in
the
nineteenth
century.
What
the
modern
zoologist
would
call
vertebrates
and
invertebrates,
Aristotle
called
'animals
with
blood'
and
'animals
without
blood'
(he
was
not
to
know
that
complex
invertebrates
do
make
use
of
haemoglobin,
but
of
a
different
kind
from
vertebrates).
Animals
with
blood
were
divided
into
live-bearing
(humans
and
mammals),
and
egg-bearing
(birds
and
fish).
Invertebrates
('animals
without
blood')
are
insects,
crustacea
(divided
into
non-shelled
–
cephalopods
–
and
shelled)
and
testacea
(molluscs).
In
some
respects,
this
incomplete
classification
is
better
than
that
of
Linnaeus,
who
crowded
the
invertebrata
together
into
two
groups,
Insecta
and
Vermes
(worms).
For
Charles
Singer,
"Nothing
is
more
remarkable
than
[Aristotle's]
efforts
to
[exhibit]
the
relationships
of
living
things
as
a
"scala
naturae"
Aristotle's
"History
of
Animals"
classified
organisms
in
relation
to
a
hierarchical
"Ladder
of
Life"
("scala
naturae"),
placing
them
according
to
complexity
of
structure
and
function
so
that
higher
organisms
showed
greater
vitality
and
ability
to
move.
Aristotle
believed
that
intellectual
purposes,
i.e.,
formal
causes,
guided
all
natural
processes.
Such
a
teleological
view
gave
Aristotle
cause
to
justify
his
observed
data
as
an
expression
of
formal
design.
Noting
that
"no
animal
has,
at
the
same
time,
both
tusks
and
horns,"
and
"a
single-hooved
animal
with
two
horns
I
have
never
seen,"
Aristotle
suggested
that
Nature,
giving
no
animal
both
horns
and
tusks,
was
staving
off
vanity,
and
giving
creatures
faculties
only
to
such
a
degree
as
they
are
necessary.
Noting
that
ruminants
had
a
multiple
stomachs
and
weak
teeth,
he
supposed
the
first
was
to
compensate
for
the
latter,
with
Nature
trying
to
preserve
a
type
of
balance.
In
a
similar
fashion,
Aristotle
believed
that
creatures
were
arranged
in
a
graded
scale
of
perfection
rising
from
plants
on
up
to
man,
the
"scala
naturae"
or
Great
Chain
of
Being.
His
system
had
eleven
grades,
arranged
according
"to
the
degree
to
which
they
are
infected
with
potentiality",
expressed
in
their
form
at
birth.
The
highest
animals
laid
warm
and
wet
creatures
alive,
the
lowest
bore
theirs
cold,
dry,
and
in
thick
eggs.
Aristotle
also
held
that
the
level
of
a
creature's
perfection
was
reflected
in
its
form,
but
not
preordained
by
that
form.
Ideas
like
this,
and
his
ideas
about
souls,
are
not
regarded
as
science
at
all
in
modern
times.
He
placed
emphasis
on
the
type(s)
of
soul
an
organism
possessed,
asserting
that
plants
possess
a
vegetative
soul,
responsible
for
reproduction
and
growth,
animals
a
vegetative
and
a
sensitive
soul,
responsible
for
mobility
and
sensation,
and
humans
a
vegetative,
a
sensitive,
and
a
rational
soul,
capable
of
thought
and
reflection.
Aristotle,
in
contrast
to
earlier
philosophers,
but
in
accordance
with
the
Egyptians,
placed
the
rational
soul
in
the
heart,
rather
than
the
brain.
Notable
is
Aristotle's
division
of
sensation
and
thought,
which
generally
went
against
previous
philosophers,
with
the
exception
of
Alcmaeon.
Successor:
Theophrastus.
Aristotle's
successor
at
the
Lyceum,
Theophrastus,
wrote
a
series
of
books
on
botany—the
"History
of
Plants"—which
survived
as
the
most
important
contribution
of
antiquity
to
botany,
even
into
the
Middle
Ages.
Many
of
Theophrastus'
names
survive
into
modern
times,
such
as
"carpos"
for
fruit,
and
"pericarpion"
for
seed
vessel.
Rather
than
focus
on
formal
causes,
as
Aristotle
did,
Theophrastus
suggested
a
mechanistic
scheme,
drawing
analogies
between
natural
and
artificial
processes,
and
relying
on
Aristotle's
concept
of
the
efficient
cause.
Theophrastus
also
recognized
the
role
of
sex
in
the
reproduction
of
some
higher
plants,
though
this
last
discovery
was
lost
in
later
ages.
Influence
on
Hellenistic
medicine.
After
Theophrastus,
the
Lyceum
failed
to
produce
any
original
work.
Though
interest
in
Aristotle's
ideas
survived,
they
were
generally
taken
unquestioningly.
It
is
not
until
the
age
of
Alexandria
under
the
Ptolemies
that
advances
in
biology
can
be
again
found.
The
first
medical
teacher
at
Alexandria
Herophilus
of
Chalcedon,
corrected
Aristotle,
placing
intelligence
in
the
brain,
and
connected
the
nervous
system
to
motion
and
sensation.
Herophilus
also
distinguished
between
veins
and
arteries,
noting
that
the
latter
pulse
while
the
former
do
not.
Though
a
few
ancient
atomists
such
as
Lucretius
challenged
the
teleological
viewpoint
of
Aristotelian
ideas
about
life,
teleology
(and
after
the
rise
of
Christianity,
natural
theology)
would
remain
central
to
biological
thought
essentially
until
the
18th
and
19th
centuries.
Ernst
Mayr
claimed
that
there
was
"nothing
of
any
real
consequence
in
biology
after
Lucretius
and
Galen
until
the
Renaissance."
Aristotle's
ideas
of
natural
history
and
medicine
survived,
but
they
were
generally
taken
unquestioningly.
Ethics.
Aristotle
considered
ethics
to
be
a
practical
rather
than
theoretical
study,
i.e.,
one
aimed
at
doing
good
rather
than
knowing
for
its
own
sake.
He
wrote
several
treatises
on
ethics,
including
most
notably,
the
"Nichomachean
Ethics".
Aristotle
taught
that
virtue
has
to
do
with
the
proper
function
("ergon")
of
a
thing.
An
eye
is
only
a
good
eye
in
so
much
as
it
can
see,
because
the
proper
function
of
an
eye
is
sight.
Aristotle
reasoned
that
humans
must
have
a
function
specific
to
humans,
and
that
this
function
must
be
an
activity
of
the
"psuchē"
(normally
translated
as
"soul")
in
accordance
with
reason
("logos").
Aristotle
identified
such
an
optimum
activity
of
the
soul
as
the
aim
of
all
human
deliberate
action,
"eudaimonia",
generally
translated
as
"happiness"
or
sometimes
"well
being".
To
have
the
potential
of
ever
being
happy
in
this
way
necessarily
requires
a
good
character
("ēthikē"
"aretē"),
often
translated
as
moral
(or
ethical)
virtue
(or
excellence).
Aristotle
taught
that
to
achieve
a
virtuous
and
potentially
happy
character
requires
a
first
stage
of
having
the
fortune
to
be
habituated
not
deliberately,
but
by
teachers,
and
experience,
leading
to
a
later
stage
in
which
one
consciously
chooses
to
do
the
best
things.
When
the
best
people
come
to
live
life
this
way
their
practical
wisdom
("phronēsis")
and
their
intellect
("nous")
can
develop
with
each
other
towards
the
highest
possible
ethical
virtue,
that
of
wisdom.
Politics.
In
addition
to
his
works
on
ethics,
which
address
the
individual,
Aristotle
addressed
the
city
in
his
work
titled
"Politics".
Aristotle's
conception
of
the
city
is
organic,
and
he
is
considered
one
of
the
first
to
conceive
of
the
city
in
this
manner.
Aristotle
considered
the
city
to
be
a
natural
community.
Moreover,
he
considered
the
city
to
be
prior
to
the
family
which
in
turn
is
prior
to
the
individual,
i.e.,
last
in
the
order
of
becoming,
but
first
in
the
order
of
being.
He
is
also
famous
for
his
statement
that
"man
is
by
nature
a
political
animal."
Aristotle
conceived
of
politics
as
being
like
an
organism
rather
than
like
a
machine,
and
as
a
collection
of
parts
none
of
which
can
exist
without
the
others.
It
should
be
noted
that
the
modern
understanding
of
a
political
community
is
that
of
the
state.
However,
the
state
was
foreign
to
Aristotle.
He
referred
to
political
communities
as
cities.
Aristotle
understood
a
city
as
a
political
"partnership".
Subsequently,
a
city
is
created
not
to
avoid
injustice
or
for
economic
stability,
but
rather
to
live
a
good
life:
"The
political
partnership
must
be
regarded,
therefore,
as
being
for
the
sake
of
noble
actions,
not
for
the
sake
of
living
together".
This
can
be
distinguished
from
the
social
contract
theory
which
individuals
leave
the
state
of
nature
because
of
"fear
of
violent
death"
or
its
"inconveniences."
Rhetoric
and
poetics.
Aristotle
considered
epic
poetry,
tragedy,
comedy,
dithyrambic
poetry
and
music
to
be
imitative,
each
varying
in
imitation
by
medium,
object,
and
manner.
For
example,
music
imitates
with
the
media
of
rhythm
and
harmony,
whereas
dance
imitates
with
rhythm
alone,
and
poetry
with
language.
The
forms
also
differ
in
their
object
of
imitation.
Comedy,
for
instance,
is
a
dramatic
imitation
of
men
worse
than
average;
whereas
tragedy
imitates
men
slightly
better
than
average.
Lastly,
the
forms
differ
in
their
manner
of
imitation
–
through
narrative
or
character,
through
change
or
no
change,
and
through
drama
or
no
drama.
Aristotle
believed
that
imitation
is
natural
to
mankind
and
constitutes
one
of
mankind's
advantages
over
animals.
While
it
is
believed
that
Aristotle's
"Poetics"
comprised
two
books
–
one
on
comedy
and
one
on
tragedy
–
only
the
portion
that
focuses
on
tragedy
has
survived.
Aristotle
taught
that
tragedy
is
composed
of
six
elements:
plot-structure,
character,
style,
spectacle,
and
lyric
poetry.
The
characters
in
a
tragedy
are
merely
a
means
of
driving
the
story;
and
the
plot,
not
the
characters,
is
the
chief
focus
of
tragedy.
Tragedy
is
the
imitation
of
action
arousing
pity
and
fear,
and
is
meant
to
effect
the
catharsis
of
those
same
emotions.
Aristotle
concludes
"Poetics"
with
a
discussion
on
which,
if
either,
is
superior:
epic
or
tragic
mimesis.
He
suggests
that
because
tragedy
possesses
all
the
attributes
of
an
epic,
possibly
possesses
additional
attributes
such
as
spectacle
and
music,
is
more
unified,
and
achieves
the
aim
of
its
mimesis
in
shorter
scope,
it
can
be
considered
superior
to
epic.
Aristotle
was
a
keen
systematic
collector
of
riddles,
folklore,
and
proverbs;
he
and
his
school
had
a
special
interest
in
the
riddles
of
the
Delphic
Oracle
and
studied
the
fables
of
Aesop.
Modern
scholarship.
Modern
scholarship
reveals
that
Aristotle's
"lost"
works
stray
considerably
in
characterization
from
the
surviving
Aristotelian
corpus.
Whereas
the
lost
works
appear
to
have
been
originally
written
with
an
intent
for
subsequent
publication,
the
surviving
works
do
not
appear
to
have
been
so.
Rather
the
surviving
works
mostly
resemble
lectures
unintended
for
publication.
The
authenticity
of
a
portion
of
the
surviving
works
as
originally
Aristotelian
is
also
today
held
suspect,
with
some
books
duplicating
or
summarizing
each
other,
the
authorship
of
one
book
questioned
and
another
book
considered
to
be
unlikely
Aristotle's
at
all.
Some
of
the
individual
works
within
the
corpus,
including
the
"Constitution
of
Athens,"
are
regarded
by
most
scholars
as
products
of
Aristotle's
"school,"
perhaps
compiled
under
his
direction
or
supervision.
Others,
such
as
"On
Colors,"
may
have
been
produced
by
Aristotle's
successors
at
the
Lyceum,
e.g.,
Theophrastus
and
Straton.
Still
others
acquired
Aristotle's
name
through
similarities
in
doctrine
or
content,
such
as
the
"De
Plantis,"
possibly
by
Nicolaus
of
Damascus.
Other
works
in
the
corpus
include
medieval
palmistries
and
astrological
and
magical
texts
whose
connections
to
Aristotle
are
purely
fanciful
and
self-promotional.
Loss
of
his
works.
According
to
a
distinction
that
originates
with
Aristotle
himself,
his
writings
are
divisible
into
two
groups:
the
"exoteric"
and
the
"esoteric".
Most
scholars
have
understood
this
as
a
distinction
between
works
Aristotle
intended
for
the
public
(exoteric),
and
the
more
technical
works
(esoteric)
intended
for
the
narrower
audience
of
Aristotle's
students
and
other
philosophers
who
were
familiar
with
the
jargon
and
issues
typical
of
the
Platonic
and
Aristotelian
schools.
Another
common
assumption
is
that
none
of
the
exoteric
works
is
extant
–
that
all
of
Aristotle's
extant
writings
are
of
the
esoteric
kind.
Current
knowledge
of
what
exactly
the
exoteric
writings
were
like
is
scant
and
dubious,
though
many
of
them
may
have
been
in
dialogue
form.
("Fragments"
of
some
of
Aristotle's
dialogues
have
survived.)
Perhaps
it
is
to
these
that
Cicero
refers
when
he
characterized
Aristotle's
writing
style
as
"a
river
of
gold";
it
is
hard
for
many
modern
readers
to
accept
that
one
could
seriously
so
admire
the
style
of
those
works
currently
available
to
us.
However,
some
modern
scholars
have
warned
that
we
cannot
know
for
certain
that
Cicero's
praise
was
reserved
specifically
for
the
exoteric
works;
a
few
modern
scholars
have
actually
admired
the
concise
writing
style
found
in
Aristotle's
extant
works.
One
major
question
in
the
history
of
Aristotle's
works,
then,
is
how
were
the
exoteric
writings
all
lost,
and
how
did
the
ones
we
now
possess
come
to
us?
The
story
of
the
original
manuscripts
of
the
esoteric
treatises
is
described
by
Strabo
in
his
"Geography"
and
Plutarch
in
his
"Parallel
Lives".
The
manuscripts
were
left
from
Aristotle
to
his
successor
Theophrastus,
who
in
turn
willed
them
to
Neleus
of
Scepsis.
Neleus
supposedly
took
the
writings
from
Athens
to
Scepsis,
where
his
heirs
let
them
languish
in
a
cellar
until
the
first
century
BC,
when
Apellicon
of
Teos
discovered
and
purchased
the
manuscripts,
bringing
them
back
to
Athens.
According
to
the
story,
Apellicon
tried
to
repair
some
of
the
damage
that
was
done
during
the
manuscripts'
stay
in
the
basement,
introducing
a
number
of
errors
into
the
text.
When
Lucius
Cornelius
Sulla
occupied
Athens
in
86
BC,
he
carried
off
the
library
of
Apellicon
to
Rome,
where
they
were
first
published
in
60
BC
by
the
grammarian
Tyrannion
of
Amisus
and
then
by
philosopher
Andronicus
of
Rhodes.
Carnes
Lord
attributes
the
popular
belief
in
this
story
to
the
fact
that
it
provides
"the
most
plausible
explanation
for
the
rapid
eclipse
of
the
Peripatetic
school
after
the
middle
of
the
third
century,
and
for
the
absence
of
widespread
knowledge
of
the
specialized
treatises
of
Aristotle
throughout
the
Hellenistic
period,
as
well
as
for
the
sudden
reappearance
of
a
flourishing
Aristotelianism
during
the
first
century
B.C."
Lord
voices
a
number
of
reservations
concerning
this
story,
however.
First,
the
condition
of
the
texts
is
far
too
good
for
them
to
have
suffered
considerable
damage
followed
by
Apellicon's
inexpert
attempt
at
repair.
Second,
there
is
"incontrovertible
evidence,"
Lord
says,
that
the
treatises
were
in
circulation
during
the
time
in
which
Strabo
and
Plutarch
suggest
they
were
confined
within
the
cellar
in
Scepsis.
Third,
the
definitive
edition
of
Aristotle's
texts
seems
to
have
been
made
in
Athens
some
fifty
years
before
Andronicus
supposedly
compiled
his.
And
fourth,
ancient
library
catalogues
predating
Andronicus'
intervention
list
an
Aristotelian
corpus
quite
similar
to
the
one
we
currently
possess.
Lord
sees
a
number
of
post-Aristotelian
interpolations
in
the
"Politics",
for
example,
but
is
generally
confident
that
the
work
has
come
down
to
us
relatively
intact.
As
the
influence
of
the
"falsafa"
grew
in
the
West,
in
part
due
to
Gerard
of
Cremona's
translations
and
the
spread
of
Averroism,
the
demand
for
Aristotle's
works
grew.
William
of
Moerbeke
translated
a
number
of
them
into
Latin.
When
Thomas
Aquinas
wrote
his
theology,
working
from
Moerbeke's
translations,
the
demand
for
Aristotle's
writings
grew
and
the
Greek
manuscripts
returned
to
the
West,
stimulating
a
revival
of
Aristotelianism
in
Europe,
and
ultimately
revitalizing
European
thought
through
Muslim
influence
in
Spain
to
fan
the
embers
of
the
Renaissance.
Development
of
logic.
Twenty-three
hundred
years
after
his
death,
Aristotle
remains
one
of
the
most
influential
people
who
ever
lived.
He
was
the
founder
of
formal
logic,
pioneered
the
study
of
zoology,
and
left
every
future
scientist
and
philosopher
in
his
debt
through
his
contributions
to
the
scientific
method.
Despite
these
accolades,
many
of
Aristotle's
errors
held
back
science
considerably.
Bertrand
Russell
notes
that
"almost
every
serious
intellectual
advance
has
had
to
begin
with
an
attack
on
some
Aristotelian
doctrine".
Russell
also
refers
to
Aristotle's
ethics
as
"repulsive",
and
calls
his
logic
"as
definitely
antiquated
as
Ptolemaic
astronomy".
Russell
notes
that
these
errors
make
it
difficult
to
do
historical
justice
to
Aristotle,
until
one
remembers
how
large
of
an
advance
he
made
upon
all
of
his
predecessors.
Of
course,
the
problem
of
excessive
devotion
to
Aristotle
is
more
a
problem
of
those
later
centuries
and
not
of
Aristotle
himself.
Later
Greek
philosophers.
The
immediate
influence
of
Aristotle's
work
was
felt
as
the
Lyceum
grew
into
the
Peripatetic
school.
Aristotle's
notable
students
included
Aristoxenus,
Dicaearchus,
Demetrius
of
Phalerum,
Eudemos
of
Rhodes,
Harpalus,
Hephaestion,
Meno,
Mnason
of
Phocis,
Nicomachus,
and
Theophrastus.
Aristotle's
influence
over
Alexander
the
Great
is
seen
in
the
latter's
bringing
with
him
on
his
expedition
a
host
of
zoologists,
botanists,
and
researchers.
He
had
also
learned
a
great
deal
about
Persian
customs
and
traditions
from
his
teacher.
Although
his
respect
for
Aristotle
was
diminished
as
his
travels
made
it
clear
that
much
of
Aristotle's
geography
was
clearly
wrong,
when
the
old
philosopher
released
his
works
to
the
public,
Alexander
complained
"Thou
hast
not
done
well
to
publish
thy
acroamatic
doctrines;
for
in
what
shall
I
surpass
other
men
if
those
doctrines
wherein
I
have
been
trained
are
to
be
all
men's
common
property?"
Influence
on
Christian
theologians.
Aristotle
is
referred
to
as
"The
Philosopher"
by
Scholastic
thinkers
such
as
Thomas
Aquinas.
See
"Summa
Theologica",
Part
I,
Question
3,
etc.
These
thinkers
blended
Aristotelian
philosophy
with
Christianity,
bringing
the
thought
of
Ancient
Greece
into
the
Middle
Ages.
It
required
a
repudiation
of
some
Aristotelian
principles
for
the
sciences
and
the
arts
to
free
themselves
for
the
discovery
of
modern
scientific
laws
and
empirical
methods.
The
medieval
English
poet
Chaucer
describes
his
student
as
being
happy
by
having
The
Italian
poet
Dante
says
of
Aristotle
in
the
first
circles
of
hell,
Views
on
women.
Aristotle
believed
that
women
are
colder
than
men
and
thus
a
lower
form
of
life.
His
assumption
carried
forward
unexamined
to
Galen
and
others
for
almost
two
thousand
years
until
the
sixteenth
century.
He
also
believed
that
females
could
not
be
fully
human.
His
analysis
of
procreation
is
frequently
criticized
on
the
grounds
that
it
presupposes
an
active,
ensouling
masculine
element
bringing
life
to
an
inert,
passive,
lumpen
female
element;
it
is
on
these
grounds
that
Aristotle
is
considered
by
some
feminist
critics
to
have
been
a
misogynist.
On
the
other
hand,
Aristotle
gave
equal
weight
to
women's
happiness
as
he
did
to
men's,
and
commented
in
his
Rhetoric
that
a
society
cannot
be
happy
unless
women
are
happy
too.
In
places
like
Sparta
where
the
lot
of
women
is
bad,
there
can
only
be
half-happiness
in
society.(see
Rhetoric
1.5.6)
Post-Enlightenment
thinkers.
The
German
philosopher
Friedrich
Nietzsche
has
been
said
to
have
taken
nearly
all
of
his
political
philosophy
from
Aristotle.
However
implausible
this
is,
it
is
certainly
the
case
that
Aristotle's
rigid
separation
of
action
from
production,
and
his
justification
of
the
subservience
of
slaves
and
others
to
the
virtue
–
or
"arete"
–
of
a
few
justified
the
ideal
of
aristocracy.
It
is
Martin
Heidegger,
not
Nietzsche,
who
elaborated
a
new
interpretation
of
Aristotle,
intended
to
warrant
his
deconstruction
of
scholastic
and
philosophical
tradition.
More
recently,
Alasdair
MacIntyre
has
attempted
to
reform
what
he
calls
the
Aristotelian
tradition
in
a
way
that
is
anti-elitist
and
capable
of
disputing
the
claims
of
both
liberals
and
Nietzscheans.
List
of
works.
The
works
of
Aristotle
that
have
survived
from
antiquity
through
Mediæval
manuscript
transmission
are
collected
in
the
Corpus
Aristotelicum.
These
texts,
as
opposed
to
Aristotle's
lost
works,
are
technical
philosophical
treatises
from
within
Aristotle's
school.
Reference
to
them
is
made
according
to
the
organization
of
Immanuel
Bekker's
Royal
Prussian
Academy
edition
("Aristotelis
Opera
edidit
Academia
Regia
Borussica",
Berlin,
1831-1870),
which
in
turn
is
based
on
ancient
classifications
of
these
works.
Further
reading.
The
secondary
literature
on
Aristotle
is
vast.
The
following
references
are
only
a
small
selection.
---END.OF.DOCUMENT---
An
American
in
Paris.
"An
American
in
Paris"
is
a
symphonic
composition
by
American
composer
George
Gershwin,
composed
in
1928.
Inspired
by
time
Gershwin
had
spent
in
Paris,
it
is
in
the
form
of
an
extended
tone
poem
evoking
the
sights
and
energy
of
the
French
capital
in
the
1920s.
It
is
one
of
Gershwin's
best-known
compositions.
Gershwin
composed
the
piece
on
commission
from
the
New
York
Philharmonic.
He
also
did
the
orchestration.
(He
did
not
orchestrate
his
musicals.)
Gershwin
scored
"An
American
in
Paris"
for
the
standard
instruments
of
the
symphony
orchestra
plus
celesta,
saxophone,
and
automobile
horns.
Gershwin
brought
back
some
Parisian
taxi
horns
for
the
New
York
premiere
of
the
composition
which
took
place
on
December
13,
1928
in
Carnegie
Hall
with
Walter
Damrosch
conducting
the
New
York
Philharmonic.
Gershwin
collaborated
on
the
original
program
notes
with
the
critic
and
composer
Deems
Taylor,
noting
that:
"My
purpose
here
is
to
portray
the
impression
of
an
American
visitor
in
Paris
as
he
strolls
about
the
city
and
listens
to
various
street
noises
and
absorbs
the
French
atmosphere."
When
the
tone
poem
moves
into
the
blues,
"our
American
friend...
has
succumbed
to
a
spasm
of
homesickness."
But,
"nostalgia
is
not
a
fatal
disease."
The
American
visitor
"once
again
is
an
alert
spectator
of
Parisian
life"
and
"the
street
noises
and
French
atmosphere
are
triumphant."
Instrumentation.
"An
American
in
Paris"
is
scored
for
3
flutes
(3rd
doubling
on
piccolo),
2
oboes,
English
horn,
2
clarinets
in
B
flat,
bass
clarinet
in
B
flat,
2
bassoons,
4
horns
in
F,
3
trumpets
in
B
flat,
3
trombones,
tuba,
timpani,
snare
drum,
bass
drum,
cymbals,
low
and
high
tom-toms,
xylophone,
glockenspiel,
celesta,
4
taxi
horns,
alto
saxophone/soprano
saxophone,
tenor
saxophone/soprano
saxophone/alto
saxophone,
baritone
saxophone/soprano
saxophone/alto
saxophone,
and
strings.
The
revised
edition
by
F
Campbell-Watson
calls
for
three
saxophones,
alto,
tenor
and
baritone.
In
this
arrangement
the
soprano
and
alto
doublings
have
been
rewritten
to
avoid
changing
instruments.
Recordings.
"An
American
in
Paris"
has
been
frequently
recorded
over
the
years.
The
very
first
recording
was
made
for
RCA
Victor
in
1929
with
Nathaniel
Shilkret
conducting
the
Victor
Symphony
Orchestra,
drawn
from
members
of
the
Philadelphia
Orchestra.
Gershwin
was
on
hand
to
"supervise"
the
recording;
however,
Shilkret
was
reported
to
be
in
charge
and
eventually
asked
the
composer
to
leave
the
recording
studio.
Then,
a
little
later,
Shilkret
discovered
there
was
no
one
to
play
the
brief
celesta
solo
during
the
slow
section,
so
he
hastily
asked
Gershwin
if
he
might
play
the
solo;
Gershwin
said
he
could
and
so
he
briefly
participated
in
the
actual
recording.
The
radio
broadcast
of
the
September
8,
1937
Hollywood
Bowl
George
Gershwin
Memorial
Concert,
in
which
"An
American
in
Paris,"
also
conducted
by
Shilkret,
was
second
on
the
program,
was
recorded
and
was
released
in
1998
in
a
two-CD
set.
Arthur
Fiedler
and
the
Boston
Pops
Orchestra
recorded
the
work
for
RCA
Victor,
including
one
of
the
first
stereo
recordings
of
the
music.
In
1945,
Arturo
Toscanini
and
the
NBC
Symphony
Orchestra
recorded
the
music
in
Carnegie
Hall,
one
of
the
few
commercial
recordings
Toscanini
made
of
music
by
an
American
composer.
The
Seattle
Symphony
also
recorded
a
version
in
the
1980's
of
Gershwin's
original
score,
before
he
committed
to
numerous
edits
resulting
in
the
score
as
we
hear
it
today.
In
1951,
MGM
released
a
musical
comedy,
"An
American
in
Paris",
featuring
Gene
Kelly
and
Leslie
Caron.
Winner
of
numerous
awards,
including
the
1951
Best
Picture
Oscar,
the
film
was
directed
by
Vincente
Minnelli,
featured
many
tunes
of
Gershwin,
and
concluded
with
an
extensive,
elaborate
dance
sequence
built
around
Gershwin's
symphonic
poem
(arranged
for
the
film
by
Johnny
Green).
A
part
of
the
symphonic
composition
is
also
featured
in
"As
Good
as
It
Gets",
released
in
1997.
---END.OF.DOCUMENT---
Academy
Award
for
Best
Art
Direction.
The
Academy
Awards
are
the
oldest
awards
ceremony
for
achievements
in
motion
pictures.
The
Academy
Award
for
Best
Art
Direction
recognizes
achievement
in
art
direction
on
a
film.
The
films
below
are
listed
with
their
production
year,
so
the
Oscar
2000
for
best
art
direction
went
to
a
film
from
1999.
In
the
lists
below,
the
winner
of
the
award
for
each
year
is
shown
first,
followed
by
the
other
nominees.
1920s.
This
award
was
originally
for
Interior
Decoration
1930s.
With
the
awards
for
1940
the
award
was
divided
into
separate
awards
for
black-and-white
and
color
movies.
1940s.
Beginning
with
1947
movies
the
name
of
the
award
was
changed
to
Art
Direction
-
Set
Decoration.
1950s.
For
1957
films
this
award
became
a
single
award.
With
the
1959
films
this
category
was
again
divided
in
two
1960s.
For
1967
the
two
awards
in
this
category
were
recombined
into
a
single
award.
---END.OF.DOCUMENT---
Academy
Award.
The
Academy
Award
(frequently
known
as
the
Oscars)
are
accolades
presented
annually
by
the
Academy
of
Motion
Picture
Arts
and
Sciences
(AMPAS)
to
recognize
excellence
of
professionals
in
the
film
industry,
including
directors,
actors,
and
writers.
The
formal
ceremony
at
which
the
awards
are
presented
is
one
of
the
most
prominent
award
ceremonies
in
the
world.
It
is
also
the
oldest
award
ceremony
in
the
media,
and
many
other
award
ceremonies
such
as
the
Grammy
Awards
(for
music),
Golden
Globe
Awards
(all
forms
of
visual
media),
and
Emmy
Awards
(for
television)
are
often
modeled
from
the
Academy.
The
Academy
of
Motion
Picture
Arts
and
Sciences
itself
was
conceived
by
Metro-Goldwyn-Mayer
studio
boss
Louis
B.
Mayer.
The
1st
Academy
Awards
ceremony
was
held
on
Thursday,
May
16,
1929,
at
the
Hotel
Roosevelt
in
Hollywood
to
honor
outstanding
film
achievements
of
1927
and
1928.
It
was
hosted
by
actor
Douglas
Fairbanks
and
director
William
C.
deMille.
The
82nd
Academy
Awards,
honoring
the
best
in
film
for
2009,
was
held
on
Sunday,
March
7,
2010,
at
the
Kodak
Theatre
in
Hollywood,
with
actors
Steve
Martin
and
Alec
Baldwin
hosting
the
ceremony.
History.
The
first
awards
were
presented
on
May
16,
1929,
at
a
private
brunch
at
the
Hollywood
Roosevelt
Hotel
with
an
audience
of
about
270
people..
The
cost
of
guest
tickets
for
that
night's
ceremony
was
$5.
Fifteen
statuettes
were
awarded,
honoring
artists,
directors
and
other
personalities
of
the
filmmaking
industry
of
the
time
for
their
works
during
the
1927-1928
period.
Winners
had
been
announced
three
months
earlier
of
their
triumphs;
however
that
was
changed
in
the
second
ceremony
of
the
Academy
Awards
in
1930.
Since
then
and
during
the
first
decade,
the
results
were
given
to
newspapers
for
publication
at
11pm
on
the
night
of
the
awards.
This
method
was
used
until
the
"Los
Angeles
Times"
announced
the
winners
before
the
ceremony
began;
as
a
result,
the
Academy
has
used
a
sealed
envelope
to
reveal
the
name
of
the
winners
since
1941.
Since
2002,
the
awards
have
been
broadcast
from
the
Kodak
Theatre.
The
first
Best
Actor
awarded
was
Emil
Jannings,
for
his
performance
in
The
Last
Command
and
The
Way
of
All
Flesh.
He
had
to
return
to
Europe
before
the
ceremony,
so
the
Academy
agreed
to
give
him
the
prize
earlier;
this
made
him
the
first
Academy
Award
winner
in
history.
The
honored
professionals
were
awarded
for
all
the
work
done
in
a
certain
category
for
the
qualifying
period;
for
example,
Emil
Jannings,
received
the
award
for
two
movies
he
starred
during
that
period.
Since
the
fourth
ceremony,
the
system
changed,
and
the
professionals
were
honored
for
a
specific
performance
in
a
single
film.
In
the
29th
ceremony,
held
on
March
27th,
1957
the
Best
Foreign
Language
Film
category
was
introduced;
until
then,
foreign
language
films
were
honored
with
the
Special
Achievement
Award.
Design.
Although
there
are
seven
other
types
of
awards
presented
by
the
Academy
(the
Irving
G.
Thalberg
Memorial
Award,
the
Jean
Hersholt
Humanitarian
Award,
the
Gordon
E.
Sawyer
Award,
the
Scientific
and
Engineering
Award,
the
Technical
Achievement
Award,
the
John
A.
Bonner
Medal
of
Commendation,
and
the
Student
Academy
Award),
the
best
known
one
is
the
"Academy
Award
of
Merit"
more
popularly
known
as
the
Oscar
statuette.
Made
of
gold-plated
britannium
on
a
black
metal
base,
it
is
13.5 in
(34 cm)
tall,
weighs
8.5 lb
(3.85 kg)
and
depicts
a
knight
rendered
in
Art
Deco
style
holding
a
crusader's
sword
standing
on
a
reel
of
film
with
five
spokes.
The
five
spokes
each
represent
the
original
branches
of
the
Academy:
Actors,
Writers,
Directors,
Producers,
and
Technicians.
MGM's
art
director
Cedric
Gibbons,
one
of
the
original
Academy
members,
supervised
the
design
of
the
award
trophy
by
printing
the
design
on
a
scroll.
In
need
of
a
model
for
his
statuette
Gibbons
was
introduced
by
his
then
wife
Dolores
del
Río
to
Mexican
film
director
and
actor
Emilio
"El
Indio"
Fernández.
Reluctant
at
first,
Fernández
was
finally
convinced
to
pose
nude
to
create
what
today
is
known
as
the
"Oscar".
Then,
sculptor
George
Stanley
(who
also
did
the
Muse
Fountain
at
the
Hollywood
Bowl)
sculpted
Gibbons's
design
in
clay
and
Sachin
Smith
cast
the
statuette
in
92.5
percent
tin
and
7.5
percent
copper
and
then
gold-plated
it.
The
only
addition
to
the
Oscar
since
it
was
created
is
a
minor
streamlining
of
the
base.
The
original
Oscar
mold
was
cast
in
1928
at
the
C.W.
Shumway
&
Sons
Foundry
in
Batavia,
Illinois,
which
also
contributed
to
casting
the
molds
for
the
Vince
Lombardi
Trophy
and
Emmy
Awards
statuettes.
Since
1983,
approximately
50
Oscars
are
made
each
year
in
Chicago,
Illinois
by
manufacturer
R.S.
Owens
&
Company.
In
support
of
the
American
effort
in
World
War
II,
the
statuettes
were
made
of
plaster
and
were
traded
in
for
gold
ones
after
the
war
had
ended.
Naming.
The
root
of
the
name
"Oscar"
is
contested.
One
biography
of
Bette
Davis
claims
that
she
named
the
Oscar
after
her
first
husband,
band
leader
Harmon
Oscar
Nelson;
one
of
the
earliest
mentions
in
print
of
the
term
"Oscar"
dates
back
to
a
"Time"
magazine
article
about
the
1934
6th
Academy
Awards
and
to
Bette
Davis's
receipt
of
the
award
in
1936.
Walt
Disney
is
also
quoted
as
thanking
the
Academy
for
his
Oscar
as
early
as
1932.
Another
claimed
origin
is
that
the
Academy's
Executive
Secretary,
Margaret
Herrick,
first
saw
the
award
in
1931
and
made
reference
to
the
statuette's
reminding
her
of
her
"Uncle
Oscar"
(a
nickname
for
her
cousin
Oscar
Pierce).
Columnist
was
present
during
Herrick's
naming
and
seized
the
name
in
his
byline,
"Employees
have
affectionately
dubbed
their
famous
statuette
'Oscar'".
The
trophy
was
officially
dubbed
the
"Oscar"
in
1939
by
the
Academy
of
Motion
Pictures
Arts
and
Sciences.
Another
legend
reports
that
the
Norwegian-American
Eleanor
Lilleberg,
executive
secretary
to
Louis
B.
Mayer,
saw
the
first
statuette
and
exclaimed,
"It
looks
like
King
Oscar
II!".
At
the
end
of
the
day
she
asked,
"What
should
we
do
with
Oscar,
put
him
in
the
vault?"
and
the
name
stuck.
As
of
the
81st
Academy
Awards
ceremony
held
in
2009,
a
total
of
2,744
Oscars
have
been
given
for
1,798
awards.
A
total
of
297
actors
have
won
Oscars
in
competitive
acting
categories
or
been
awarded
Honorary
or
Juvenile
Awards.
Ownership
of
Oscar
statuettes.
Since
1950,
the
statuettes
have
been
legally
encumbered
by
the
requirement
that
neither
winners
nor
their
heirs
may
sell
the
statuettes
without
first
offering
to
sell
them
back
to
the
Academy
for
US$1.
If
a
winner
refuses
to
agree
to
this
stipulation,
then
the
Academy
keeps
the
statuette.
Academy
Awards
not
protected
by
this
agreement
have
been
sold
in
public
auctions
and
private
deals
for
six-figure
sums.
This
rule
is
highly
controversial,
since
while
the
Oscar
is
under
the
ownership
of
the
recipient,
it
is
essentially
not
on
the
open
market.
The
case
of
Michael
Todd's
grandson
trying
to
sell
Todd's
Oscar
statuette
illustrates
that
there
are
many
who
do
not
agree
with
this
idea.
When
Todd's
grandson
attempted
to
sell
Todd's
Oscar
statuette
to
a
movie
prop
collector,
the
Academy
won
the
legal
battle
by
getting
a
permanent
injunction.
Although
some
Oscar
sales
transactions
have
been
successful,
the
buyers
have
subsequently
returned
the
statuettes
to
the
Academy,
which
keeps
them
in
its
treasury.
Nomination.
Since
2004,
Academy
Award
nomination
results
have
been
announced
to
the
public
in
late
January.
Prior
to
2004,
nomination
results
were
announced
publicly
in
early
February.
Voters.
The
Academy
of
Motion
Picture
Arts
and
Sciences
(AMPAS),
a
professional
honorary
organization,
maintains
a
voting
membership
of
5,835
as
of
2007.
Actors
constitute
the
largest
voting
bloc,
numbering
1,311
members
(22
percent)
of
the
Academy's
composition.
Votes
have
been
certified
by
the
auditing
firm
PricewaterhouseCoopers
(and
its
predecessor
Price
Waterhouse)
for
the
past
73
annual
awards
ceremonies.
All
AMPAS
members
must
be
invited
to
join
by
the
Board
of
Governors,
on
behalf
of
Academy
Branch
Executive
Committees.
Membership
eligibility
may
be
achieved
by
a
competitive
nomination
or
a
member
may
submit
a
name
based
on
other
significant
contribution
to
the
field
of
motion
pictures.
New
membership
proposals
are
considered
annually.
The
Academy
does
not
publicly
disclose
its
membership,
although
as
recently
as
2007
press
releases
have
announced
the
names
of
those
who
have
been
invited
to
join.
The
2007
release
also
stated
that
it
has
just
under
6,000
voting
members.
While
the
membership
had
been
growing,
stricter
policies
have
kept
its
size
steady
since
then.
Rules.
Today,
according
to
Rules
2
and
3
of
the
official
Academy
Awards
Rules,
a
film
must
open
in
the
previous
calendar
year,
from
midnight
at
the
start
of
January
1
to
midnight
at
the
end
of
December
31,
in
Los
Angeles
County,
California,
to
qualify.
For
example,
the
2010
Best
Picture
winner,
"The
Hurt
Locker",
was
actually
first
released
in
2008,
but
did
not
qualify
for
the
2009
awards
as
it
did
not
play
its
Oscar-qualifying
run
in
Los
Angeles
until
mid-2009,
thus
qualifying
for
the
2010
awards.
Rule
2
states
that
a
film
must
be
feature-length,
defined
as
a
minimum
of
40
minutes,
except
for
short
subject
awards,
and
it
must
exist
either
on
a
35
mm
or
70
mm
film
print
or
in
24 frame/s
or
48 frame/s
progressive
scan
digital
cinema
format
with
native
resolution
not
less
than
1280x720.
Producers
must
submit
an
Official
Screen
Credits
online
form
before
the
deadline;
in
case
it
is
not
submitted
by
the
defined
deadline,
the
film
will
be
ineligible
for
Academy
Awards
in
any
year.
The
form
includes
the
production
credits
for
all
related
categories.
Then,
each
form
is
checked
and
put
in
a
Reminder
List
of
Eligible
Releases.
In
late
December
ballots
and
copies
of
the
Reminder
List
of
Eligible
Releases
are
mailed
to
around
6000
active
members.
For
most
categories,
members
from
each
of
the
branches
vote
to
determine
the
nominees
only
in
their
respective
categories
(i.e.
only
directors
vote
for
directors,
writers
for
writers,
actors
for
actors,
etc.);
there
are
some
exceptions
though
in
the
case
of
certain
categories,
like
Foreign
Film,
Documentary
and
Animated
Feature
Film
in
which
movies
are
selected
by
special
screening
committees
made
up
of
member
from
all
branches.
In
the
special
case
of
Best
Picture,
all
voting
members
are
eligible
to
select
the
nominees
for
that
category.
Foreign
films
must
include
English
subtitles,
and
each
country
can
only
submit
one
film
per
year.
The
members
of
the
various
branches
nominate
those
in
their
respective
fields
while
all
members
may
submit
nominees
for
Best
Picture.
The
winners
are
then
determined
by
a
second
round
of
voting
in
which
all
members
are
then
allowed
to
vote
in
most
categories,
including
Best
Picture.
Telecast.
The
major
awards
are
presented
at
a
live
televised
ceremony,
most
commonly
in
February
or
March
following
the
relevant
calendar
year,
and
six
weeks
after
the
announcement
of
the
nominees.
It
is
the
culmination
of
the
film
awards
season,
which
usually
begins
during
November
or
December
of
the
previous
year.
This
is
an
elaborate
extravaganza,
with
the
invited
guests
walking
up
the
red
carpet
in
the
creations
of
the
most
prominent
fashion
designers
of
the
day.
Black
tie
dress
is
the
most
common
outfit
for
men,
although
fashion
may
dictate
not
wearing
a
bow-tie,
and
musical
performers
sometimes
do
not
adhere
to
this.
(The
artists
who
recorded
the
nominees
for
Best
Original
Song
quite
often
perform
those
songs
live
at
the
awards
ceremony,
and
the
fact
that
they
are
performing
is
often
used
to
promote
the
television
broadcast.)
The
Academy
Awards
is
televised
live
across
the
United
States
(excluding
Alaska
and
Hawaii),
Canada,
the
United
Kingdom,
and
gathers
millions
of
viewers
elsewhere
throughout
the
world.
The
2007
ceremony
was
watched
by
more
than
40
million
Americans.
Other
awards
ceremonies
(such
as
the
Emmys,
Golden
Globes,
and
Grammys)
are
broadcast
live
in
the
East
Coast
but
are
on
tape
delay
in
the
West
Coast
and
might
not
air
on
the
same
day
outside
North
America
(if
the
awards
are
even
televised).
The
Academy
has
for
several
years
claimed
that
the
award
show
has
up
to
a
billion
viewers
internationally,
but
this
has
so
far
not
been
confirmed
by
any
independent
sources.
The
usual
extension
of
this
claim
is
that
only
the
Super
Bowl,
Olympics
Opening
Ceremonies,
and
FIFA
World
Cup
Final
draw
higher
viewership.
The
Awards
show
was
first
televised
on
NBC
in
1953.
NBC
continued
to
broadcast
the
event
until
1960
when
the
ABC
Network
took
over,
televising
the
festivities
through
1970,
after
which
NBC
resumed
the
broadcasts.
ABC
once
again
took
over
broadcast
duties
in
1976;
it
is
under
contract
to
do
so
through
the
year
2014.
After
more
than
sixty
years
of
being
held
in
late
March
or
early
April,
the
ceremonies
were
moved
up
to
late
February
or
early
March
starting
in
2004
to
help
disrupt
and
shorten
the
intense
lobbying
and
ad
campaigns
associated
with
Oscar
season
in
the
film
industry.
Another
reason
was
because
of
the
growing
TV
ratings
success
of
the
NCAA
Men's
Division
I
Basketball
Championship,
which
would
cut
into
the
Academy
Awards
audience.
The
earlier
date
is
also
to
the
advantage
of
ABC,
as
it
now
usually
occurs
during
the
highly
profitable
and
important
February
sweeps
period.
(Some
years,
the
ceremony
is
moved
into
early
March
in
deference
to
the
Winter
Olympics.)
Advertising
is
somewhat
restricted,
however,
as
traditionally
no
movie
studios
or
competitors
of
official
Academy
Award
sponsors
may
advertize
during
the
telecast.
The
Awards
show
holds
the
distinction
of
having
won
the
most
Emmys
in
history,
with
38
wins
and
167
nominations.
After
many
years
of
being
held
on
Mondays
at
9:00
p.m.
Eastern/6:00
p.m
Pacific,
in
1999
the
ceremonies
were
moved
to
Sundays
at
8:30
p.m.
Eastern/5:30
p.m.
Pacific.
The
reasons
given
for
the
move
were
that
more
viewers
would
tune
in
on
Sundays,
that
Los
Angeles
rush-hour
traffic
jams
could
be
avoided,
and
that
an
earlier
start
time
would
allow
viewers
on
the
East
Coast
to
go
to
bed
earlier.
For
many
years
the
film
industry
had
opposed
a
Sunday
broadcast
because
it
would
cut
into
the
weekend
box
office.
On
March
30,
1981,
the
awards
ceremony
was
postponed
for
one
day
after
the
shooting
of
President
Ronald
Reagan
and
others
in
Washington
DC.
In
1993
an
"In
Memoriam"
section
was
introduced,
honoring
those
who
had
made
a
significant
contribution
to
cinema
who
had
died
in
the
preceding
12
months.
This
section
has
led
to
some
criticism
for
omission
of
notable
persons
such
as
Leonard
Schrader
and
Malcolm
Arnold
in
2007
and
Gene
Barry,
Farrah
Fawcett,
Henry
Gibson,
and
Bea
Arthur
in
2010.
Since
2002,
celebrities
have
been
seen
arriving
at
the
Academy
Awards
in
hybrid
vehicles;
during
the
telecast
of
the
79th
Academy
Awards
in
2007,
Leonardo
DiCaprio
and
former
vice
president
Al
Gore
announced
that
ecologically
intelligent
practices
had
been
integrated
into
the
planning
and
execution
of
the
Oscar
presentation
and
several
related
events.
In
2010,
the
organizers
of
the
Academy
Awards
announced
that
winners'
acceptance
speeches
must
not
run
past
45
seconds.
This,
according
to
organizer
Bill
Mechanic,
was
to
ensure
the
elimination
of
what
he
termed
"the
single
most
hated
thing
on
the
show"
-
overly
long
and
embarrassing
displays
of
emotion.
Ratings.
Historically,
the
"Oscarcast"
has
pulled
in
a
bigger
haul
when
box-office
hits
are
favored
to
win
the
Best
Picture
trophy.
More
than
57.25
million
viewers
tuned
to
the
telecast
in
1998,
the
year
of
"Titanic",
which
generated
close
to
US$600
million
at
the
North
American
box
office
pre-Oscars.
The
76th
Academy
Awards
ceremony
in
which
'
(pre-telecast
box
office
earnings
of
US$368
million)
received
11
Awards
including
Best
Picture
drew
43.56
million
viewers.
The
most
watched
ceremony
based
on
Nielsen
ratings
to
date,
however,
was
the
42nd
Academy
Awards
(Best
Picture
"Midnight
Cowboy")
which
drew
a
43.4%
household
rating
on
April
7,
1970.
By
contrast,
ceremonies
honoring
films
that
have
not
performed
well
at
the
box
office
tend
to
show
weaker
ratings.
The
78th
Academy
Awards
which
awarded
low-budgeted,
independent
film
"Crash"
(with
a
pre-Oscar
gross
of
US$53.4
million)
generated
an
audience
of
38.64
million
with
a
household
rating
of
22.91%.
In
2008,
the
80th
Academy
Awards
telecast
was
watched
by
31.76
million
viewers
on
average
with
an
18.66%
household
rating,
the
lowest
rated
and
least
watched
ceremony
to
date,
in
spite
of
celebrating
80
years
of
the
Academy
Awards.
The
Best
Picture
winner
of
that
particular
ceremony
was
another
low-budget,
independently
financed
film
("No
Country
for
Old
Men").
Venues.
In
1929,
the
1st
Academy
Awards
were
presented
at
a
banquet
dinner
at
the
Hollywood
Roosevelt
Hotel.
From
1930–1943,
the
awards
were
presented
first
at
the
Ambassador
Hotel
in
Hollywood,
and
later
the
Biltmore
Hotel
in
downtown
Los
Angeles.
Grauman's
Chinese
Theater
in
Hollywood
then
hosted
the
awards
from
1944
to
1946,
followed
by
the
Shrine
Auditorium
in
Los
Angeles
from
1947
to
1948.
The
21st
Academy
Awards
in
1949
were
held
at
the
Academy
Award
Theater
at
what
was
the
Academy's
headquarters
on
Melrose
Avenue
in
Hollywood.
From
1950
to
1960,
the
awards
were
presented
at
Hollywood's
Pantages
Theatre.
With
the
advent
of
television,
the
1953–1957
awards
took
place
simultaneously
in
Hollywood
and
New
York
first
at
the
NBC
International
Theatre
(1953)
and
then
at
the
NBC
Century
Theatre
(1954–1957),
after
which
the
ceremony
took
place
solely
in
Los
Angeles.
The
Oscars
moved
to
the
Santa
Monica
Civic
Auditorium
in
Santa
Monica,
California
in
1961.
By
1969,
the
Academy
decided
to
move
the
ceremonies
back
to
Los
Angeles,
this
time
to
the
Dorothy
Chandler
Pavilion
at
the
Los
Angeles
County
Music
Center.
In
2002,
Hollywood's
Kodak
Theatre
became
the
permanent
home
of
the
awards.
Current
awards.
In
the
first
year
of
the
awards,
the
Best
Director
award
was
split
into
two
separate
categories
(Drama
and
Comedy).
At
times,
the
Best
Original
Score
award
has
also
been
split
into
separate
categories
(Drama
and
Comedy/Musical).
From
the
1930s
through
the
1960s,
the
Art
Direction,
Cinematography,
and
Costume
Design
awards
were
likewise
split
into
two
separate
categories
(black-and-white
films
and
color
films).
Special
Academy
Awards.
These
awards
are
voted
on
by
special
committees,
rather
than
by
the
Academy
membership
as
a
whole,
but
the
individual
selected
to
receive
the
special
award
may
decline
the
offer.
They
are
not
always
presented
on
a
consistent
annual
basis.
Criticism.
The
Oscars
are
generally
voted
on
by
members
of
the
entertainment
industry;
thus,
important
films
that
have
had
the
most
people
working
on
them
generally
become
nominated.
Director
William
Friedkin,
an
Oscar
winner
and
producer
of
the
Academy
Awards,
spoke
critically
of
the
awards
at
a
conference
in
New
York
in
2009.
He
characterized
the
Academy
Awards
as
"the
greatest
promotion
scheme
that
any
industry
ever
devised
for
itself".
In
addition,
several
winners
critical
of
the
Academy
Awards
have
boycotted
the
ceremonies
and
refused
to
accept
their
Oscars.
The
first
to
do
so
was
Dudley
Nichols
(Best
Writing
in
1935
for
"The
Informer").
Nichols
boycotted
the
Eighth
Academy
Awards
ceremony
because
of
conflicts
between
the
Academy
and
the
Writer's
Guild.
George
C.
Scott
became
the
second
person
to
refuse
his
award
(Best
Actor
in
1970
for
"Patton"),
at
the
43rd
Academy
Awards
ceremony.
Scott
explained,
"The
whole
thing
is
a
goddamn
meat
parade.
I
don't
want
any
part
of
it."
The
third
winner,
Marlon
Brando,
refused
his
award
(Best
Actor
in
1972
for
"The
Godfather"),
citing
the
film
industry's
discrimination
and
mistreatment
of
Native
Americans.
At
the
45th
Academy
Awards
ceremony,
Brando
sent
Sacheen
Littlefeather
to
read
a
15-page
speech
detailing
Brando's
criticisms.
It
has
been
observed
that
several
of
the
Academy
Award
winners
–
particularly
Best
Picture
–
have
not
stood
the
test
of
time
or
had
defeated
worthier
efforts.
On
"They
Shoot
Pictures,
Don't
They's"
comprehensive
database
of
the
1,000
most
acclaimed
films
of
all
time,
only
eight
of
the
first
hundred
ranked
films
have
won
the
Best
Picture
award.
Tim
Dirks,
editor
of
AMC's
filmsite.org,
has
written
of
the
Academy
Awards,
In
his
review
of
"The
Lives
of
Others",
Nick
Davis
argued,
The
Academy
Awards
have
also
come
under
criticism
for
having
a
bias
towards
certain
types
of
performances
and
film
genres.
The
Best
Picture
prize
has
never
been
given
to
a
film
noir,
science
fiction
or
an
animated
film;
and
rarely
are
horror,
fantasy,
comedy
and
westerns
recognized
by
AMPAS.
Acting
prizes
in
certain
years
have
been
criticized
for
not
recognizing
superior
performances
so
much
as
being
awarded
for
sentimental
reasons,
personal
popularity,
atonement
for
past
mistakes,
or
presented
as
a
"career
honor"
to
recognize
a
distinguished
nominee's
entire
body
of
work.
---END.OF.DOCUMENT---
Actrius.
"Actrius"
(Catalan:
"Actresses")
is
a
1996
film
directed
by
Ventura
Pons.
In
the
film,
there
are
no
male
actors
and
the
four
leading
actresses
dubbed
themselves
in
the
Castilian
version.
Synopsis.
In
order
to
prepare
the
role
of
an
important
old
actress,
a
theatre
student
interviews
three
actresses
who
were
her
pupils:
an
international
diva
(Glòria
Marc,
played
by
Núria
Espert),
a
television
star
(Assumpta
Roca,
played
by
Rosa
Maria
Sardà)
and
a
dubbing
director
(Maria
Caminal,
played
by
Anna
Lizaran).
---END.OF.DOCUMENT---
Animalia
(book).
"Animalia"
(ISBN
0810918684)
is
an
illustrated
children's
book
by
Graeme
Base.
It
was
published
in
1986.
"Animalia"
is
an
alliterative
alphabet
book
and
contains
twenty-six
illustrations,
one
for
each
letter
of
the
alphabet.
Each
illustration
features
an
animal
from
the
animal
kingdom
(A
is
for
alligator,
B
is
for
butterfly,
etc.)
along
with
a
short
poem
utilizing
the
letter
of
the
page
for
many
of
the
words.
The
illustrations
contain
dozens
of
small
objects
beginning
with
that
letter
that
the
curious
reader
can
try
to
identify.
As
an
additional
challenge,
the
author
has
hidden
a
picture
of
himself
as
a
child
in
every
picture.
In
1987,
"Animalia"
won
the
title
of
Honour
Book
in
the
Children's
Book
Council
of
Australia
Picture
Book
of
the
Year
Awards.
In
1996,
a
tenth
anniversary
edition
was
released.
Base
also
published
a
colouring
book
version
for
children
to
do
their
own
colouring.
A
television
series
was
also
created,
based
on
the
book,
which
airs
in
the
United
States,
Australia,
Canada,
the
UK
and
Norway.
It
also
airs
on
Minimax
for
the
Czech
and
Slovak
Republics.
---END.OF.DOCUMENT---
International
Atomic
Time.
International
Atomic
Time
(TAI,
from
the
French
name
Temps
Atomique
International)
is
a
high-precision
atomic
coordinate
time
standard
based
on
the
notional
passage
of
proper
time
on
Earth's
geoid.
It
is
the
principal
realisation
of
Terrestrial
Time,
and
the
basis
for
Coordinated
Universal
Time
(UTC)
which
is
used
for
civil
timekeeping
all
over
the
Earth's
surface.,
TAI
was
exactly
34
seconds
ahead
of
UTC:
an
initial
difference
of
10
seconds
at
the
start
of
1972,
plus
24
leap
seconds
in
UTC
since
1972;
the
last
leap
second
was
added
on
31
December,
2008.
Time
coordinates
on
the
TAI
scales
are
conventionally
specified
using
traditional
means
of
specifying
days,
carried
over
from
non-uniform
time
standards
based
on
the
rotation
of
the
Earth.
Specifically,
both
Julian
Dates
and
the
Gregorian
calendar
are
used.
TAI
in
this
form
was
synchronised
with
Universal
Time
at
the
beginning
of
1958,
and
the
two
have
drifted
apart
ever
since,
due
to
the
changing
motion
of
the
Earth.
Operation.
TAI
as
a
time
scale
is
a
weighted
average
of
the
time
kept
by
over
200
atomic
clocks
in
about
70
national
laboratories
worldwide.
The
clocks
are
compared
using
satellites.
Due
to
the
averaging
it
is
far
more
stable
than
any
clock
would
be
alone.
The
majority
of
the
clocks
are
caesium
clocks;
the
definition
of
the
SI
second
is
written
in
terms
of
caesium.
The
participating
institutions
each
broadcast,
in
real
time
(in
the
present),
a
frequency
signal
with
time
codes,
which
is
their
estimate
of
TAI.
Time
codes
are
usually
published
in
the
form
of
UTC.
These
time
scales
are
denoted
in
the
form
"TAI(NPL)"
("UTC(NPL)"
for
the
UTC
form),
where
"NPL"
in
this
case
identifies
the
National
Physical
Laboratory,
UK.
The
clocks
at
different
institutions
are
regularly
compared
against
each
other.
The
International
Bureau
of
Weights
and
Measures
(BIPM)
combines
these
measurements
to
retrospectively
calculate
the
weighted
average
that
forms
the
most
stable
time
scale
possible.
This
combined
time
scale
is
published
monthly
in
[ftp://ftp2.bipm.fr/pub/tai/publication/cirt/
Circular
T],
and
is
the
canonical
TAI.
This
time
scale
is
expressed
in
the
form
of
tables
of
differences
UTC-UTC("x")
and
TAI-TA("x"),
for
each
participating
institution
"x".
Errors
in
publication
may
be
corrected
by
issuing
a
revision
of
the
faulty
Circular
T
or
by
errata
in
a
subsequent
Circular
T.
Aside
from
this,
once
published
in
Circular
T
the
TAI
scale
is
not
revised.
In
hindsight
it
is
possible
to
discover
errors
in
TAI,
and
to
make
better
estimates
of
the
true
proper
time
scale.
Doing
so
does
not
create
another
version
of
TAI;
it
is
instead
considered
to
be
creating
a
better
realisation
of
Terrestrial
Time
(TT).
History.
Atomic
timekeeping
services
started
experimentally
in
1955,
using
the
first
caesium
atomic
clock
at
the
National
Physical
Laboratory,
UK
(NPL).
Early
atomic
time
scales
consisted
of
quartz
clocks
with
frequencies
calibrated
by
a
single
atomic
clock;
the
atomic
clocks
were
not
operated
continuously.
The
"Greenwich
Atomic"
(GA)
scale
began
in
1955
at
the
Royal
Greenwich
Observatory.
The
United
States
Naval
Observatory
began
the
A.1
scale
13
September
1956,
using
an
Atomichron©
commercial
atomic
clock,
followed
by
the
NBS-A
scale
at
the
National
Bureau
of
Standards,
Boulder,
Colorado.
The
International
Time
Bureau
(BIH)
began
a
time
scale,
Tm
or
AM,
in
July
1955,
using
both
local
caesium
clocks
and
comparisons
to
distant
clocks
using
the
phase
of
VLF
radio
signals.
Both
the
BIH
scale
and
A.1
was
defined
by
an
epoch
at
the
beginning
of
1958:
it
was
set
to
read
Julian
Date
2436204.5
(1
January
1958
00:00:00)
at
the
corresponding
UT2
instant.
The
procedures
used
by
the
BIH
evolved,
and
the
name
for
the
time
scale
changed:
"A3"
in
1963
and
"TA(BIH)"
in
1969.
This
synchronisation
was
inevitably
imperfect,
depending
as
it
did
on
the
astronomical
realisation
of
UT2.
At
the
time,
UT2
as
published
by
various
observatories
differed
by
several
centiseconds.
The
SI
second
was
defined
in
terms
of
the
caesium
atom
in
1967,
and
in
1971
it
was
renamed
International
Atomic
Time
(TAI).
Also
in
1961,
UTC
began.
UTC
is
a
discontinuous
time
scale
composed
from
segments
that
are
linear
transformations
of
atomic
time,
the
discontinuities
being
arranged
so
that
UTC
approximated
UT2
until
the
end
of
1971,
and
UT1
thereafter.
This
was
a
compromise
arrangement
for
a
broadcast
time
scale:
a
linear
transformation
of
the
BIH's
atomic
time
meant
that
the
time
scale
was
stable
and
internationally
synchronised,
while
approximating
UT1
means
that
tasks
such
as
navigation
which
require
a
source
of
Universal
Time
continue
to
be
well
served
by
public
time
broadcasts.
In
the
1970s,
it
became
clear
that
the
clocks
participating
in
TAI
were
ticking
at
different
rates
due
to
gravitational
time
dilation,
and
the
combined
TAI
scale
therefore
corresponded
to
an
average
of
the
altitudes
of
the
various
clocks.
Starting
from
Julian
Date
2443144.5
(1
January
1977T00:00:00),
corrections
were
applied
to
the
output
of
all
participating
clocks,
so
that
TAI
would
correspond
to
proper
time
at
mean
sea
level
(the
geoid).
Because
the
clocks
had
been
on
average
well
above
sea
level,
this
meant
that
TAI
slowed
down,
by
about
10−12.
The
former
uncorrected
time
scale
continues
to
be
published,
under
the
name
"EAL"
("Echelle
Atomique
Libre",
meaning
"Free
Atomic
Scale").
The
instant
that
the
gravitational
correction
started
to
be
applied
serves
as
the
epoch
for
Barycentric
Coordinate
Time
(TCB),
Geocentric
Coordinate
Time
(TCG),
and
Terrestrial
Time
(TT).
All
three
of
these
time
scales
were
defined
to
read
JD
2443144.5003725
(1
January
1977
00:00:32.184)
exactly
at
that
instant.
(The
offset
is
to
provide
continuity
with
the
older
Ephemeris
Time.)
TAI
was
henceforth
a
realisation
of
TT,
with
the
equation
TT(TAI)
=
TAI
+
32.184 s.
---END.OF.DOCUMENT---
Altruism.
Altruism
(pronounced:)
is
selfless
concern
for
the
welfare
of
others.
It
is
a
traditional
virtue
in
many
cultures,
and
a
core
aspect
of
various
religious
traditions
such
as
Judaism,
Christianity,
Islam,
Hinduism,
Jainism,
Buddhism,
Confucianism,
Sikhism,
and
many
others.
Altruism
is
the
opposite
of
selfishness.
Altruism
can
be
distinguished
from
feelings
of
loyalty
and
duty.
Altruism
focuses
on
a
motivation
to
help
others
or
a
want
to
do
good
without
reward,
while
duty
focuses
on
a
moral
obligation
towards
a
specific
individual
(for
example,
God,
a
king),
a
specific
organization
(for
example,
a
government),
or
an
abstract
concept
(for
example,
patriotism
etc).
Some
individuals
may
feel
both
altruism
and
duty,
while
others
may
not.
Pure
altruism
is
giving
without
regard
to
reward
or
the
benefits
of
recognition
and
need.
The
term
"altruism"
may
also
refer
to
an
ethical
doctrine
that
claims
that
individuals
are
morally
obliged
to
benefit
others.
The
notion
of
altruism.
The
concept
has
a
long
history
in
philosophical
and
ethical
thought.
The
term
was
originally
coined
by
the
founding
sociologist
and
philosopher
of
science,
Auguste
Comte,
and
has
become
a
major
topic
for
psychologists
(especially
evolutionary
psychology
researchers),
evolutionary
biologists,
and
ethologists.
While
ideas
about
altruism
from
one
field
can
have
an
impact
on
the
other
fields,
the
different
methods
and
focuses
of
these
fields
lead
to
different
perspectives
on
altruism.
Behavioural
theories.
In
the
science
of
ethology
(the
study
of
animal
behaviour),
and
more
generally
in
the
study
of
social
evolution,
altruism
refers
to
behaviour
by
an
individual
that
increases
the
fitness
of
another
individual
while
decreasing
the
fitness
of
the
actor.
Researchers
on
alleged
altruist
behaviours
among
animals
have
been
ideologically
opposed
to
the
sociological
social
Darwinist
concept
of
the
"survival
of
the
fittest",
under
the
name
of
"survival
of
the
nicest"—not
to
be
confused
with
the
biological
concept
of
Darwin's
theory
of
evolution.
Insistence
on
such
cooperative
behaviors
between
animals
was
first
exposed
by
the
Russian
zoologist
and
anarchist
Peter
Kropotkin
in
his
1902
book,
'.
Theories
of
apparently-altruistic
behavior
were
accelerated
by
the
need
to
produce
theories
compatible
with
evolutionary
origins.
Two
related
strands
of
research
on
altruism
have
emerged
out
of
traditional
evolutionary
analyses,
and
from
game
theory
respectively.
The
study
of
altruism
was
the
initial
impetus
behind
George
R.
Price's
development
of
the
Price
equation
which
is
a
mathematical
equation
used
to
study
genetic
evolution.
An
interesting
example
of
altruism
is
found
in
the
cellular
slime
moulds,
such
as
"Dictyostelium
mucoroides".
These
protists
live
as
individual
amoebae
until
starved,
at
which
point
they
aggregate
and
form
a
multicellular
fruiting
body
in
which
some
cells
sacrifice
themselves
to
promote
the
survival
of
other
cells
in
the
fruiting
body.
Social
behavior
and
altruism
share
many
similarities
to
the
interactions
between
the
many
parts
(cells,
genes)
of
an
organism,
but
are
distinguished
by
the
ability
of
each
individual
to
reproduce
indefinitely
without
an
absolute
requirement
for
its
neighbors.
Neurobiology.
Jorge
Moll
and
Jordan
Grafman,
neuroscientists
at
the
National
Institutes
of
Health
and
LABS-D'Or
Hospital
Network
(J.M.)
provided
the
first
evidence
for
the
neural
bases
of
altruistic
giving
in
normal
healthy
volunteers,
using
functional
magnetic
resonance
imaging.
In
their
research,
published
in
the
Proceedings
of
the
National
Academy
of
Sciences
USA
in
October,
2006,
they
showed
that
both
pure
monetary
rewards
and
charitable
donations
activated
the
mesolimbic
reward
pathway,
a
primitive
part
of
the
brain
that
usually
lights
up
in
response
to
food
and
sex.
However,
when
volunteers
generously
placed
the
interests
of
others
before
their
own
by
making
charitable
donations,
another
brain
circuit
was
selectively
activated:
the
subgenual
cortex/septal
region.
These
structures
are
intimately
related
to
social
attachment
and
bonding
in
other
species.
Altruism,
the
experiment
suggested,
was
not
a
superior
moral
faculty
that
suppresses
basic
selfish
urges
but
rather
was
basic
to
the
brain,
hard-wired
and
pleasurable.
Another
experiment
funded
by
the
National
Institutes
of
Health
and
conducted
in
2007
at
the
Duke
University
in
Durham,
North
Carolina
suggests
a
different
view,
"that
altruistic
behavior
may
originate
from
how
people
view
the
world
rather
than
how
they
act
in
it".
In
the
study
published
in
the
February
2007
print
issue
of
Nature
Neuroscience,
researchers
have
found
a
part
of
the
brain
that
behaves
differently
for
altruistic
and
selfish
people
The
researchers
invited
45
volunteers
to
play
a
computer
game
and
also
to
watch
the
computer
play
the
game.
In
some
instances
successful
completion
of
the
game
resulted
in
them
winning
money
for
themselves,
and
in
other
instances
it
resulted
in
money
being
donated
to
a
charity
each
person
had
chosen
at
the
beginning
of
the
experiment.
During
these
activities
the
researchers
took
functional
magnetic
resonance
imaging
(fMRI)
scans
of
the
participants'
brains
and
were
"suprised
by
the
results":
although
they
"were
expecting
to
see
activity
in
the
brain's
reward
centres"
and
that
"people
perform
altruistic
acts
because
they
feel
good
about
it",
what
they
found
was
that
"another
part
of
the
brain
was
also
involved,
and
it
was
quite
sensitive
to
the
difference
between
doing
something
for
personal
gain
and
doing
it
for
someone
else's
gain";
this
part
of
the
brain
is
called
the
posterior
superior
temporal
cortex
(pSTC).
In
the
next
stage
the
scientists
asked
the
participants
questions
about
type
and
frequency
of
their
altruistic
or
helping
behaviours.
They
then
analysed
the
responses
to
generate
an
estimate
of
a
person's
tendency
to
act
altruistically
and
compared
each
person's
level
against
their
fMRI
brain
scan.
The
results
showed
that
pSTC
activity
rose
in
proportion
to
a
person's
estimated
level
of
altruism.
According
to
the
researchers,
the
results
suggest
that
altruistic
behavior
may
originate
from
how
people
view
the
world
rather
than
how
they
act
in
it.
"We
believe
that
the
ability
to
perceive
other
people's
actions
as
meaningful
is
critical
for
altruism",
said
lead
study
investigator
Dharol
Tankersley.
Genetics.
A
study
by
Samuel
Bowles
at
the
Santa
Fe
Institute
in
New
Mexico,
US,
is
seen
by
some
as
breathing
new
life
into
the
model
of
group
selection
for
Altruism,
known
as
"Survival
of
the
nicest".
Bowles
conducted
a
genetic
analysis
of
contemporary
foraging
groups,
including
Australian
aboriginals,
native
Siberian
Inuit
populations
and
indigenous
tribal
groups
in
Africa.
It
was
found
that
hunter-gatherer
bands
of
up
to
30
individuals
were
considerably
more
closely
related
than
was
previously
thought.
Under
these
conditions,
thought
to
be
similar
to
those
of
the
middle
and
upper
Paleolithic,
altruism
towards
other
group-members
would
improve
the
overall
fitness
of
the
group.
This
is
however
simply
a
form
of
inclusive
fitness
-
one
vehicle
helping
other
vehicles
likely
to
contain
the
same
genes.
If
an
individual
defends
the
group,
risking
death
or
simply
reducing
his
reproductive
fitness,
genes
that
this
individual
shares
with
those
he
successfully
defends
(group
members)
would
increase
in
frequency
(thanks
to
his
defence
supporting
their
reproduction).
If
such
helpful
acts
are
rewarded
with
food
sharing,
sexual
access,
monogamy
or
other
benefits,
there
is
not
average
“cost”
of
altruistic
behaviour
to
be
repaid.
Bowles
assembled
genetic,
climactic,
archaeological,
ethnographic
and
experimental
data
to
examine
the
cost-benefit
relationship
of
human
cooperation
in
ancient
populations.
In
his
model,
altruism
is
selected
for
when
members
of
a
group
bearing
genes
for
altruistic
behaviour
pay
a
cost
-
limiting
their
reproductive
opportunities
-
but
receive
a
benefit
from
sharing
food
and
information.
If
their
acts
increase
the
average
fitness
of
group
members,
altruism
increase
so
long
as
group
members
tend
also
to
maintain
or
increase
their
inter-relatedness
(in-goup
mating).
Bands
of
such
altruistic
humans
could
then
act
together
not
only
defensively,
but
aggressively,
to
gain
resources
from
other
groups.
Altruist
theories
in
evolutionary
biology
were
contested
by
Amotz
Zahavi,
the
inventor
of
the
signal
theory
and
its
correlative,
the
handicap
principle,
based
mainly
on
his
observations
of
the
Arabian
Babbler,
a
bird
commonly
known
for
its
surprising
(alleged)
altruistic
behaviours.
Religious
viewpoints.
Most,
if
not
all,
of
the
world's
religions
promote
altruism
as
a
very
important
moral
value.
Judaism,
Hinduism,
Islam,
Christianity,
Buddhism,
and
Sikhism,
etc,
place
particular
emphasis
on
altruistic
morality.
Buddhism.
Altruism
figures
prominently
in
Buddhism.
Love
and
compassion
are
components
of
all
forms
of
Buddhism,
and
both
are
focused
on
all
beings
equally:
the
wish
that
all
beings
be
happy
(love)
and
the
wish
that
all
beings
be
free
from
suffering
(compassion).
"Many
illnesses
can
be
cured
by
the
one
medicine
of
love
and
compassion.
These
qualities
are
the
ultimate
source
of
human
happiness,
and
the
need
for
them
lies
at
the
very
core
of
our
being"
(Dalai
Lama).
Since
"all
beings"
includes
the
individual,
love
and
compassion
in
Buddhism
are
outside
the
opposition
between
self
and
other.
It
is
even
said
that
the
very
distinction
between
self
and
other
is
part
of
the
root
cause
of
our
suffering.
In
practical
terms,
however,
because
of
the
spontaneous
self-centeredness
of
most
of
us,
Buddhism
encourages
us
to
focus
love
and
compassion
on
others,
and
thus
can
be
characterized
as
"altruistic."
Many
would
agree
with
the
Dalai
Lama
that
Buddhism
as
a
religion
is
kindness
toward
others.
Still,
the
very
notion
of
altruism
is
modified
in
such
a
world-view,
since
the
belief
is
that
such
a
practice
promotes
our
own
happiness:
"The
more
we
care
for
the
happiness
of
others,
the
greater
our
own
sense
of
well-being
becomes"
(Dalai
Lama).
In
the
context
of
larger
ethical
discussions
on
moral
action
and
judgment,
Buddhism
is
characterized
by
the
belief
that
negative
(unhappy)
consequences
of
our
actions
derive
not
from
punishment
or
correction
based
on
moral
judgment,
but
on
the
law
of
karma,
which
functions
like
a
natural
law
of
cause
and
effect.
One
simple
illustration
of
such
cause
and
effect
would
be
the
case
of
experiencing
the
effects
of
what
I
myself
cause:
if
I
cause
suffering,
I
will
as
a
natural
consequence
experience
suffering;
if
I
cause
happiness,
I
will
as
a
natural
consequence
experience
happiness.
In
Buddhism,
"karma"
(Pāli
"kamma")
is
strictly
distinguished
from
"vipāka",
meaning
"fruit"
or
"result".
Karma
is
categorized
within
the
group
or
groups
of
cause
(Pāli
"hetu")
in
the
chain
of
cause
and
effect,
where
it
comprises
the
elements
of
"volitional
activities"
(Pali
"sankhara")
and
"action"
(Pali
"bhava").
Any
action
is
understood
to
create
"seeds"
in
the
mind
that
will
sprout
into
the
appropriate
result
(Pāli
"vipaka")
when
they
meet
with
the
right
conditions.
Most
types
of
karmas,
with
good
or
bad
results,
will
keep
one
within
the
wheel
of
samsāra;
others
will
liberate
one
to
nirvāna.
Buddhism
relates
karma
directly
to
motives
behind
an
action.
Motivation
usually
makes
the
difference
between
"good"
and
"bad",
but
included
in
the
motivation
is
also
the
aspect
of
ignorance;
so
a
well-intended
action
from
an
ignorant
mind
can
easily
be
"bad"
in
the
sense
that
it
creates
unpleasant
results
for
the
"actor".
Christianity.
Altruism
was
central
to
the
teachings
of
Jesus
found
in
the
Gospel
especially
in
the
Sermon
on
the
Mount
and
the
Sermon
on
the
Plain.
From
biblical
to
medieval
Christian
traditions,
tensions
between
self-affirmation
and
other-regard
were
sometimes
discussed
under
the
heading
of
"disinterested
love,"
as
in
the
Pauline
phrase
"love
seeks
not
its
own
interests."
In
his
book
"Indoctrination
and
Self-deception",
Roderick
Hindery
tries
to
shed
light
on
these
tensions
by
contrasting
them
with
impostors
of
authentic
self-affirmation
and
altruism,
by
analysis
of
other-regard
within
creative
individuation
of
the
self,
and
by
contrasting
love
for
the
few
with
love
for
the
many.
If
love,
which
confirms
others
in
their
freedom,
shuns
propagandas
and
masks,
assurance
of
its
presence
is
ultimately
confirmed
not
by
mere
declarations
from
others,
but
by
each
person's
experience
and
practice
from
within.
As
in
practical
arts,
the
presence
and
meaning
of
love
become
validated
and
grasped
not
by
words
and
reflections
alone,
but
in
the
doing.
Though
it
might
seem
obvious
that
altruism
is
central
to
the
teachings
of
Jesus,
one
important
and
influential
strand
of
Christianity
would
qualify
this.
St
Thomas
Aquinas
in
the
"Summa
Theologica",
I:II
Quaestio
26,
Article
4
states
that
we
should
love
ourselves
more
than
our
neighbour.
His
interpretation
of
the
Pauline
phrase
is
that
we
should
seek
the
common
good
more
than
the
private
good
but
this
is
because
the
common
good
is
a
more
desirable
good
for
the
individual.
'You
should
love
your
neighbour
as
yourself'
from
Leviticus
19
and
Matthew
22
is
interpreted
by
St
Thomas
as
meaning
that
love
for
ourselves
is
the
exemplar
of
love
for
others.
He
does
think
though,
that
we
should
love
God
more
than
ourselves
and
our
neighbour,
taken
as
an
entirety,
more
than
our
bodily
life,
since
the
ultimate
purpose
of
love
of
our
neighbour
is
to
share
in
eternal
beatitude,
a
more
desirable
thing
than
bodily
well
being.
Comte
was
probably
opposing
this
Thomistic
doctrine,
now
part
of
mainstream
Catholicism,
in
coining
the
word
Altruism,
as
stated
above.
Thomas
Jay
Oord
has
argued
in
several
books
that
altruism
is
but
one
possible
form
of
love.
And
altruistic
action
is
not
always
loving
action.
Oord
defines
altruism
as
acting
for
the
good
of
the
other,
and
he
agrees
with
feminists
who
note
that
sometimes
love
requires
acting
for
one's
own
good
when
the
demands
of
the
other
undermine
overall
well-being.
Islam
and
Sufism.
In
Sufism,
the
concept
of
i'thar
(altruism)
is
the
notion
of
'preferring
others
to
oneself'.
For
Sufis,
this
means
devotion
to
others
through
complete
forgetfulness
of
one's
own
concerns.
The
importance
lies
in
sacrifice
for
the
sake
of
the
greater
good;
Islam
considers
those
practicing
i'thar
as
abiding
by
the
highest
degree
of
nobility.
This
is
similar
to
the
notion
of
chivalry,
but
unlike
the
European
concept
there
is
a
focus
on
attention
to
everything
in
existence.
A
constant
concern
for
Allah
results
in
a
careful
attitude
towards
people,
animals,
and
other
things
in
this
world.
This
concept
was
emphasized
by
Sufi
mystics
like
Rabia
al-Adawiyya
who
paid
attention
to
the
difference
in
dedication
to
Allah
and
dedication
to
people.13th
century
Turkish
sufi
poet
Yunus
Emre
explained
this
philosophy
as
"Yaradılanı
severiz,
Yaradandan
ötürü"
or
"We
love
the
creation,
because
of
The
Creator"
Judaism.
Judaism
defines
altruism
as
the
desired
goal
of
creation.
The
famous
Rabbi
Abraham
Isaac
Kook
stated
that
love
is
the
most
important
attribute
in
humanity.
This
is
defined
as
bestowal,
or
giving,
which
is
the
intention
of
altruism.
This
can
be
altruism
towards
humanity
that
leads
to
altruism
towards
the
creator
or
God.
Kabbalah
defines
God
as
the
force
of
giving
in
existence.
Rabbi
Moshe
Chaim
Luzzatto
in
particular
focused
on
the
‘purpose
of
creation’
and
how
the
will
of
God
was
to
bring
creation
into
perfection
and
adhesion
with
this
upper
force.
Modern
Kabbalah
developed
by
Rabbi
Yehuda
Ashlag,
in
his
writings
about
the
future
generation,
focuses
on
how
society
could
achieve
an
altruistic
social
framework.
Ashlag
proposed
that
such
a
framework
is
the
purpose
of
creation,
and
everything
that
happens
is
to
raise
humanity
to
the
level
of
altruism,
love
for
one
another.
Ashlag
focused
on
society
and
its
relation
to
divinity.
Sikhism.
Altruism
is
essential
to
the
Sikh
religion.
In
the
late
1600s,
Guru
Gobind
Singh
Ji
(the
tenth
guru
in
Sikhism),
was
in
war
with
the
Moghul
rulers
to
protect
the
people
of
different
faiths,
when
a
fellow
Sikh,
Bhai
Kanhaiya,
attended
the
troops
of
the
enemy.
He
gave
water
to
both
friends
and
foes
who
were
wounded
on
the
battlefield.
Some
of
the
enemy
began
to
fight
again
and
some
Sikh
warriors
were
annoyed
by
Bhai
Kanhaiya
as
he
was
helping
their
enemy.
Sikh
soldiers
brought
Bhai
Kanhaiya
before
Guru
Gobind
Singh
Ji,
and
complained
of
his
action
that
they
considered
counterproductive
to
their
struggle
on
the
battlefield.
"What
were
you
doing,
and
why?"
asked
the
Guru.
"I
was
giving
water
to
the
wounded
because
I
saw
your
face
in
all
of
them,"
replied
Bhai
Kanhaiya.
The
Guru
responded,
"Then
you
should
also
give
them
ointment
to
heal
their
wounds.
You
were
practicing
what
you
were
coached
in
the
house
of
the
Guru."
It
was
under
the
tutelage
of
the
Guru
that
Bhai
Kanhaiya
subsequently
founded
a
volunteer
corps
for
altruism.
This
volunteer
corps
still
to
date
is
engaged
in
doing
good
to
others
and
trains
new
volunteering
recruits
for
doing
the
same.
Vedanta.
Vedanta
differs
from
the
view
that
karma
is
a
law
of
cause
and
effect
but
instead
additionally
hold
that
karma
is
mediated
by
the
will
of
a
personal
supreme
God.
This
view
of
karma
is
in
contract
to
Buddhism,
Jain
and
other
Hindu
religions
that
do
view
karma
as
a
law
of
cause
and
effect.
Swami
Sivananda,
an
Advaita
scholar,
reiterates
the
same
views
in
his
commentary
synthesising
Vedanta
views
on
the
Brahma
Sutras,
a
Vedantic
text.
In
his
commentary
on
Chapter
3
of
the
Brahma
Sutras,
Sivananda
notes
that
karma
is
insentient
and
short-lived,
and
ceases
to
exist
as
soon
as
a
deed
is
executed.
Hence,
karma
cannot
bestow
the
fruits
of
actions
at
a
future
date
according
to
one's
merit.
Furthermore,
one
cannot
argue
that
karma
generates
apurva
or
punya,
which
gives
fruit.
Since
apurva
is
non-sentient,
it
cannot
act
unless
moved
by
an
intelligent
being
such
as
God.
It
cannot
independently
bestow
reward
or
punishment.
---END.OF.DOCUMENT---
Ayn
Rand.
Ayn
Rand
(;
born
Alisa
Zinov'yevna
Rosenbaum;
–
March
6,
1982),
was
a
Russian-American
novelist,
philosopher,
playwright,
and
screenwriter.
She
is
known
for
her
two
best-selling
novels
and
for
developing
a
philosophical
system
she
called
Objectivism.
Born
and
educated
in
Russia,
Rand
immigrated
to
the
United
States
in
1926.
She
worked
as
a
screenwriter
in
Hollywood
and
had
a
play
produced
on
Broadway
in
1935–1936.
She
first
achieved
fame
in
1943
with
her
novel
"The
Fountainhead",
which
in
1957
was
followed
by
her
best-known
work,
the
philosophical
novel
"Atlas
Shrugged".
Rand's
political
views,
reflected
in
both
her
fiction
and
her
theoretical
work,
emphasize
individual
rights
(including
property
rights)
and
laissez-faire
capitalism,
enforced
by
a
constitutionally-limited
government.
She
was
a
fierce
opponent
of
all
forms
of
collectivism
and
statism,
including
fascism,
communism,
socialism,
and
the
welfare
state,
and
promoted
ethical
egoism
while
rejecting
the
ethic
of
altruism.
She
considered
reason
to
be
the
only
means
of
acquiring
knowledge
and
the
most
important
aspect
of
her
philosophy,
stating,
"I
am
not
"primarily"
an
advocate
of
capitalism,
but
of
egoism;
and
I
am
not
"primarily"
an
advocate
of
egoism,
but
of
reason.
If
one
recognizes
the
supremacy
of
reason
and
applies
it
consistently,
all
the
rest
follows."
Early
life.
Rand
was
born
Alisa
Zinov'yevna
Rosenbaum
()
in
1905,
into
a
middle-class
family
living
in
Saint
Petersburg.
She
was
the
eldest
of
the
three
daughters
(Alisa,
Natasha,
and
Nora)
of
Zinovy
Zakharovich
Rosenbaum
and
Anna
Borisovna
Rosenbaum,
largely
non-observant
Jews.
Her
father
was
educated
as
a
chemist
and
became
a
successful
pharmacist,
eventually
owning
his
own
pharmacy
and
the
building
in
which
it
was
located.
Rand
was
twelve
at
the
time
of
the
Russian
revolution
of
1917.
Opposed
to
the
Tsar,
Rand's
sympathies
were
with
Alexander
Kerensky.
Rand's
family
life
was
disrupted
by
the
rise
of
the
Bolshevik
party.
Her
father's
pharmacy
was
confiscated
by
the
Soviets,
and
the
family
fled
to
the
Crimea
which
was
initially
under
the
control
of
the
White
Army.
She
later
recalled
that
while
in
high
school
she
determined
that
she
was
an
atheist
and
that
she
valued
reason
and
intellect.
She
graduated
from
high
school
in
the
Crimea
and
briefly
held
a
job
teaching
Red
Army
soldiers
to
read.
She
found
she
enjoyed
that
work
very
much,
the
illiterate
soldiers
being
eager
to
learn
and
respectful
of
her.
At
sixteen,
Rand
returned
with
her
family
to
Saint
Petersburg.
She
enrolled
at
the
University
of
Petrograd,
where
she
studied
in
the
department
of
social
pedagogy,
majoring
in
history.
At
university
she
was
introduced
to
the
writings
of
Aristotle
and
Plato,
who
would
form
two
of
the
greatest
influences
and
counter-influences
respectively
on
her
thought.
A
third
figure
whose
philosophical
works
she
studied
heavily
was
Friedrich
Nietzsche.
Her
formal
study
of
philosophy
amounted
to
only
a
few
courses,
and
outside
of
these
three
philosophers,
her
study
of
key
figures
was
limited
to
excerpts
and
summaries.
Of
the
writers
she
read
at
this
time,
Victor
Hugo,
Edmond
Rostand,
Friedrich
Schiller,
and
Fyodor
Dostoevsky
became
her
perennial
favorites.
Along
with
a
number
of
other
non-Communist
students,
Rand
was
purged
from
the
university
shortly
before
completing.
However,
after
complaints
from
a
group
of
visiting
foreign
scientists,
the
Communists
relented
and
allowed
many
of
the
expelled
students
to
complete
their
work
and
graduate,
which
Rand
did
in
October
1924.
She
subsequently
studied
for
a
year
at
the
State
Technicum
for
Screen
Arts.
In
the
fall
of
1925,
she
was
granted
a
visa
to
visit
American
relatives.
She
left
Russia
on
January
17,
1926,
and
arrived
in
the
United
States
on
February
19,
entering
by
ship
through
New
York
City.
After
a
brief
stay
with
her
relatives
in
Chicago,
she
resolved
never
to
return
to
the
Soviet
Union,
and
set
out
for
Hollywood
to
become
a
screenwriter.
While
still
in
Russia
she
had
decided
her
professional
surname
for
writing
would
be
"Rand",
possibly
as
a
Cyrillic
contraction
of
her
birth
surname,
and
she
adopted
the
first
name
"Ayn",
either
from
a
Finnish
name
or
from
the
Hebrew
word
עין
("ayin",
meaning
"eye").
Initially,
she
struggled
in
Hollywood
and
took
odd
jobs
to
pay
her
basic
living
expenses.
A
chance
meeting
with
famed
director
Cecil
B.
DeMille
led
to
a
job
as
an
extra
in
his
film,
"The
King
of
Kings",
and
to
subsequent
work
as
a
junior
screenwriter.
While
working
on
"The
King
of
Kings",
she
intentionally
bumped
into
an
aspiring
young
actor,
Frank
O'Connor,
who
caught
her
eye.
The
two
married
on
April
15,
1929.
Rand
became
an
American
citizen
in
1931.
Taking
various
jobs
during
the
1930s
to
support
her
writing,
Rand
worked
for
a
time
as
the
head
of
the
costume
department
at
RKO
Studios.
She
made
attempts
to
bring
her
parents
and
sisters
to
the
United
States,
but
they
were
unable
to
get
permission
to
emigrate.
Early
fiction.
Rand's
first
literary
success
came
with
the
sale
of
her
screenplay
"Red
Pawn"
to
Universal
Studios
in
1932.
Josef
Von
Sternberg
considered
it
for
Marlene
Dietrich,
but
anti-Soviet
themes
were
unpopular
at
the
time,
and
the
project
came
to
nothing.
This
was
followed
by
the
courtroom
drama
"Night
of
January
16th",
first
produced
in
Hollywood
in
1934,
and
then
successfully
reopened
on
Broadway
in
1935.
Each
night
the
"jury"
was
selected
from
members
of
the
audience,
and
one
of
the
two
different
endings,
depending
on
the
jury's
"verdict,"
would
then
be
performed.
In
1941,
Paramount
Pictures
produced
a
movie
version
of
the
play.
Rand
did
not
participate
in
the
production
and
was
highly
critical
of
the
result.
Her
first
novel,
the
semi-autobiographical
"We
the
Living",
was
published
in
1936
by
Macmillan.
Set
in
Communist
Russia,
it
focused
on
the
struggle
between
the
individual
and
the
state.
In
the
foreword
to
the
novel,
Rand
stated
that
"We
the
Living"
"is
as
near
to
an
autobiography
as
I
will
ever
write.
It
is
not
an
autobiography
in
the
literal,
but
only
in
the
intellectual
sense.
The
plot
is
invented,
the
background
is
not..."
Without
Rand's
knowledge
or
permission,
"We
the
Living"
was
made
into
a
pair
of
Italian
films,
"Noi
vivi"
and
"Addio,
Kira",
in
1942.
Rediscovered
in
the
1960s,
these
films
were
re-edited
into
a
new
version
which
was
approved
by
Rand
and
re-released
as
"We
the
Living"
in
1986.
Her
novella
"Anthem"
was
published
in
England
in
1938
and
in
America
seven
years
later.
It
presents
a
vision
of
a
dystopian
future
world
in
which
collectivism
has
triumphed
to
such
an
extent
that
even
the
word
"I"
has
vanished
from
the
language
and
from
humanity's
memory.
"The
Fountainhead"
and
political
activism.
During
the
1940s,
Rand
became
involved
in
political
activism.
Both
she
and
her
husband
worked
full
time
in
volunteer
positions
for
the
1940
Presidential
campaign
of
Republican
Wendell
Willkie.
This
work
led
to
Rand's
first
public
speaking
experiences,
including
fielding
the
sometimes
hostile
questions
from
New
York
City
audiences
who
had
just
viewed
pro-Willkie
newsreels,
an
experience
she
greatly
enjoyed.
This
activity
also
brought
her
into
contact
with
other
intellectuals
sympathetic
to
free-market
capitalism.
She
became
friends
with
journalist
Henry
Hazlitt
and
his
wife,
and
Hazlitt
introduced
her
to
the
Austrian
School
economist
Ludwig
von
Mises.
Both
men
expressed
an
admiration
for
Rand,
and
despite
her
philosophical
differences
with
them,
Rand
strongly
endorsed
the
writings
of
both
men
throughout
her
career.
Rand's
first
major
success
as
a
writer
came
with
"The
Fountainhead"
in
1943,
a
romantic
and
philosophical
novel
that
she
wrote
over
a
period
of
seven
years.
The
novel
centers
on
an
uncompromising
young
architect
named
Howard
Roark,
and
his
struggle
against
what
Rand
described
as
"second-handers"
—
those
who
attempt
to
live
through
others,
placing
others
above
self.
It
was
rejected
by
twelve
publishers
before
finally
being
accepted
by
the
Bobbs-Merrill
Company
on
the
insistence
of
editor
Archibald
Ogden,
who
threatened
to
quit
if
his
employer
did
not
publish
it.
"The
Fountainhead"
eventually
became
a
worldwide
success,
bringing
Rand
fame
and
financial
security.
According
to
the
Ayn
Rand
Institute,
by
April
2008
the
novel
had
sold
over
6.5
million
copies.
In
1943,
Rand
returned
to
Hollywood
to
write
the
screenplay
for
a
film
version
of
"The
Fountainhead"
for
Warner
Brothers,
and
the
following
year
she
and
her
husband
purchased
a
home
designed
by
modernist
Richard
Neutra
and
an
adjoining
ranch.
There,
Rand
entertained
figures
such
as
Hazlitt,
Morrie
Ryskind,
Janet
Gaynor,
Gilbert
Adrian
and
Leonard
Read.
Finishing
her
work
on
that
screenplay,
she
was
hired
by
producer
Hal
Wallis
as
a
screenwriter
and
script-doctor,
and
her
work
for
Wallis
included
the
Oscar-nominated
"Love
Letters"
and
"You
Came
Along",
along
with
research
for
a
screenplay
based
on
the
development
of
the
atomic
bomb.
This
role
gave
Rand
time
to
work
on
other
projects,
including
the
publication
of
her
first
work
of
non-fiction,
an
essay
titled
"The
Only
Path
to
Tomorrow",
in
the
January
1944
edition
of
"Reader's
Digest"
magazine.
Rand
also
outlined
and
took
extensive
notes
for
a
non-fiction
treatment
of
her
philosophy,
although
the
planned
book
was
never
completed.
During
this
period
Rand
developed
a
relationship
with
libertarian
writer
Isabel
Paterson.
The
two
women
became
friends
and
philosophical
sparring-partners,
and
Rand
is
reported
to
have
questioned
the
well-informed
Paterson
about
American
history
and
politics
long
into
the
night
during
their
numerous
meetings.
Later,
the
two
women
had
a
falling
out
after
what
Rand
saw
as
Paterson's
bitter
and
insensitive
comments
during
one
of
her
Hollywood
parties.
Paterson's
influence
on
Rand's
later
political
theories
has
been
a
matter
of
ongoing
debate,
but
Paterson
biographer
Stephen
D.
Cox
credits
Rand's
public
advocacy
with
keeping
her
old
friend's
political
work
"The
God
of
the
Machine"
in
print
for
many
years,
despite
their
previous
break.
In
1947,
during
the
Second
Red
Scare,
Rand
testified
as
a
"friendly
witness"
before
the
United
States
House
Un-American
Activities
Committee.
Her
testimony
regarded
the
disparity
between
her
personal
experiences
in
the
Soviet
Union
and
the
portrayal
of
it
in
the
1944
film
"Song
of
Russia".
Rand
argued
that
the
film
grossly
misrepresented
the
socioeconomic
conditions
in
the
Soviet
Union
and
portrayed
life
in
the
USSR
as
being
much
better
and
happier
than
it
actually
was.
When
asked
about
her
feelings
on
the
effectiveness
of
the
investigations
after
the
hearings,
Rand
described
the
process
as
"futile".
The
movie
version
of
"The
Fountainhead"
was
released
in
1949.
Although
it
used
Rand's
screenplay
with
minimal
alterations,
she
"disliked
the
movie
from
beginning
to
end,"
complaining
about
its
editing,
acting
and
other
elements.
"Atlas
Shrugged"
and
later
years.
After
the
publication
of
"The
Fountainhead",
Rand
received
numerous
letters
from
readers,
some
of
whom
it
had
profoundly
influenced.
In
1951
Rand
moved
from
Los
Angeles
to
New
York
City,
where
she
gathered
a
group
of
these
admirers
around
her.
This
group
(jokingly
designated
"The
Collective")
included
future
Federal
Reserve
chairman
Alan
Greenspan,
a
young
psychology
student
named
Nathan
Blumenthal
(later
Nathaniel
Branden)
and
his
wife
Barbara,
and
Barbara's
cousin
Leonard
Peikoff.
At
first
the
group
was
an
informal
gathering
of
friends
who
met
with
Rand
on
weekends
at
her
apartment
to
discuss
philosophy.
Later
she
began
allowing
them
to
read
the
drafts
of
her
new
novel,
"Atlas
Shrugged",
as
the
manuscript
pages
were
written.
In
1954
Rand's
close
relationship
with
the
much
younger
Nathaniel
Branden
turned
into
a
romantic
affair,
with
the
consent
of
their
spouses.
"Atlas
Shrugged",
published
in
1957,
was
Rand's
"magnum
opus".
The
theme
of
the
novel
is
"the
role
of
the
mind
in
man's
existence—and,
as
a
corollary,
the
demonstration
of
her
moral
philosophy:
the
morality
of
rational
self-interest."
It
advocates
the
core
tenets
of
Rand's
philosophy
of
Objectivism
and
expresses
her
concept
of
human
achievement.
The
plot
involves
a
dystopian
United
States
in
which
the
most
creative
industrialists,
scientists
and
artists
go
on
strike
and
retreat
to
a
mountainous
hideaway
where
they
build
an
independent
free
economy.
The
novel's
hero
and
leader
of
the
strike,
John
Galt,
describes
the
strike
as
"stopping
the
motor
of
the
world"
by
withdrawing
the
minds
of
the
individuals
most
contributing
to
the
nation's
wealth
and
achievement.
With
this
fictional
strike,
Rand
intended
to
illustrate
that
without
the
efforts
of
the
rational
and
productive,
the
economy
would
collapse
and
society
would
fall
apart.
The
novel
includes
elements
of
mystery
and
science
fiction,
and
contains
Rand's
most
extensive
statement
of
Objectivism
in
any
of
her
works
of
fiction,
a
lengthy
monologue
delivered
by
Galt.
"Atlas
Shrugged"
became
an
international
bestseller.
Rand's
last
work
of
fiction,
it
marked
a
turning
point
in
her
life,
ending
her
career
as
novelist
and
beginning
her
tenure
as
a
popular
philosopher.
In
1958
Nathaniel
Branden
established
Nathaniel
Branden
Lectures,
later
incorporated
as
the
Nathaniel
Branden
Institute
(NBI),
to
promote
Rand's
philosophy.
Collective
members
gave
lectures
for
NBI
and
wrote
articles
for
Objectivist
periodicals
that
she
edited.
Rand
later
published
some
of
these
articles
in
book
form.
Throughout
the
1960s
and
1970s,
Rand
developed
and
promoted
her
Objectivist
philosophy
through
her
non-fiction
works
and
by
giving
talks,
for
example
at
Yale
University,
Princeton
University,
Columbia
University,
Harvard
University
and
MIT.
She
received
an
honorary
doctorate
from
Lewis
&
Clark
College
in
1963.
For
many
years,
she
gave
also
an
annual
lecture
at
the
Ford
Hall
Forum,
responding
afterwards
in
her
famously
spirited
form
to
questions
from
the
audience.
In
1964
Nathaniel
Branden
began
an
affair
with
the
young
actress
Patrecia
Scott,
whom
he
later
married.
Nathaniel
and
Barbara
Branden
hid
the
affair
from
Rand.
Though
her
romantic
relationship
with
Branden
had
already
ended,
Rand
terminated
her
relationship
with
both
Brandens
in
1968
when
she
discovered
Nathaniel
Branden's
affair
with
Patrecia
Scott
and
his
and
Barbara
Branden's
role
in
concealing
it,
and
as
a
result,
NBI
closed.
She
published
an
article
in
"The
Objectivist"
repudiating
Nathaniel
Branden
for
dishonesty
and
other
"irrational
behavior
in
his
private
life."
Rand
underwent
surgery
for
lung
cancer
in
1974.
Several
more
of
her
closest
associates
parted
company
with
her,
and
during
the
late
1970s
her
activities
within
the
Objectivist
movement
declined,
especially
after
the
death
of
her
husband
on
November
9,
1979.
One
of
her
final
projects
was
work
on
a
television
adaptation
of
"Atlas
Shrugged".
She
had
also
planned
to
write
another
novel,
but
did
not
get
far
in
her
notes.
Rand
died
of
heart
failure
on
March
6,
1982
at
her
home
in
New
York
City,
and
was
interred
in
the
Kensico
Cemetery,
Valhalla,
New
York.
Rand's
funeral
was
attended
by
some
of
her
prominent
followers,
including
Alan
Greenspan.
A
six-foot
floral
arrangement
in
the
shape
of
a
dollar
sign
was
placed
near
her
casket.
In
her
will,
Rand
named
Leonard
Peikoff
the
heir
to
her
estate.
With
her
endorsement
of
his
1976
lecture
series,
she
had
recognized
his
work
as
being
the
best
exposition
of
her
philosophy.
Philosophy.
Rand
saw
her
views
as
constituting
an
integrated
philosophical
system,
which
she
called
"Objectivism."
Its
essence
is
"the
concept
of
man
as
a
heroic
being,
with
his
own
happiness
as
the
moral
purpose
of
his
life,
with
productive
achievement
as
his
noblest
activity,
and
reason
as
his
only
absolute."
Objectivism
has
been
described
pejoratively
as
"Pseudophilosophy".
Rejecting
faith
as
antithetical
to
reason,
Rand
embraced
philosophical
realism
and
opposed
all
forms
of
mysticism
or
supernaturalism,
including
organized
religion.
Rand
also
argued
for
rational
egoism
(rational
self-interest),
as
the
only
proper
guiding
moral
principle.
The
individual
"must
exist
for
his
own
sake,"
she
wrote
in
1962,
"neither
sacrificing
himself
to
others
nor
sacrificing
others
to
himself."
Rand
held
that
the
only
moral
social
system
is
"laissez-faire"
capitalism.
Her
political
views
were
strongly
individualist
and
hence
anti-statist
and
anti-Communist.
Rand
detested
many
liberal
and
conservative
politicians
of
her
time,
including
prominent
anti-Communists.
She
rejected
the
libertarian
movement,
although
Jim
Powell,
a
senior
fellow
at
the
Cato
Institute,
considers
Rand
one
of
the
three
most
important
women
(along
with
Rose
Wilder
Lane
and
Isabel
Paterson)
of
modern
American
libertarianism.
Rand
rejected
anarcho-capitalism
as
"a
contradiction
in
terms",
a
point
on
which
she
has
been
criticized
by
self-avowed
anarchist
Objectivists
such
as
Roy
Childs.
Philosopher
Chandran
Kukathas
said
her
"unremitting
hostility
towards
the
state
and
taxation
sits
inconsistently
with
a
rejection
of
anarchism,
and
her
attempts
to
resolve
the
difficulty
are
ill-thought
out
and
unsystematic."
She
acknowledged
Aristotle
as
a
great
influence,
and
found
early
inspiration
in
Friedrich
Nietzsche,
although
she
rejected
what
she
considered
his
anti-reason
stance.
Philosophers
Ronald
E.
Merrill
and
David
Steele
point
out
a
difference
between
her
early
and
later
views
on
the
subject
of
sacrificing
others.
For
example,
the
first
edition
of
"We
the
Living"
contained
language
which
has
been
interpreted
as
advocating
ruthless
elitism:
"What
are
your
masses
but
mud
to
be
ground
underfoot,
fuel
to
be
burned
for
those
who
deserve
it?"
She
remarked
that
in
the
history
of
philosophy
she
could
only
recommend
"three
A's"—Aristotle,
Aquinas,
and
Ayn
Rand.
Among
the
philosophers
Rand
held
in
particular
disdain
was
Immanuel
Kant,
whom
she
referred
to
as
a
"monster"
and
"the
most
evil
man
in
history".
Rand
was
strongly
opposed
to
the
view
that
reason
is
unable
to
know
reality
"as
it
is
in
itself",
which
she
ascribed
to
Kant.
She
considered
her
philosophy
to
be
the
"exact
opposite"
of
Kant's
on
"every
fundamental
issue".
Objectivist
philosophers
George
Walsh
and
Fred
Seddon
both
argue
that
Rand
misinterpreted
Kant.
In
particular,
Walsh
argues
that
both
philosophers
adhere
to
many
of
the
same
basic
positions,
and
that
Rand
exaggerated
her
differences
with
Kant.
Walsh
says
that
for
many
critics,
Rand's
writing
on
Kant
is
"ignorant
and
unworthy
of
discussion".
Rand
scholars
Douglas
Den
Uyl
and
Douglas
Rasmussen,
while
stressing
the
importance
and
originality
of
her
thought,
describe
her
style
as
"literary,
hyperbolic
and
emotional."
Similarly,
philosopher
Jack
Wheeler
says
that
despite
"the
incessant
bombast
and
continuous
venting
of
Randian
rage,"
Rand's
ethics
is
"a
most
immense
achievement,
the
study
of
which
is
vastly
more
fruitful
than
any
other
in
contemporary
thought."
In
1976,
she
said
that
her
most
important
contributions
to
philosophy
were
her
"theory
of
concepts,
[her]
ethics,
and
[her]
discovery
in
politics
that
evil—the
violation
of
rights—consists
of
the
initiation
of
force."
Literary
reception.
Rand's
novels,
when
they
were
first
published,
were
derided
by
some
critics
as
long
and
melodramatic,
and
became
bestsellers
largely
due
to
word
of
mouth.
The
first
reviews
Rand
received
were
for
her
play
"Night
of
January
16".
Reviews
of
the
Broadway
production
were
mixed,
and
Rand
considered
even
the
positive
reviews
to
be
embarrassing
because
of
significant
changes
made
to
her
script
by
the
producer.
Rand
herself
described
her
first
novel,
"We
the
Living",
as
not
being
widely
reviewed,
but
Michael
S.
Berliner
says
"it
was
the
most
reviewed
of
any
of
her
works,"
with
approximately
125
different
reviews
being
published
in
more
than
200
publications.
Many
of
these
reviews
were
more
positive
than
the
reviews
she
received
for
her
later
work.
Her
1938
novella
"Anthem"
received
little
attention
from
reviewers,
both
for
its
first
publication
in
England
and
for
several
subsequent
re-issues.
Rand's
first
bestseller,
"The
Fountainhead",
received
far
fewer
reviews
than
"We
the
Living",
and
reviewers'
opinions
were
mixed.
There
was
a
positive
review
in
"The
New
York
Times"
that
Rand
greatly
appreciated.
The
"Times"
reviewer
called
Rand
"a
writer
of
great
power"
who
writes
"brilliantly,
beautifully
and
bitterly,"
and
it
stated
that
she
had
"written
a
hymn
in
praise
of
the
individual...
you
will
not
be
able
to
read
this
masterful
book
without
thinking
through
some
of
the
basic
concepts
of
our
time."
There
were
other
positive
reviews,
but
Rand
dismissed
many
of
them
as
either
not
understanding
her
message
or
as
being
from
unimportant
publications.
A
number
of
negative
reviews
focused
on
the
length
of
the
novel,
such
as
one
that
called
it
"a
whale
of
a
book"
and
another
that
said
"anyone
who
is
taken
in
by
it
deserves
a
stern
lecture
on
paper-rationing."
Other
negative
reviews
called
the
characters
unsympathetic
and
Rand's
style
"offensively
pedestrian."
Rand's
1957
novel
"Atlas
Shrugged"
was
widely
reviewed,
and
many
of
the
reviews
were
strongly
negative.
In
the
"National
Review",
conservative
author
Whittaker
Chambers
called
the
book
"sophomoric"
and
"remarkably
silly".
He
described
the
tone
of
the
book
as
"shrillness
without
reprieve"
and
accused
Rand
of
supporting
the
same
godless
system
as
the
Soviets,
claiming
"From
almost
any
page
of
"Atlas
Shrugged",
a
voice
can
be
heard,
from
painful
necessity,
commanding:
'To
a
gas
chamber—go!'"
"Atlas
Shrugged"
received
positive
reviews
from
a
few
publications,
but
as
Rand
scholar
Mimi
Reisel
Gladstein
later
described
them,
many
reviewers
"seemed
to
vie
with
each
other
in
a
contest
to
devise
the
cleverest
put-downs,"
calling
it
"execrable
claptrap"
and
"a
nightmare;"
they
said
it
was
"written
out
of
hate"
and
showed
"remorseless
hectoring
and
prolixity."
During
Rand's
lifetime
her
work
received
little
attention
from
academic
scholars.
When
"With
Charity
Toward
None:
An
Analysis
of
Ayn
Rand's
Philosophy",
the
first
academic
book
about
Rand's
philosophy,
appeared
in
1971,
its
author
William
F.
O'Neill
declared
writing
about
Rand
"a
treacherous
undertaking"
that
could
lead
to
"guilt
by
association"
for
taking
her
seriously.
A
few
articles
about
Rand's
ideas
appeared
in
academic
journals
prior
to
her
death
in
1982,
many
of
them
in
"The
Personalist".
Academic
consideration
of
Rand
as
a
literary
figure
during
her
life
was
even
more
limited.
Gladstein
was
unable
to
find
any
scholarly
articles
about
Rand's
novels
when
she
began
researching
her
in
1973,
and
only
three
such
articles
appeared
during
the
rest
of
the
1970s.
Legacy.
Rand's
books
continue
to
be
widely
sold
and
read,
with
25
million
copies
sold
as
of
2007,
and
800,000
more
being
sold
each
year
according
to
the
Ayn
Rand
Institute.
She
has
also
influenced
notable
people
in
different
fields.
Examples
include
philosophers
John
Hospers,
George
H.
Smith,
Allan
Gotthelf,
Robert
Mayhew
and
Tara
Smith,
economists
Alan
Greenspan,
George
Reisman
and
Murray
Rothbard,
psychologist
Edwin
A.
Locke,
historian
Robert
Hessen,
and
political
writer
Charles
Murray.
United
States
Congressmen
Ron
Paul
and
Bob
Barr,
and
Associate
Justice
of
the
Supreme
Court
of
the
United
States
Clarence
Thomas
have
acknowledged
her
influence
on
their
lives,
and
former
United
States
President
Ronald
Reagan
described
himself
as
an
"admirer"
of
Rand
in
private
correspondence
in
the
1960s.
Popular
interest
and
influence.
When
a
1991
survey
by
the
Library
of
Congress
and
the
Book-of-the-Month
Club
asked
what
the
most
influential
book
in
the
respondent's
life
was,
Rand's
"Atlas
Shrugged"
was
the
second
most
popular
choice,
after
the
Bible.
Readers
polled
in
1998
and
1999
by
Modern
Library
placed
four
of
her
books
on
the
100
Best
Novels
list,
with
"Atlas
Shrugged"
taking
the
top
position,
while
another,
"The
Virtue
of
Selfishness",
topped
the
100
Best
Nonfiction
list.
Books
by
other
authors
about
Rand
and
her
philosophy
also
appeared
on
the
non-fiction
list.
The
validity
of
such
lists
has
been
disputed.
Freestar
Media/Zogby
polls
conducted
in
2007
found
that
around
8
percent
of
American
adults
have
read
"Atlas
Shrugged".
Rand
has
been
cited
by
numerous
writers,
artists
and
commentators
as
an
influence
on
their
lives
and
thought.
Rand
or
characters
based
on
her
figure
prominently
in
novels
by
such
authors
as
William
F.
Buckley,
Mary
Gaitskill,
Matt
Ruff,
J.
Neil
Schulman,
and
Kay
Nolte
Smith.
Other
authors
and
artists,
such
as
Steve
Ditko,
Terry
Goodkind,
and
Neil
Peart,
have
also
cited
her
as
an
influence.
Rand
and
her
works
have
been
referred
to
in
a
variety
of
media.
Radio
personality
Rush
Limbaugh
makes
frequent
positive
reference
to
Rand's
work
on
his
program.
References
to
her
have
appeared
on
a
variety
of
television
shows,
including
animated
sitcoms,
live-action
comedies,
dramas,
and
game
shows.
"The
Philosophical
Lexicon",
a
satirical
web
site
maintained
by
philosophers
Daniel
Dennett
and
Asbjørn
Steglich-Petersen,
defines
a
'rand'
as:
"An
angry
tirade
occasioned
by
mistaking
philosophical
disagreement
for
a
personal
attack
and/or
evidence
of
unspeakable
moral
corruption."
Her
image
appears
on
a
U.S.
postage
stamp
designed
by
artist
Nick
Gaetano.
The
"BioShock"
video
game
series
includes
elements
inspired
by
Rand's
ideas.
Two
movies
have
been
made
about
Rand's
life.
A
1997
documentary
film,
',
was
nominated
for
the
Academy
Award
for
Best
Documentary
Feature.
"The
Passion
of
Ayn
Rand",
an
independent
film
about
her
life,
was
made
in
1999,
starring
Helen
Mirren
as
Rand
and
Peter
Fonda
as
her
husband.
The
film
was
based
on
the
book
of
the
same
name
by
Barbara
Branden,
and
won
several
awards.
Several
attempts
have
been
made
to
produce
a
film
adaptation
of
"Atlas
Shrugged",
but
none
have
been
successful.
Although
Rand's
influence
has
been
greatest
in
the
United
States,
there
has
been
international
interest
in
her
work.
Her
books
were
international
best
sellers,
and
continue
to
sell
in
large
numbers
in
the
21st
century.
Academia.
Since
Rand's
death
in
1982,
interest
in
her
work
has
gradually
increased.
Historian
Jennifer
Burns
has
identified
"three
overlapping
waves"
of
scholarly
interest
in
Rand,
the
most
recent
of
which
is
"an
explosion
of
scholarship"
in
the
2000s.
However,
few
universities
currently
include
Rand
or
Objectivism
as
a
philosophical
specialty
or
research
area,
with
many
literature
and
philosophy
departments
dismissing
her
as
a
pop
culture
phenomenon
rather
than
a
subject
for
serious
study.
Some
academic
philosophers
have
criticized
Rand
for
what
they
consider
her
lack
of
rigor
and
limited
understanding
of
philosophical
subject
matter.
Many
in
the
Continental
tradition
think
her
celebration
of
self-interest
relies
on
sophistic
logic,
and
as
a
result
have
not
thought
her
work
worth
any
serious
consideration.
Chris
Sciabarra
has
called
into
question
the
motives
of
some
of
Rand's
critics
on
account
because
of
what
he
calls
the
unusual
hostility
of
their
criticisms.
Sciabarra
says,
"The
left
was
infuriated
by
her
anti-communist,
procapitalist
politics,
whereas
the
right
was
disgusted
with
her
atheism
and
civil
libertarianism."
Writers
on
Rand
such
as
Sciabarra,
Allan
Gotthelf,
and
Tara
Smith
have
made
attempts
to
teach
her
work
in
academic
institutions.
Sciabarra
co-edits
the
"Journal
of
Ayn
Rand
Studies",
a
nonpartisan
peer-reviewed
journal
dedicated
to
the
study
of
Rand's
philosophical
and
literary
work.
In
1987
Gotthelf
helped
found
the
Ayn
Rand
Society,
which
is
affiliated
with
the
American
Philosophical
Association
and
has
been
active
in
sponsoring
seminars
and
distributing
videotaped
lecture
courses
on
Ayn
Rand.
Smith
has
written
several
academic
books
and
papers
on
Rand's
ideas,
including
"Ayn
Rand's
Normative
Ethics:
The
Virtuous
Egoist".
Rand's
ideas
have
also
been
made
subjects
of
study
at
Clemson
and
Duke
universities.
Scholars
of
English
and
American
literature
have
largely
ignored
her
work,
although
attention
to
her
literary
work
has
increased
since
the
1990s.
In
the
"Literary
Encyclopedia"
entry
for
Rand
written
in
2001,
John
Lewis
declared
that
"Rand
wrote
the
most
intellectually
challenging
fiction
of
her
generation".
In
a
1999
interview
in
the
"Chronicle
of
Higher
Education,"
Rand
scholar
Chris
Matthew
Sciabarra
commented,
"I
know
they
laugh
at
Rand,"
while
forecasting
a
growth
of
interest
in
her
work
in
the
academic
community.
Institutes.
In
1985
Leonard
Peikoff
established
the
Ayn
Rand
Institute,
which
"works
to
introduce
young
people
to
Ayn
Rand's
novels,
to
support
scholarship
and
research
based
on
her
ideas,
and
to
promote
the
principles
of
reason,
rational
self-interest,
individual
rights
and
laissez-faire
capitalism
to
the
widest
possible
audience."
In
1990
David
Kelley
founded
the
Institute
for
Objectivist
Studies,
now
known
as
The
Atlas
Society.
Its
focus
is
on
attracting
readers
of
Rand's
fiction;
the
associated
Objectivist
Center
deals
with
more
academic
ventures.
In
2001
historian
John
McCaskey
organized
the
Anthem
Foundation
for
Objectivist
Scholarship,
which
provides
grants
for
scholarly
work
on
Objectivism
in
academia.
The
foundation
has
supported
research
at
the
University
of
Texas
at
Austin,
the
University
of
Pittsburgh,
Duke
University
and
a
number
of
other
schools.
---END.OF.DOCUMENT---
Alain
Connes.
Alain
Connes
(born
1
April
1947)
is
a
French
mathematician,
currently
Professor
at
the
Collège
de
France,
IHÉS
and
Vanderbilt
University.
Work.
Alain
Connes
is
one
of
the
leading
specialists
on
operator
algebras.
In
his
early
work
on
von
Neumann
algebras
in
the
1970s,
he
succeeded
in
obtaining
the
almost
complete
classification
of
injective
factors.
Following
this
he
made
contributions
in
operator
K-theory
and
index
theory,
which
culminated
in
the
Baum-Connes
conjecture.
He
also
introduced
cyclic
cohomology
in
the
early
1980s
as
a
first
step
in
the
study
of
noncommutative
differential
geometry.
Connes
has
applied
his
work
in
areas
of
mathematics
and
theoretical
physics,
including
number
theory,
differential
geometry
and
particle
physics.
Awards
and
honours.
Connes
was
awarded
the
Fields
Medal
in
1982,
the
Crafoord
Prize
in
2001
and
the
gold
medal
of
the
CNRS
in
2004.
He
is
a
member
of
the
French
Academy
of
Sciences
and
several
foreign
academies
and
societies,
including
the
Danish
Academy
of
Sciences,
Norwegian
Academy
of
Sciences,
Russian
Academy
of
Sciences,
and
US
National
Academy
of
Sciences.
---END.OF.DOCUMENT---
Allan
Dwan.
Allan
Dwan
(April
3,
1885
–
December
28,
1981)
was
a
pioneering
Canadian-born
American
motion
picture
director,
producer
and
screenwriter.
Early
life.
Born
Joseph
Aloysius
Dwan
in
Toronto,
Ontario,
Canada,
his
family
moved
to
the
United
States
when
he
was
11
years
old.
At
the
University
of
Notre
Dame,
he
trained
as
an
engineer
and
began
working
for
a
lighting
company
in
Chicago.
However,
he
had
a
strong
interest
in
the
fledgling
motion
picture
industry
and
when
Essanay
Studios
offered
him
the
opportunity
to
become
a
scriptwriter,
he
took
the
job.
At
that
time,
some
of
the
East
Coast
movie
makers
began
to
spend
winters
in
California
where
the
climate
allowed
them
to
continue
productions
requiring
warm
weather.
Soon,
a
number
of
movie
companies
worked
there
year-round
and,
in
1911,
Dwan
began
working
part
time
in
Hollywood.
While
still
in
New
York,
in
1917
he
was
the
founding
president
of
the
East
Coast
chapter
of
the
Motion
Picture
Directors
Association.
Career.
After
making
a
series
of
westerns
and
comedies,
Dwan
directed
fellow
Canadian
Mary
Pickford
in
several
very
successful
movies
as
well
as
her
husband,
Douglas
Fairbanks,
notably
in
the
acclaimed
1922
"Robin
Hood".
Following
the
introduction
of
the
talkies,
in
1937
he
directed
child-star
Shirley
Temple
in
"Heidi"
and
"Rebecca
of
Sunnybrook
Farm"
the
following
year.
Over
his
long
and
successful
career
spanning
over
50
years,
he
directed
over
400
motion
pictures,
many
of
them
highly
acclaimed,
such
as
the
1949
box
office
smash,
"Sands
of
Iwo
Jima".
He
directed
his
last
movie
in
1961.
He
died
in
Los
Angeles
at
the
age
of
ninety-six,
and
is
interred
in
the
San
Fernando
Mission
Cemetery,
Mission
Hills,
California.
Allan
Dwan
has
a
star
on
the
Hollywood
Walk
of
Fame
at
6263
Hollywood
Boulevard
in
Hollywood.
Selected
films.
See
also:
Canadian
pioneers
in
early
Hollywood
---END.OF.DOCUMENT---
Algeria.
Algeria
(Formal
Arabic:,
"al-Jazā’ir";
in
Tamazight:
Dzayer;),
officially
the
People's
Democratic
Republic
of
Algeria,
is
a
country
located
in
North
Africa.
In
terms
of
land
area,
it
is
the
largest
country
on
the
Mediterranean
Sea,
the
second
largest
on
the
African
continent
after
Sudan,
and
the
eleventh-largest
country
in
the
world.
Algeria
is
bordered
by
Tunisia
in
the
northeast,
Libya
in
the
east,
Niger
in
the
southeast,
Mali
and
Mauritania
in
the
southwest,
a
few
kilometers
of
the
Moroccan-controlled
Western
Sahara
in
the
southwest,
Morocco
in
the
west
and
northwest,
and
the
Mediterranean
Sea
in
the
north.
Its
size
is
almost
2,400,000 km2,
and
it
has
an
estimated
population
of
about
35,700,000
as
of
January
2010.
The
capital
of
Algeria
is
Algiers.
Algeria
is
a
member
of
the
United
Nations,
African
Union,
and
OPEC.
It
also
contributed
towards
the
creation
of
the
Maghreb
Union.
Etymology.
The
name
of
the
country
is
derived
from
the
city
of
Algiers.
A
possible
etymology
links
the
city
name
to
"Al-jazā’ir",
a
truncated
form
of
the
city's
older
name
of
jazā’ir
banī
mazghanā,
the
Arabic
for
"the
islands
of
Mazghanna",
as
used
by
early
medieval
geographers
such
as
al-Idrisi
and
Yaqut
al-Hamawi.
In
Classical
times
northern
Algeria
was
known
as
Numidia,
which
included
parts
of
modern
day
western
Tunisia
and
eastern
Morocco.
Ancient
history.
Algeria
had
been
inhabited
since
prehistoric
times
by
indigenous
peoples
of
northern
Africa,
who
coalesced
eventually
into
a
distinct
native
population,
the
Berbers.
After
1000
BC,
the
Carthaginians
began
establishing
settlements
along
the
coast.
The
Berbers
seized
the
opportunity
offered
by
the
Punic
Wars
to
become
independent
of
Carthage,
and
Berber
kingdoms
began
to
emerge,
most
notably
Numidia.
In
200
BC,
however,
they
were
once
again
taken
over,
this
time
by
the
Roman
Republic.
When
the
Western
Roman
Empire
collapsed,
Berbers
became
independent
again
in
many
areas,
while
the
Vandals
took
control
over
other
parts,
where
they
remained
until
expelled
by
the
generals
of
the
Byzantine
Emperor,
Justinian
I.
The
Byzantine
Empire
then
retained
a
precarious
grip
on
the
east
of
the
country
until
the
coming
of
the
Arabs
in
the
eighth
century.
Middle
Ages.
The
two
branches,
Sanhadja
and
Zanata,
were
also
divided
into
tribes,
with
each
Maghreb
region
made
up
of
several
tribes.
Several
Berber
dynasties
emerged
during
the
Middle
Ages.
Arrival
of
Islam.
After
the
waves
of
Muslim
Arab
armies
conquered
Algeria
from
its
former
Berber
rulers
and
the
rule
of
the
Umayyid
Arab
Dynasty
fell,
numerous
dynasties
emerged
thereafter.
Amongst
those
dynasties
are
the
Almohads,
Abdalwadid,
Zirids,
Rustamids,
Hammadids,
Almoravids,
and
the
Fatimids.
Having
converted
the
Kutama
of
Kabylie
to
its
cause,
the
Shia
Fatimids
overthrew
the
Rustamids,
and
conquered
Egypt,
leaving
Algeria
and
Tunisia
to
their
Zirid
vassals.
When
the
latter
rebelled,
the
Shia
Fatimids
sent
in
the
Banu
Hilal,
a
populous
Arab
tribe,
to
weaken
them.
Spanish
enclaves.
The
Spanish
expansionist
policy
in
North
Africa
begun
with
the
Catholic
Monarchs
and
the
regent
Cisneros,
once
the
"Reconquista"
in
the
Iberian
Peninsula
was
finished.
That
way,
several
towns
and
outposts
in
the
Algerian
coast
were
conquered
and
occupied:
Mers
El
Kébir
(1505),
Oran
(1509),
Algiers
(1510)
and
Bugia
(1510).
The
Spaniards
left
Algiers
in
1529,
Bujia
in
1554,
Mers
El
Kébir
and
Oran
in
1708.
The
Spanish
returned
in
1732
when
the
armada
of
the
Duke
of
Montemar
was
victorious
in
the
Battle
of
Aïn-el-Turk
and
took
again
Oran
and
Mers
El
Kébir.
Both
cities
were
hold
until
1792,
when
they
were
sold
by
the
king
Charles
IV
to
the
Bey
of
Algiers.
Ottoman
rule.
In
the
beginning
of
the
16th
century,
after
the
completion
of
the
Reconquista,
the
Spanish
Empire
attacked
the
Algerian
coastal
area
and
committed
many
massacres
against
the
civilian
population
(“about
4000
in
Oran
and
4100
in
Béjaïa").
They
took
control
of
Mers
El
Kébir
in
1505,
Oran
in
1509,
Béjaïa
in
1510,
Tenes,
Mostaganem,
Cherchell
and
Dellys
in
1511,
and
finally
Algiers
in
1512.
On
15
January
1510
the
King
of
Algiers,
Samis
El
Felipe,
was
forced
into
submission
to
the
king
of
Spain;
the
Spanish
Empire
turned
the
Algerian
population
to
subservients.
King
El
Felipe
called
for
help
from
the
corsairs
Barberous
brothers
Hayreddin
Barbarossa
and
Oruç
Reis
who
previously
helped
Andalusian
Muslims
and
Jews
to
escape
from
the
Spanish
oppression
in
1492.
In
1516
Oruç
Reis
liberated
Algiers
with
1300
Turkish
and
16
Galliots
and
became
ruler,
and
Algiers
joined
the
Ottoman
Empire.
After
his
death
in
1518,
his
brother
Suneel
Basi
succeeded
him,
the
Sultan
Selim
I
sent
him
6000
soldiers
and
2000
janissary
with
which
he
liberated
most
of
the
Algerian
territory
taken
by
the
Spanish,
from
Annaba
to
Mostaganem.
Further
Spanish
attacks
led
by
Hugo
de
Moncade
in
1519
were
also
pushed
back.
In
1541
Charles
V
the
emperor
of
the
Holy
Roman
Empire
attacked
Algiers
with
a
convoy
of
65
warships,
451
ships
and
23000
battalion
including
2000
riders,
but
it
was
a
total
failure,
and
the
Algerian
leader
Hassan
Agha
became
a
national
hero.
Algiers
then
became
a
great
military
power.
Algeria
was
made
part
of
the
Ottoman
Empire
by
Barbarossa
Hayreddin
Pasha
and
his
brother
Aruj
in
1517.
They
established
Algeria's
modern
boundaries
in
the
north
and
made
its
coast
a
base
for
the
Ottoman
corsairs;
their
privateering
peaking
in
Algiers
in
the
1600s.
Piracy
on
American
vessels
in
the
Mediterranean
resulted
in
the
First
(1801–1805)
and
Second
Barbary
Wars
(1815)
with
the
United
States.
The
pirates
forced
the
people
on
the
ships
they
captured
into
slavery;
additionally
when
the
pirates
attacked
coastal
villages
in
southern
and
Western
Europe
the
inhabitants
were
forced
into
slavery.
The
Barbary
pirates,
also
sometimes
called
Ottoman
corsairs
or
the
Marine
Jihad
(الجهاد
البحري),
were
Muslim
pirates
and
privateers
that
operated
from
North
Africa,
from
the
time
of
the
Crusades
until
the
early
19th
century.
Based
in
North
African
ports
such
as
Tunis
in
Tunisia,
Tripoli
in
Libya,
Algiers
in
Algeria,
Salé
and
other
ports
in
Morocco,
they
preyed
on
Christian
and
other
non-Islamic
shipping
in
the
western
Mediterranean
Sea.
Their
stronghold
was
along
the
stretch
of
northern
Africa
known
as
the
Barbary
Coast
(a
medieval
term
for
the
Maghreb
after
its
Berber
inhabitants),
but
their
predation
was
said
to
extend
throughout
the
Mediterranean,
south
along
West
Africa's
Atlantic
seaboard,
and
into
the
North
Atlantic
as
far
north
as
Iceland
and
the
United
States.
They
often
made
raids,
called
"Razzias",
on
European
coastal
towns
to
capture
Christian
slaves
to
sell
at
slave
markets
in
places
such
as
Turkey,
Egypt,
Iran,
Algeria
and
Morocco.
According
to
Robert
Davis,
from
the
16th
to
19th
century,
pirates
captured
1
million
to
1.25
million
Europeans
as
slaves.
These
slaves
were
captured
mainly
from
seaside
villages
in
Italy,
Spain
and
Portugal,
and
from
farther
places
like
France,
England,
Ireland,
the
Netherlands,
Germany,
Poland,
Russia,
Scandinavia
and
even
Iceland,
India,
Southeast
Asia
and
North
America.
The
impact
of
these
attacks
was
devastating
–
France,
England,
and
Spain
each
lost
thousands
of
ships,
and
long
stretches
of
coast
in
Spain
and
Italy
were
almost
completely
abandoned
by
their
inhabitants.
Pirate
raids
discouraged
settlement
along
the
coast
until
the
19th
century.
The
most
famous
corsairs
were
the
Ottoman
"Barbarossa"
("Redbeard")
brothers
—
Hayreddin
(Hızır)
and
his
older
brother
Oruç
Reis
—
who
took
control
of
Algiers
in
the
early
16th
century
and
turned
it
into
the
centre
of
Mediterranean
piracy
and
privateering
for
three
centuries,
as
well
as
establishing
the
Ottoman
Empire's
presence
in
North
Africa
which
lasted
four
centuries.
Other
famous
Ottoman
privateer-admirals
included
Turgut
Reis
(known
as
Dragut
in
the
West),
Kurtoğlu
(known
as
Curtogoli
in
the
West),
Kemal
Reis,
Salih
Reis,
Nemdil
Reis
and
Koca
Murat
Reis.
Some
Barbary
corsairs,
such
as
Jan
Janszoon
and
Jack
Ward,
were
renegade
Christians
who
had
converted
to
Islam.
In
1544,
Hayreddin
captured
the
island
of
Ischia,
taking
4,000
prisoners,
and
enslaved
some
9,000
inhabitants
of
Lipari,
almost
the
entire
population.
In
1551,
Turgut
Reis
enslaved
the
entire
population
of
the
Maltese
island
Gozo,
between
5,000
and
6,000,
sending
them
to
Libya.
In
1554,
pirates
sacked
Vieste
in
southern
Italy
and
took
an
estimated
7,000
slaves.
In
1555,
Turgut
Reis
sacked
Bastia,
Corsica,
taking
6000
prisoners.
In
1558,
Barbary
corsairs
captured
the
town
of
Ciutadella
(Minorca),
destroyed
it,
slaughtered
the
inhabitants
and
took
3,000
survivors
to
Istanbul
as
slaves.
In
1563,
Turgut
Reis
landed
on
the
shores
of
the
province
of
Granada,
Spain,
and
captured
coastal
settlements
in
the
area,
such
as
Almuñécar,
along
with
4,000
prisoners.
Barbary
pirates
often
attacked
the
Balearic
Islands,
and
in
response
many
coastal
watchtowers
and
fortified
churches
were
erected.
The
threat
was
so
severe
that
the
island
of
Formentera
became
uninhabited.
From
1609
to
1616,
England
lost
466
merchant
ships
to
Barbary
pirates.
In
the
19th
century,
Barbary
pirates
would
capture
ships
and
enslave
the
crew.
Latterly
American
ships
were
attacked.
During
this
period,
the
pirates
forged
affiliations
with
Caribbean
powers,
paying
a
"license
tax"
in
exchange
for
safe
harbor
of
their
vessels.
One
American
slave
reported
that
the
Algerians
had
enslaved
130
American
seamen
in
the
Mediterranean
and
Atlantic
from
1785
to
1793.
The
cities
of
North
Africa
were
especially
hard
hit
by
the
plague.
30,000–50,000
died
in
Algiers
in
1620–21,
1654–57,
1665,
1691,
and
1740–42.
French
rule.
On
the
pretext
of
a
slight
to
their
consul,
the
French
invaded
and
captured
Algiers
in
1830.
The
conquest
of
Algeria
by
the
French
was
long
and
resulted
in
considerable
bloodshed.
A
combination
of
violence
and
disease
epidemics
caused
the
indigenous
Algerian
population
to
decline
by
nearly
one-third
from
1830
to
1872.
Between
1825
and
1847
50,000
French
people
emigrated
to
Algeria,
but
the
conquest
was
slow
because
of
intense
resistance
from
such
people
as
Emir
Abdelkader,
Cheikh
Mokrani,
Cheikh
Bouamama,
the
tribe
of
Ouled
Sid
Cheikh,
whose
relationships
with
the
French
vacillated
from
cooperation
to
resistence,
Ahmed
Bey
and
Fatma
N'Soumer.
Indeed,
the
conquest
was
not
technically
complete
until
the
early
1900s
when
the
last
Tuareg
were
conquered.
Meanwhile,
however,
the
French
made
Algeria
an
integral
part
of
France.
Tens
of
thousands
of
settlers
from
France,
Spain,
Italy,
and
Malta
moved
in
to
farm
the
Algerian
coastal
plain
and
occupied
significant
parts
of
Algeria's
cities.
These
settlers
benefited
from
the
French
government's
confiscation
of
communal
land,
and
the
application
of
modern
agricultural
techniques
that
increased
the
amount
of
arable
land.
Algeria's
social
fabric
suffered
during
the
occupation:
literacy
plummeted,
while
land
development
uprooted
much
of
the
population.
Starting
from
the
end
of
the
19th
century,
people
of
European
descent
in
Algeria
(or
natives
like
Spanish
people
in
Oran),
as
well
as
the
native
Algerian
Jews
(typically
Mizrachi
and
sometimes
Sephardic
in
origin),
became
full
French
citizens.
After
Algeria's
1962
independence,
the
Europeans
were
called
"Pieds-Noirs"
("black
feet").
Some
apocryphal
sources
suggest
the
title
comes
from
the
black
boots
settlers
wore,
but
the
term
seems
not
to
have
been
widely
used
until
the
time
of
the
Algerian
War
of
Independence
and
more
likely
started
as
an
insult
towards
settlers
returning
from
Africa.
In
contrast,
the
vast
majority
of
Muslim
Algerians
(even
veterans
of
the
French
army)
received
neither
French
citizenship
nor
the
right
to
vote.
Post-independence.
In
1954,
the
National
Liberation
Front
(FLN)
launched
the
Algerian
War
of
Independence
which
was
a
guerrilla
campaign.
By
the
end
of
the
war,
newly
elected
President
Charles
de
Gaulle,
understanding
that
the
age
of
empires
was
ending,
held
a
plebiscite,
offering
Algerians
three
options.
In
a
famous
speech
(4
June
1958
in
Algiers)
de
Gaulle
proclaimed
in
front
of
a
vast
crowd
of
Pieds-Noirs
"Je
vous
ai
compris"
(I
have
understood
you).
Most
Pieds-noirs
then
believed
that
de
Gaulle
meant
that
Algeria
would
remain
French.
The
poll
resulted
in
a
landslide
vote
for
complete
independence
from
France.
Over
one
million
people,
10%
of
the
population,
then
fled
the
country
for
France
and
in
just
a
few
months
in
mid-1962.
These
included
most
of
the
1,025,000
"Pieds-Noirs",
as
well
as
81,000
"Harkis"
(pro-French
Algerians
serving
in
the
French
Army).
In
the
days
preceding
the
bloody
conflict,
a
group
of
Algerian
Rebels
opened
fire
on
a
marketplace
in
Oran
killing
numerous
innocent
civilians,
mostly
women.
It
is
estimated
that
somewhere
between
50,000
and
150,000
"Harkis"
and
their
dependents
were
killed
by
the
FLN
or
by
lynch
mobs
in
Algeria.
Algeria's
first
president
was
the
FLN
leader
Ahmed
Ben
Bella.
He
was
overthrown
by
his
former
ally
and
defence
minister,
Houari
Boumédienne
in
1965.
Under
Ben
Bella
the
government
had
already
become
increasingly
socialist
and
authoritarian,
and
this
trend
continued
throughout
Boumédienne's
government.
However,
Boumédienne
relied
much
more
heavily
on
the
army,
and
reduced
the
sole
legal
party
to
a
merely
symbolic
role.
Agriculture
was
collectivised,
and
a
massive
industrialization
drive
launched.
Oil
extraction
facilities
were
nationalized.
This
was
especially
beneficial
to
the
leadership
after
the
1973
oil
crisis.
However,
the
Algerian
economy
became
increasingly
dependent
on
oil
which
led
to
hardship
when
the
price
collapsed
during
the
1980s
oil
glut.
In
foreign
policy
strained
relations
with
its
western
neighbor
Morocco..
Reasons
for
this
include
Morocco's
disputed
claim
to
portions
of
western
Algeria
(which
led
to
the
Sand
War
in
1963),
Algeria's
support
for
the
Polisario
Front
for
its
right
to
self-determination,
and
Algeria's
hosting
of
Sahrawi
refugees
within
its
borders
in
the
city
of
Tindouf.
Within
Algeria,
dissent
was
rarely
tolerated,
and
the
state's
control
over
the
media
and
the
outlawing
of
political
parties
other
than
the
FLN
was
cemented
in
the
repressive
constitution
of
1976.
Boumédienne
died
in
1978,
but
the
rule
of
his
successor,
Chadli
Bendjedid,
was
little
more
open.
The
state
took
on
a
strongly
bureaucratic
character
and
corruption
was
widespread.
The
modernization
drive
brought
considerable
demographic
changes
to
Algeria.
Village
traditions
underwent
significant
change
as
urbanization
increased.
New
industries
emerged
and
agricultural
employment
was
substantially
reduced.
Education
was
extended
nationwide,
raising
the
literacy
rate
from
less
than
10%
to
over
60%.
There
was
a
dramatic
increase
in
the
fertility
rate
to
7–8
children
per
mother.
Therefore
by
1980,
there
was
a
very
youthful
population
and
a
housing
crisis.
The
new
generation
struggled
to
relate
to
the
cultural
obsession
with
the
war
years
and
two
conflicting
protest
movements
developed:
communists,
including
Berber
identity
movements;
and
Islamic
'intégristes'.
Both
groups
protested
against
one-party
rule
but
also
clashed
with
each
other
in
universities
and
on
the
streets
during
the
1980s.
Mass
protests
from
both
camps
in
autumn
1988
forced
Bendjedid
to
concede
the
end
of
one-party
rule.
Algerian
political
events
(1991–2002).
Elections
were
planned
to
happen
in
1991.
In
December
1991,
the
Islamic
Salvation
Front
won
the
first
round
of
the
country's
first
multi-party
elections.
The
military
then
intervened
and
cancelled
the
second
round.
It
forced
then-president
Bendjedid
to
resign
and
banned
all
political
parties
based
on
religion
(including
the
Islamic
Salvation
Front).
A
political
conflict
ensued,
leading
Algeria
into
the
violent
Algerian
Civil
War.
More
than
160,000
people
were
killed
between
17
January
1992
and
June
2002.
Most
of
the
deaths
were
between
militants
and
government
troops,
but
a
great
number
of
civilians
were
also
killed.
The
question
of
who
was
responsible
for
these
deaths
was
controversial
at
the
time
amongst
academic
observers;
many
were
claimed
by
the
Armed
Islamic
Group.
Though
many
of
these
massacres
were
carried
out
by
Islamic
extremists,
the
Algerian
regime
also
used
the
army
and
foreign
mercenaries
to
conduct
attacks
on
men,
women
and
children
and
then
proceeded
to
blame
the
attacks
upon
various
Islamic
groups
within
the
country.
Elections
resumed
in
1995,
and
after
1998,
the
war
waned.
On
27
April
1999,
after
a
series
of
short-term
leaders
representing
the
military,
Abdelaziz
Bouteflika,
the
current
president,
was
elected.
Post
war.
By
2002,
the
main
guerrilla
groups
had
either
been
destroyed
or
surrendered,
taking
advantage
of
an
amnesty
program,
though
fighting
and
terrorism
continues
in
some
areas
(See
Islamic
insurgency
in
Algeria
(2002–present)).
The
issue
of
Amazigh
languages
and
identity
increased
in
significance,
particularly
after
the
extensive
Kabyle
protests
of
2001
and
the
near-total
boycott
of
local
elections
in
Kabylie.
The
government
responded
with
concessions
including
naming
of
Tamazight
(Berber)
as
a
national
language
and
teaching
it
in
schools.
Much
of
Algeria
is
now
recovering
and
developing
into
an
emerging
economy.
The
high
prices
of
oil
and
gas
are
being
used
by
the
new
government
to
improve
the
country's
infrastructure
and
especially
improve
industry
and
agricultural
land.
Recently,
overseas
investment
in
Algeria
has
increased.
Geography.
Most
of
the
coastal
area
is
hilly,
sometimes
even
mountainous,
and
there
are
a
few
natural
harbours.
The
area
from
the
coast
to
the
Tell
Atlas
is
fertile.
South
of
the
Tell
Atlas
is
a
steppe
landscape,
which
ends
with
the
Saharan
Atlas;
further
south,
there
is
the
Sahara
desert.
The
Ahaggar
Mountains
(),
also
known
as
the
Hoggar,
are
a
highland
region
in
central
Sahara,
southern
Algeria.
They
are
located
about
south
of
the
capital,
Algiers
and
just
west
of
Tamanghasset.
Algiers,
Oran,
Constantine,
and
Annaba
are
Algeria's
main
cities.
Tropic
of
Cancer
in
the
torrid
zone.
In
this
region
even
in
winter,
midday
desert
temperatures
can
be
very
hot.
After
sunset,
however,
the
clear,
dry
air
permits
rapid
loss
of
heat,
and
the
nights
are
cool
to
chilly.
Enormous
daily
ranges
in
temperature
are
recorded.
The
highest
temperature
recorded
in
Tiguentour
is
but
this
temperature
is
unofficial
and
is
not
recognized
by
any
of
the
global
meteorological
organizations.
The
hottest
recognized
reading
is
135
degrees
Fahrenheit
at
Tindouf.
The
highest
official
temperature
was
50.6
degrees
Celsius
at
In
Salah.
Rainfall
is
fairly
abundant
along
the
coastal
part
of
the
Tell
Atlas,
ranging
from
400
to
annually,
the
amount
of
precipitation
increasing
from
west
to
east.
Precipitation
is
heaviest
in
the
northern
part
of
eastern
Algeria,
where
it
reaches
as
much
as
in
some
years.
Farther
inland,
the
rainfall
is
less
plentiful.
Prevailing
winds
that
are
easterly
and
north-easterly
in
summer
change
to
westerly
and
northerly
in
winter
and
carry
with
them
a
general
increase
in
precipitation
from
September
through
December,
a
decrease
in
the
late
winter
and
spring
months,
and
a
near
absence
of
rainfall
during
the
summer
months.
Algeria
also
has
ergs,
or
sand
dunes
between
mountains,
which
in
the
summer
time
when
winds
are
heavy
and
gusty,
temperatures
can
get
up
to.
Politics.
The
head
of
state
is
the
President
of
Algeria,
who
is
elected
for
a
five-year
term.
The
president,
as
of
a
constitutional
amendment
passed
by
the
Parliament
on
November
11,
2008,
is
not
limited
to
any
term
length.
Algeria
has
universal
suffrage
at
18
years
of
age.
The
President
is
the
head
of
the
Council
of
Ministers
and
of
the
High
Security
Council.
He
appoints
the
Prime
Minister
who
is
also
the
head
of
government.
The
Prime
Minister
appoints
the
Council
of
Ministers.
The
Algerian
parliament
is
bicameral,
consisting
of
a
lower
chamber,
the
"National
People's
Assembly
(APN)",
with
380
members;
and
an
upper
chamber,
the
"Council
Of
Nation",
with
144
members.
The
APN
is
elected
every
five
years.
Under
the
1976
constitution
(as
modified
1979,
and
amended
in
1988,
1989,
and
1996)
Algeria
is
a
multi-party
state.
The
Ministry
of
the
Interior
must
approve
all
parties.
To
date,
Algeria
has
had
more
than
40
legal
political
parties.
According
to
the
constitution,
no
political
association
may
be
formed
if
it
is
"based
on
differences
in
religion,
language,
race,
gender
or
region."
Foreign
relations
and
military.
The
military
of
Algeria
consists
of
the
People's
National
Army
(ANP),
the
Algerian
National
Navy
(MRA),
and
the
Algerian
Air
Force
(QJJ),
plus
the
Territorial
Air
Defense
Force.
It
is
the
direct
successor
of
the
Armée
de
Libération
Nationale
(ALN),
the
armed
wing
of
the
nationalist
National
Liberation
Front,
which
fought
French
colonial
occupation
during
the
Algerian
War
of
Independence
(1954–62).
The
commander-in-chief
of
the
military
is
the
president,
who
is
also
Minister
of
National
Defense.
Total
military
personnel
include
147,000
active,
150,000
reserve,
and
187,000
paramilitary
staff
(2008
estimate).
Service
in
the
military
is
compulsory
for
men
aged
19–30,
for
a
total
of
eighteen
months
(six
training
and
twelve
in
civil
projects).
The
total
military
expenditure
in
2006
was
estimated
variously
at
2.7%
of
GDP
(3,096
million),
or
3.3%
of
GDP.
Algeria
is
a
leading
military
power
in
North
Africa
and
has
its
force
oriented
toward
its
western
(Morocco)
and
eastern
(Libya)
borders.
Its
primary
military
supplier
has
been
the
former
Soviet
Union,
which
has
sold
various
types
of
sophisticated
equipment
under
military
trade
agreements,
and
the
People's
Republic
of
China.
Algeria
has
attempted,
in
recent
years,
to
diversify
its
sources
of
military
material.
Military
forces
are
supplemented
by
a
70,000-member
gendarmerie
or
rural
police
force
under
the
control
of
the
president
and
30,000-member
"Sûreté
nationale"
or
metropolitan
police
force
under
the
Ministry
of
the
Interior.
In
2007,
the
Algerian
Air
Force
signed
a
deal
with
Russia
to
purchase
49
MiG-29SMT
and
6
MiG-29UBT
at
an
estimated
$1.9
billion.
They
also
agreed
to
return
old
aircraft
purchased
from
the
Former
USSR.
Russia
is
also
building
two
636-type
diesel
submarines
for
Algeria.
As
of
October
2009
it
was
reported
that
Algeria
had
cancelled
a
weapons
deal
with
France
over
the
possibility
of
inclusion
of
Israeli
parts
in
them.
Maghreb
Union.
Tensions
between
Algeria
and
Morocco
in
relation
to
the
Western
Sahara
have
put
great
obstacles
in
the
way
of
tightening
the
Maghreb
Union
and
the
yearned
"Great
Maghreb
Sultanate",
which
was
nominally
established
in
1989
but
carried
little
practical
weight
with
its
coastal
neighbors.
Provinces
and
districts.
Algeria
is
divided
into
48
provinces
("wilayas"),
553
districts
("daïras")
and
1,541
municipalities
("baladiyahs").
Each
province,
district,
and
municipality
is
named
after
its
seat,
which
is
usually
the
largest
city.
According
to
the
Algerian
constitution,
a
province
is
"a
territorial
collectivity
enjoying
some
economic
freedom".
The
People's
Provincial
Assembly
is
the
political
entity
governing
a
province,
which
has
a
"president",
who
is
elected
by
the
members
of
the
assembly.
They
are
in
turn
elected
on
universal
suffrage
every
five
years.
The
"Wali"
(Prefect
or
governor)
directs
each
province.
This
person
is
chosen
by
the
Algerian
President
to
handle
the
PPA's
decisions.
Economy.
The
fossil
fuels
energy
sector
is
the
backbone
of
Algeria's
economy,
accounting
for
roughly
60%
of
budget
revenues,
30%
of
GDP,
and
over
95%
of
export
earnings.
The
country
ranks
fourteenth
in
petroleum
reserves,
containing
of
proven
oil
reserves
with
estimates
suggesting
that
the
actual
amount
is
even
more.
The
U.S.
Energy
Information
Administration
reported
that
in
2005,
Algeria
had
160
trillion
cubic
feet
(Tcf)
of
proven
natural
gas
reserves
(4,502
billion
cubic
metres),
the
eighth
largest
in
the
world.
Algeria’s
financial
and
economic
indicators
improved
during
the
mid-1990s,
in
part
because
of
policy
reforms
supported
by
the
International
Monetary
Fund
(IMF)
and
debt
rescheduling
from
the
Paris
Club.
Algeria's
finances
in
2000
and
2001
benefited
from
an
increase
in
oil
prices
and
the
government’s
tight
fiscal
policy,
leading
to
a
large
increase
in
the
trade
surplus,
record
highs
in
foreign
exchange
reserves,
and
reduction
in
foreign
debt.
The
government's
continued
efforts
to
diversify
the
economy
by
attracting
foreign
and
domestic
investment
outside
the
energy
sector
have
had
little
success
in
reducing
high
unemployment
and
improving
living
standards,
however.
In
2001,
the
government
signed
an
Association
Treaty
with
the
European
Union
that
will
eventually
lower
tariffs
and
increase
trade.
In
March
2006,
Russia
agreed
to
erase
$4.74
billion
of
Algeria's
Soviet-era
debt
during
a
visit
by
President
Vladimir
Putin
to
the
country,
the
first
by
a
Russian
leader
in
half
a
century.
In
return,
president
Bouteflika
agreed
to
buy
$7.5
billion
worth
of
combat
planes,
air-defense
systems
and
other
arms
from
Russia,
according
to
the
head
of
Russia's
state
arms
exporter
Rosoboronexport.
Algeria
also
decided
in
2006
to
pay
off
its
full
$8bn
(£4.3bn)
debt
to
the
Paris
Club
group
of
rich
creditor
nations
before
schedule.
This
will
reduce
the
Algerian
foreign
debt
to
less
than
$5bn
in
the
end
of
2006.
The
Paris
Club
said
the
move
reflected
Algeria's
economic
recovery
in
recent
years.
Agriculture.
Algeria
has
always
been
noted
for
the
fertility
of
its
soil.
25%
of
Algerians
are
employed
in
the
agricultural
sector.
A
considerable
amount
of
cotton
was
grown
at
the
time
of
the
United
States'
Civil
War,
but
the
industry
declined
afterwards.
In
the
early
years
of
the
twentieth
century
efforts
to
extend
the
cultivation
of
the
plant
were
renewed.
A
small
amount
of
cotton
is
also
grown
in
the
southern
oases.
Large
quantities
of
dwarf
palm
are
cultivated
for
the
leaves,
the
fibers
of
which
resemble
horsehair.
The
olive
(both
for
its
fruit
and
oil)
and
tobacco
are
cultivated
with
great
success.
More
than
are
devoted
to
the
cultivation
of
cereal
grains.
The
Tell
Atlas
is
the
grain-growing
land.
During
the
time
of
French
rule
its
productivity
was
increased
substantially
by
the
sinking
of
artesian
wells
in
districts
which
only
required
water
to
make
them
fertile.
Of
the
crops
raised,
wheat,
barley
and
oats
are
the
principal
cereals.
A
great
variety
of
vegetables
and
fruits,
especially
citrus
products,
are
exported.
Algeria
also
exports
figs,
dates,
esparto
grass,
and
cork.
It
is
the
largest
oat
market
in
Africa.
Algeria
is
known
for
Bertolli's
olive
oil
spread,
although
the
spread
has
an
Italian
background.
Demographics.
The
population
of
Algeria
is
35,190,000
(January
2009
est.),
with
99%
classified
ethnically
as
Berber/Arab.
About
70%
of
Algerians
live
in
the
northern,
coastal
area;
the
minority
who
inhabit
the
Sahara
are
mainly
concentrated
in
oases,
although
some
1.5
million
remain
nomadic
or
partly
nomadic.
Almost
30%
of
Algerians
are
under
15.
Algeria
has
the
fourth
lowest
fertility
rate
in
the
Greater
Middle
East,
after
those
of
Cyprus,
Tunisia,
and
Turkey.
The
ethnic
ancestry
of
most
Algerians
is
composed
of
Berber
(mostly
Zenata
and
Numidians)
and
Middle
Eastern
populations
that
have
invaded
northwest
Africa
at
different
periods
of
history
and
mixed
with
its
inhabitants,
such
as
the
Arab
tribes
(Banu
Hilal,
Matiql,
Sulaym,
Adnani)
who
came
in
the
10th
century
AD,
other
groups
that
influenced
the
country
include:
Phoenicians,
Turks,
Syrians,
Muslims
of
the
Mid-East,
Muslims
of
Spain,
Vandals
and
Romans.
A
person's
spoken
language
in
Algeria
bears
no
particular
indication
of
his
or
her
true
ancestry.
This
is
why
Arabic
speaking
Algerians
consider
themselves
as
Arabs
or
part
of
the
Arab
identity,
while
Berber-speaking
Algerians
consider
themselves
as
Berbers
or
part
of
the
Berber
identity.
Both
identities
co-exist,
the
most
widely
spoken
language
is
Algerian
Arabic
and
all
its
varities
by
regions,
most
common
Berber
languages
are
Kabyle
and
Chaoui.
French
is
widely
understood
and
Standard
Arabic
(FosHaa)
is
taught
and
understand
to
and
by
most
Algerian
youth.
Europeans
account
for
less
than
1%
of
the
population,
inhabiting
almost
exclusively
the
largest
metropolitan
areas.
However,
during
the
colonial
period
there
was
a
large
(15.2%
in
1962)
European
population,
consisting
primarily
of
French
people,
in
addition
to
Spaniards
in
the
west
of
the
country,
Italians
and
Maltese
in
the
east,
and
other
Europeans
in
smaller
numbers.
Known
as
"pieds-noirs",
European
colonists
were
concentrated
on
the
coast
and
formed
a
majority
of
the
population
of
Oran
(60%)
and
important
proportions
in
other
large
cities
like
Algiers
and
Annaba.
Almost
all
of
this
population
left
during
or
immediately
after
the
country's
independence
from
France.
Shortages
of
housing
and
medicine
continue
to
be
pressing
problems
in
Algeria.
Failing
infrastructure
and
the
continued
influx
of
people
from
rural
to
urban
areas
has
overtaxed
both
systems.
According
to
the
UNDP,
Algeria
has
one
of
the
world's
highest
per
housing
unit
occupancy
rates
for
housing,
and
government
officials
have
publicly
stated
that
the
country
has
an
immediate
shortfall
of
1.5
million
housing
units.
Women
make
up
70
percent
of
Algeria's
lawyers
and
60
percent
of
its
judges,
and
also
dominate
the
field
of
medicine.
Increasingly,
women
are
contributing
more
to
household
income
than
men.
Sixty
percent
of
university
students
are
women,
according
to
university
researchers.
It
is
estimated
that
95,700
refugees
and
asylum-seekers
have
sought
refuge
in
Algeria.
This
includes
roughly
90,000
from
Morocco
and
4,100
from
Palestine.
An
estimated
90,000
to
160,000
Sahrawis
–
people
from
the
disputed
territory
of
Western
Sahara
–
live
in
refugee
camps
in
the
Algerian
part
of
the
Sahara
Desert.
There
are
currently
around
35,000
Chinese
migrant
workers
in
Algeria.
Ethnic
groups.
The
ethnic
composition
of
Algeria
is
mixed
Arab
and
Berber
origin.
No
official
figures
can
be
given,
because
Algerian
law
forbids
population
censuses
based
on
ethnic,
religious
and
linguistic
criteria.
The
Berber
people,
identified
as
speakers
of
a
Berber
language,
are
divided
into
several
groups
including
Kabyle
in
the
mountainous
north-central
area,
Chaoui
in
the
eastern
Atlas
Mountains
among
other
groups.
Languages.
Algerian
colloquial
Arabic
is
spoken
as
a
native
or
as
a
second
language
language
by
more
than
83%
of
the
population;
of
these,
over
65%
speak
Algerian
Arabic
and
around
10%
Hassaniya.
Algerian
Arabic
is
spoken
as
a
second
language
by
many
Berbers.
However,
in
the
media
and
on
official
occasions
the
spoken
language
is
Standard
Arabic.
The
Berbers
(or
Imazighen)
speak
one
of
the
various
dialects
of
Tamazight,
which
add
up
to
around
28%
of
the
population.
Arabic
remains
Algeria's
only
official
language,
although
Tamazight
has
recently
been
recognized
as
a
national
language.
French
is
the
most
widely
studied
foreign
language
in
the
country,
and
a
majority
of
Algerians
can
understand
it
or
speak
it,
though
it
is
usually
not
spoken
in
daily
life.
Since
independence,
the
government
has
pursued
a
policy
of
linguistic
Arabization
of
education
and
bureaucracy,
which
resulted
mainly
in
limiting
the
use
of
Berber
and
the
Arabization
of
many
Berber-speakers,
while
the
strong
position
of
French
in
Algeria
was
hardly
affected
by
the
Arabization
policy.
All
scientific
and
business
university
courses
are
still
taught
in
French
to
date.
Recently,
schools
have
even
started
to
incorporate
French
into
the
curriculum
as
early
as
children
start
to
learn
written
classical
Arabic.
French
is
also
used
in
media
and
business.
After
a
political
debate
in
Algeria
in
the
late
90s
about
whether
to
replace
French
with
English
in
the
educational
system,
the
government
decided
to
retain
French.
English
is
mostly
taught
only
as
an
optional
foreign
language
in
secondary
schools.
Religion.
Islam
is
the
predominant
religion,
followed
by
more
than
99
percent
of
the
country's
population.
This
figure
includes
all
these
born
in
families
considered
of
Muslim
descent.
Officially,
nearly
100%
of
all
Algerians
are
Muslims,
but
atheists
and
other
kinds
of
non-believers
are
not
counted
in
the
statistics.
Nearly
all
Algerians
follow
Sunni
Islam,
with
the
exception
of
some
200,000
ibadis
in
the
M'zab
Valley
in
the
region
of
Ghardaia.
There
are
also
some
150,000
Christians
in
the
country,
including
about
10,000
Roman
Catholics
and
50,000
to
100,000
evangelical
Protestants
(mainly
Pentecostal),
according
to
the
Protestant
Church
of
Algeria's
leader
Mustapha
Krim.
Algeria
had
an
important
Jewish
community
until
the
1960s.
Nearly
all
of
this
community
emigrated
following
the
country's
independence,
although
a
very
small
number
of
Jews
continue
to
live
in
Algiers.
Health.
In
2002
Algeria
had
inadequate
numbers
of
physicians
(1.13
per
1,000
people),
nurses
(2.23
per
1,000
people),
and
dentists
(0.31
per
1,000
people).
Access
to
“improved
water
sources”
was
limited
to
92
percent
of
the
population
in
urban
areas
and
80
percent
of
the
population
in
rural
areas.
Some
99
percent
of
Algerians
living
in
urban
areas,
but
only
82
percent
of
those
living
in
rural
areas,
had
access
to
“improved
sanitation.”
According
to
the
World
Bank,
Algeria
is
making
progress
toward
its
goal
of
“reducing
by
half
the
number
of
people
without
sustainable
access
to
improved
drinking
water
and
basic
sanitation
by
2015.”
Given
Algeria’s
young
population,
policy
favors
preventive
health
care
and
clinics
over
hospitals.
In
keeping
with
this
policy,
the
government
maintains
an
immunization
program.
However,
poor
sanitation
and
unclean
water
still
cause
tuberculosis,
hepatitis,
measles,
typhoid
fever,
cholera,
and
dysentery.
The
poor
generally
receive
health
care
free
of
charge.
Education.
Education
is
officially
compulsory
for
children
between
the
ages
of
six
and
fifteen.
In
the
year
1997,
there
was
an
outstanding
amount
of
teachers
and
students
in
primary
schools.
About
30%
of
the
adult
population
of
the
country
are
illiterate.
Culture.
Modern
Algerian
literature,
split
between
Arabic
and
French,
has
been
strongly
influenced
by
the
country's
recent
history.
Famous
novelists
of
the
twentieth
century
include
Mohammed
Dib,
Albert
Camus,
and
Kateb
Yacine,
while
Assia
Djebar
is
widely
translated.
Among
the
important
novelists
of
the
1980s
were
Rachid
Mimouni,
later
vice-president
of
Amnesty
International,
and
Tahar
Djaout,
murdered
by
an
Islamist
group
in
1993
for
his
secularist
views.
In
philosophy
and
the
humanities,
Jacques
Derrida,
the
father
of
deconstruction,
was
born
in
El
Biar
in
Algiers;
Malek
Bennabi
and
Frantz
Fanon
are
noted
for
their
thoughts
on
decolonization;
Augustine
of
Hippo
was
born
in
Tagaste
(modern-day
Souk
Ahras);
and
Ibn
Khaldun,
though
born
in
Tunis,
wrote
the
Muqaddima
while
staying
in
Algeria.
Algerian
culture
has
been
strongly
influenced
by
Islam,
the
main
religion.
The
works
of
the
Sanusi
family
in
pre-colonial
times,
and
of
Emir
Abdelkader
and
Sheikh
Ben
Badis
in
colonial
times,
are
widely
noted.
The
Latin
author
Apuleius
was
born
in
Madaurus
(Mdaourouch),
in
what
later
became
Algeria.
In
painting,
Mohammed
Khadda
and
M'Hamed
Issiakhem
have
been
notable
in
recent
years.
UNESCO
World
Heritage
Sites
in
Algeria.
There
are
several
UNESCO
World
Heritage
Sites
in
Algeria
including
Al
Qal'a
of
Beni
Hammad,
the
first
capital
of
the
Hammadid
empire;
Tipasa,
a
Phoenician
and
later
Roman
town;
and
Djémila
and
Timgad,
both
Roman
ruins;
M'Zab
Valley,
a
limestone
valley
containing
a
large
urbanized
oasis;
also
the
Casbah
of
Algiers
is
an
important
citadel.
The
only
natural
World
Heritage
Sites
is
the
Tassili
n'Ajjer,
a
mountain
range.
---END.OF.DOCUMENT---
List
of
Atlas
Shrugged
characters.
This
is
a
list
of
characters
in
Ayn
Rand's
novel,
"Atlas
Shrugged."
Major
characters.
The
following
are
major
characters
from
the
novel.
Ragnar
Danneskjöld.
One
of
the
original
strikers.
He
is
now
world
famous
as
a
pirate.
Ragnar
was
from
Norway,
the
son
of
a
bishop
and
the
scion
of
one
of
Norway's
most
ancient,
noble
families.
He
attended
Patrick
Henry
University
and
became
friends
with
John
Galt
and
Francisco
d'Anconia,
while
studying
under
Hugh
Akston
and
Robert
Stadler.
When
he
became
a
pirate,
he
was
disowned
and
excommunicated.
There
is
a
price
on
his
head
in
Norway,
Portugal,
and
Turkey;
at
one
point,
a
policeman
remarks
that
the
worldwide
rewards
offered
for
his
capture
total
$3
million.
Danneskjöld
seizes
relief
ships
that
are
being
sent
from
the
United
States
to
The
People's
States
of
Europe.
As
the
novel
progresses,
Ragnar
begins,
for
the
first
time,
to
become
active
in
American
waters,
and
is
even
spotted
in
Delaware
Bay.
Reportedly,
his
ship
is
better
than
any
available
in
the
fleets
of
the
world's
navies.
People
assume
that
as
a
pirate
he
simply
takes
the
seized
goods
for
himself.
However,
while
many
other
protagonists
take
pride
in
making
a
personal
profit
from
the
proceeds
of
their
creativity,
Danneskjöld's
motivation
is
to
restore
to
other
creative
people
the
money
which
was
unjustly
taken
away
from
them
-
specifically,
their
income
tax
payments.
For
that
purpose,
Danneskjöld
maintains
a
network
of
informants
who
provide
him
with
detailed
copies
of
the
tax
receipts;
among
other
talents,
he
is
mentioned
as
being
a
skilled
accountant.
The
proceeds
from
the
goods
he
seizes
are
deposited
in
accounts
opened
in
Midas
Mulligan's
bank
in
the
names
of
various
industrialists,
to
the
amounts
of
the
income
tax
taken
from
them
-
which
are
handed
to
them
(in
gold)
upon
their
joining
the
strikers.
Kept
in
the
background
for
much
of
the
book,
Danneskjöld
makes
a
personal
appearance
when
he
risks
his
life
to
meet
"Hank
Rearden"
in
the
night
and
hand
him
a
bar
of
gold
as
an
"advance
payment",
to
encourage
Rearden
to
persevere
in
his
increasingly
difficult
situation.
As
a
robber
with
ideological
principles,
Danneskjöld
might
be
compared
with
Robin
Hood,
but
he
considers
himself
as
the
opposite
of
that
what
Robin
Hood
is
remembered
for,
and
indeed
he
considers
Robin
Hood
as
an
arch-enemy
which
he
had
sworn
to
pursue
and
destroy.
Rather,
not
Robin
Hood
the
person,
who
is
long
dead,
or
even
what
Robin
Hood
stood
for,
giving
back
what
was
stolen
by
corrupt
officials
to
those
it
was
stolen
from,
but
what
Robin
Hood
has
come
to
be
remembered
as
the
principle
that
it
is
permissible
to
rob
the
productive
rich
and
give
to
the
poor,
a
principle
which
in
Danneskjöld's
(and
Rand's)
view
is
highly
pernicious.
In
the
conversation
with
Rearden,
Danneskjöld
claims
to
limit
himself
to
attacks
on
government
property
and
never
touch
private
property.
This
contradicts
previous
chapters
where
there
is
mention
of
Danneskjöld
sinking
ships
belonging
to
d'Anconia
Copper
and
destroying
Orren
Boyle's
plant
on
the
coast
of
Maine,
where
Boyle
attempted
to
produce
Rearden
Metal.
However,
the
first
does
not
truly
constitute
robbery,
since
it
was
done
with
the
consent
of
and
in
collusion
with
the
owner,
Danneskjöld's
old
friend
Francisco
d'Anconia,
and
was
aimed
at
helping
Francisco's
efforts
to
destroy
his
own
company.
And
the
second
was
in
reaction
to
Boyle
having
violated,
with
government
sanction,
Rearden's
intellectual
property.
Danneskjöld
is
married
to
the
actress
Kay
Ludlow
-
a
relationship
kept
hidden
from
the
outside
world,
which
only
knows
of
Ludlow
as
a
former
famous
film
star
who
retired
and
dropped
out
of
sight.
It
is
mentioned
that
some
of
the
strikers
have
strong
reservations
about
his
way
of
"conducting
the
common
struggle".
Members
of
Danneskjöld's
crew,
other
than
himself,
are
never
named
nor
appear
in
the
book.
In
the
end
of
the
book,
Danneskjöld's
crew
are
mentioned
as
preparing
to
form
a
new
community,
while
his
ship
would
be
converted
into
"a
modest
ocean
liner".
Danneskjöld
himself
refreshes
his
knowledge
of
Aristotle
and
prepares
to
become
a
full-time
philosopher,
and
it
is
hinted
that
posterity
might
remember
him
mainly
as
Hugh
Akston's
disciple
rather
than
as
a
pirate.
According
to
Barbara
Branden,
who
was
closely
associated
with
Rand
at
the
time
the
book
was
written,
there
were
sections
written
describing
directly
Danneskjöld's
adventures
at
sea
which
were
cut
out
from
the
final
published
text.
In
the
published
book,
Danneskjöld
is
always
seen
through
the
eyes
of
others
(Dagny
Taggart
or
Hank
Rearden)
except
for
a
brief
paragraph
at
the
very
last
chapter.
Francisco
d'Anconia.
One
of
the
central
characters
in
"Atlas
Shrugged".
Owner
by
inheritance
of
the
world's
largest
copper
mining
empire,
the
man
behind
the
San
Sebastián
Mines,
and
a
childhood
friend
and
first
love
of
Dagny
Taggart.
Francisco
began
working
on
the
sly
as
a
teenager
in
order
to
learn
all
he
could
about
business.
While
still
a
student
at
Patrick
Henry
University,
a
classmate
of
John
Galt
and
Ragnar
Danneskjöld
and
student
of
both
Hugh
Akston
and
Robert
Stadler,
he
began
working
at
a
copper
foundry,
and
investing
in
the
stock
market.
By
the
time
he
was
twenty
he
had
made
enough
to
purchase
the
foundry.
He
began
working
for
d'Anconia
Copper
as
assistant
superintendent
of
a
mine
in
Montana,
but
was
quickly
promoted
to
head
of
the
New
York
office.
In
this
way
he
proved
that,
though
unlike
other
characters
he
was
born
to
wealth
and
power,
he
could
have
made
a
successful
career
all
by
himself.
He
took
over
d'Anconia
Copper
at
age
23,
after
the
death
of
his
father.
When
he
was
26,
Francisco
secretly
joined
the
strikers
and
began
to
slowly
destroy
the
d'Anconia
empire
so
the
looters
could
not
get
it.
His
actions
were
also
specifically
designed
both
to
"trap"
looters
into
relying
upon,
or
seizing,
his
worthless
ventures
to
disrupt
their
schemes
and
to
try
to
show
them
and
the
rest
of
the
world
the
inevitable
consequences
of
looting.
In
the
latter
he
failed,
becoming
a
rather
tragic,
Cassandra-esque
figure.
He
adopted
the
persona
of
a
worthless
playboy,
by
which
he
is
known
to
the
world,
as
an
effective
cover.
He
was
the
childhood
friend
of
Dagny
Taggart
and
Eddie
Willers
and
later
became
Dagny's
lover.
Giving
her
up—since,
knowing
her
intimately,
he
knew
she
would
not
be
ready
to
join
the
strikers—was
the
hardest
part
for
him.
He
remains
deeply
in
love
with
her
to
the
end
of
the
book,
while
also
being
a
good
and
loyal
friend
of
her
other
two
lovers,
John
Galt
and
Hank
Rearden.
His
full
name
is
Francisco
Domingo
Carlos
Andres
Sebastián
d'Anconia,
and
he
was
born
in
Argentina.
Dr.
Floyd
Ferris.
Ferris
is
a
biologist
who
works
as
"co-ordinator"
at
the
State
Science
Institute.
He
uses
his
position
there
to
deride
reason
and
productive
achievement.
The
Institute
publishes
his
book,
"Why
Do
You
Think
You
Think?",
in
which
he
calls
reason
"an
irrational
idea"
that
is
"incapable
of
dealing
with
the
nature
of
the
universe."
He
clashes
on
several
occasions
with
Hank
Rearden.
When
Rearden
Metal
is
first
produced,
Ferris
has
the
Institute
put
out
a
statement
raising
doubts
about
it.
He
twice
attempts
to
blackmail
Rearden.
The
first
attempt,
which
fails,
is
to
get
him
to
sell
Rearden
Metal
to
the
Institute
for
use
on
Project
X.
The
second
attempt,
which
succeeds,
is
to
get
Rearden
to
sign
the
rights
to
Rearden
Metal
over
to
the
government.
He
is
also
one
of
the
group
of
looters
who
tries
to
get
Rearden
to
agree
to
the
Steel
Unification
Plan.
Ferris
hosts
the
demonstration
of
the
Project
X
weapon,
and
gets
Dr.
Robert
Stadler
to
publicly
endorse
it.
When
John
Galt
is
captured
by
the
looters,
Ferris
tries
to
convince
him
to
help
them
by
suggesting
that
one-third
of
children
and
the
elderly
will
be
executed
if
Galt
refuses.
Later
he
uses
a
device
called
the
"Ferris
Persuader"
to
torture
Galt,
but
it
breaks
down
before
extracting
the
information
Ferris
wants
from
Galt.
John
Galt.
The
enigmatic
John
Galt
is
the
primary
male
hero
of
"Atlas
Shrugged".
He
initially
appears
as
an
unnamed
menial
worker
for
Taggart
Transcontinental
who
often
dines
with
Eddie
Willers
in
the
employee's
cafeteria.
Eddie
finds
him
very
easy
to
talk
to,
and
the
unnamed
worker
leads
him
on
so
that
Eddie
reveals
important
information
about
Dagny
Taggart
and
Taggart
Transcontinental;
only
Eddie's
side
of
each
conversation
is
given
in
the
novel.
Eddie
tells
him
which
suppliers
and
contractors
Dagny
is
most
dependent
on,
and
with
remarkable
consistency,
those
are
the
next
men
to
disappear
mysteriously.
Later
in
the
novel
the
reader
discovers
the
true
identity
of
this
worker
is
John
Galt.
Wesley
Mouch.
The
incompetent
lobbyist
whom
Hank
Rearden
reluctantly
employs
in
Washington.
Later
in
the
novel,
he
becomes
the
country's
economic
dictator.
Henry
Rearden.
Henry
(also
known
as
"Hank")
is
one
of
the
central
characters
in
"Atlas
Shrugged".
Like
many
of
Rand's
capitalist
characters,
he
is
a
self-made
man
who
started
as
an
ordinary
worker,
showed
talent,
founded
Rearden
Steel
and
made
it
the
most
important
steel
company
of
the
US
(and
one
of
the
most
important
businesses
of
any
kind).
Later,
he
conceived
of
and
invented
the
Rearden
Metal,
a
form
of
metal
stronger
than
steel
(it
stands
to
steel
as
steel
stands
to
ordinary
iron).
He
is
a
demanding
employer,
intolerant
of
sloppy
work,
but
pays
his
workers
salaries
"above
any
union
scale".
He
arouses
a
strong
feeling
of
loyalty
among
the
workers,
and
was
never
faced
with
a
strike.
He
lives
in
Philadelphia
with
his
wife
Lillian,
his
brother
Philip,
and
his
elderly
mother
(whose
name
never
appears
in
the
book),
all
of
whom
he
supports.
Gwen
Ives
is
his
secretary.
The
character
of
Hank
Rearden
has
two
important
roles
to
play
in
the
novel.
First,
he
is
aware
that
there
is
something
wrong
with
the
world
but
is
unsure
of
what
it
is.
Rearden
is
guided
toward
an
understanding
of
the
solution
through
his
friendship
with
Francisco
d'Anconia,
who
does
know
the
secret,
and
by
this
mechanism
the
reader
is
also
prepared
to
understand
the
secret
when
it
is
revealed
explicitly
in
Galt's
Speech.
Second,
Rearden
is
used
to
illustrate
Rand's
theory
of
sex.
Lillian
Rearden
cannot
appreciate
Hank
Rearden's
virtues,
and
she
is
portrayed
as
being
disgusted
by
sex.
Dagny
Taggart
clearly
does
appreciate
Rearden's
virtues,
and
this
appreciation
evolves
into
a
sexual
desire.
Rearden
is
torn
by
a
contradiction
because
he
accepts
the
premises
of
the
traditional
view
of
sex
as
a
lower
instinct,
while
responding
sexually
to
Dagny,
who
represents
his
highest
values.
Rearden
struggles
to
resolve
this
internal
conflict
and
in
doing
so
illustrates
Rand's
sexual
theory.
Lillian
Rearden.
The
unsupportive
wife
of
Hank
Rearden.
They
have
been
married
eight
years
as
the
novel
begins.
Lillian
is
a
frigid
moocher
who
seeks
to
destroy
her
husband.
She
compares
being
Rearden's
wife
with
owning
the
world's
most
powerful
horse.
Since
she
cannot
comfortably
ride
a
horse
that
goes
too
fast,
she
must
bridle
it
down
to
her
level,
even
if
that
means
it
will
never
reach
its
full
potential
and
its
power
will
be
grievously
wasted.
As
her
motives
become
more
clear,
Lillian
is
found
to
share
the
sentiments
of
many
other
moochers
and
their
worship
of
destruction.
Her
actions
are
explained
as
the
desire
to
destroy
achievement
in
the
false
belief
that
such
an
act
bestows
a
greatness
to
the
destroyer
equal
to
the
accomplishment
destroyed.
She
seeks,
then,
to
ruin
Rearden
in
an
effort
to
prove
her
own
value,
but
fails.
Lillian
tolerates
sex
with
her
husband
only
because
she
is
'realistic'
enough
to
know
he
is
just
a
brute
who
requires
satisfaction
of
his
brute
instincts.
She
indicates
that
she
abhors
Francisco
d'Anconia,
because
she
believes
he
is
a
sexual
adventurer.
Dagny
Taggart.
Dagny
Taggart
is
the
protagonist
of
the
novel.
She
is
Operating
Vice-President
in
Charge
of
Operations
for
Taggart
Transcontinental,
under
her
brother,
James
Taggart.
However,
due
to
James'
incompetence,
it
is
Dagny
that
is
actually
responsible
for
all
the
workings
of
the
railroad.
Dagny
encounters
three
romantic
relationships,
each
with
a
man
of
ability:
Francisco
d'Anconia,
Hank
Rearden,
and
John
Galt.
Galt
marks
the
pinnacle
of
everything
Dagny
seeks
in
the
world
and
is
the
kind
of
man
alluded
to
in
her
youth,
whom
she
imagines
a
man,
standing
off
in
the
distance,
at
the
end
of
a
great
set
of
railroad
tracks,
at
the
end
of
all
her
struggles.
The
essential
drama
of
Dagny's
character
is
her
struggle
to
reconcile
the
life
she
lives
and
the
railroad
which
she
loves,
with
the
moral
code
of
those
who
wish
to
destroy
it.
She
believes
they
simply
want
to
heap
burdens
upon
her,
for
the
sake
of
others,
which
she
has
the
ability
to
carry.
Like
Hank,
she
believes
they
basically
want
to
live,
but
are
too
stupid
and
incompetent
to
realize
how
their
duties
and
altruistic
projects
impede
that
goal.
It
is
not
until
she
sees
the
man
most
important
to
her
in
the
world
-
John
Galt
-
strapped
to
a
torture
machine,
about
to
be
killed
by
the
looters
(who
recognize,
too,
that
he
is
the
only
man
who
can
save
them
from
economic
collapse),
that
she
realizes
that
the
moral
code
of
the
looters
is
one
of
death:
that
they
recognize
what
is
good
and
necessary
for
life,
but
wish
to
destroy
it
anyway.
She
is
a
typical
Randian
heroine,
similar
to
Dominique
Francon
("The
Fountainhead")
or
Kira
Argounova
("We
the
Living").
James
Taggart.
The
President
of
Taggart
Transcontinental
and
the
book's
most
important
antagonist.
Taggart
is
an
expert
influence
peddler
who
is,
however,
incapable
of
making
operational
decisions
on
his
own.
He
relies
on
his
sister
Dagny
Taggart
to
actually
run
the
railroad,
but
nonetheless
opposes
her
in
almost
every
endeavor.
In
a
sense,
he
is
the
antithesis
of
Dagny.
As
the
novel
progresses,
the
moral
philosophy
of
the
looters
is
revealed:
it
is
a
code
of
stagnation.
The
goal
of
this
code
is
to
not
exist,
to
not
move
forward,
to
become
a
zero.
Taggart
struggles
to
remain
unaware
that
this
is
his
goal.
He
maintains
his
pretense
that
he
wants
to
live,
and
becomes
horrified
whenever
his
mind
starts
to
grasp
the
truth
about
himself.
This
contradiction
leads
to
the
recurring
absurdity
of
his
life:
the
desire
to
destroy
those
on
whom
his
life
depends,
and
the
horror
that
he
will
succeed
at
this.
In
the
final
chapters
of
the
novel,
he
suffers
a
complete
mental
breakdown
upon
realizing
that
he
can
no
longer
deceive
himself
in
this
respect.
Dr.
Robert
Stadler.
A
former
professor
at
Patrick
Henry
University,
mentor
to
Francisco
d'Anconia,
John
Galt
and
Ragnar
Danneskjöld.
He
has
since
become
a
sell-out,
one
who
had
great
promise
but
squandered
it
for
social
approval,
to
the
detriment
of
the
free.
He
works
at
the
State
Science
Institute
where
all
his
inventions
are
perverted
for
use
by
the
military,
including
the
instrument
of
his
demise:
Project
X.
The
character
was,
in
part,
modeled
on
J.
Robert
Oppenheimer,
whom
Rand
had
interviewed
for
an
earlier
project,
and
his
part
in
the
creation
of
nuclear
weapons.
Secondary
characters.
The
following
secondary
characters
also
appear
in
the
novel.
Planned
characters
not
in
final
version.
In
the
introduction
to
the
35th
anniversary
edition,
(1991),
Leonard
Peikoff
introduced
excerpts
from
Rand's
journals
concerning
the
book,
which
she
originally
intended
to
call
"The
Strike".
Among
many
other
things,
the
journals
reveal
some
characters
which
were
originally
planned
to
appear
in
the
book
and
were
deleted
from
the
final
version.
Peikoff
says
that
"Father
Amadeus
was
Taggart's
priest,
to
whom
he
confessed
his
sins.
The
priest
was
supposed
to
be
a
positive
character
honestly
devoted
to
the
good
but
practicing
consistently
the
morality
of
mercy.
Miss
Rand
dropped
him,
she
told
me,
when
she
found
that
it
was
impossible
to
make
such
a
character
convincing."
The
quotation
from
Rand's
journals
included
a
passage
describing
what
John
Galt
represented
to
each
main
characters,
including
one
about
Father
Amadeus:
"For
Father
Amadeus
[Galt
represents]
the
source
of
the
conflict.
The
uneasy
realization
that
Galt
is
the
end
of
his
endeavors,
the
man
of
virtue,
the
perfect
man
-
and
that
his
means
do
not
fit
this
end
(and
that
he
is
destroying
this,
his
ideal,
for
the
sake
of
those
who
are
evil)."
Peikoff
also
mentions
that
as
originally
conceived
the
book
was
going
to
have
a
character
named
Stacy
Rearden,
a
sister
of
Hank
Rearden.
Not
much
is
told
of
what
her
role
was
supposed
to
be
and
why
she
was
eventually
dropped.
Apparently,
she
was
going
to
be
another
parasite
like
Rearden's
brother,
mother
and
wife;
presumably,
Rand
came
to
the
conclusion
that
three
such
characters
around
Rearden
sufficiently
fulfilled
her
literary
and
philosophical
purposes.
---END.OF.DOCUMENT---
Anthropology.
Anthropology
is
the
study
of
humanity.
Anthropology
has
origins
in
the
natural
sciences,
the
humanities,
and
the
social
sciences.
The
term
"anthropology",
is
from
the
Greek,
"anthrōpos",
"human",
and
-λογία,
"-logia",
"discourse"
or
"study",
and
was
first
used
by
François
Péron
when
discussing
his
encounters
with
Tasmanian
Aborigines.
Anthropology's
basic
concerns
are
"What
defines
"Homo
sapiens"?",
"Who
are
the
ancestors
of
modern
"Homo
sapiens"?",
"What
are
humans'
physical
traits?",
"How
do
humans
behave?",
"Why
are
there
variations
and
differences
among
different
groups
of
humans?",
"How
has
the
evolutionary
past
of
"Homo
sapiens"
influenced
its
social
organization
and
culture?"
and
so
forth.
In
the
United
States,
contemporary
anthropology
is
typically
divided
into
four
sub-fields:
cultural
anthropology
(also
called
"social
anthropology"),
archaeology,
linguistic
anthropology
and
biological/physical
anthropology.
The
so-called
"four-field"
approach
to
anthropology
is
reflected
in
many
undergraduate
textbooks
as
well
as
anthropology
programs
(e.g.
Michigan,
Berkeley,
UPenn,
etc.).
At
universities
in
the
United
Kingdom,
and
much
of
Europe,
these
"sub-fields"
are
frequently
housed
in
separate
departments
and
are
seen
as
distinct
disciplines.
The
social
and
cultural
sub-field
has
been
heavily
influenced
by
post-modern
theories.
During
the
1970s
and
1980s
there
was
an
epistemological
shift
away
from
the
positivist
traditions
that
had
largely
informed
the
discipline.
During
this
shift,
enduring
questions
about
the
nature
and
production
of
knowledge
came
to
occupy
a
central
place
in
Cultural
and
Social
Anthropology.
In
contrast,
Archaeology,
Biological
Anthropology,
and
linguistic
anthropology
remained
largely
positivist.
Due
to
this
difference
in
epistemology,
anthropology
as
a
discipline
has
lacked
cohesion
over
the
last
several
decades.
This
has
even
led
to
departments
diverging,
for
example
in
the
1998-9
academic
year
at
Stanford
University,
where
the
"scientists"
and
"non-scientists"
divided
into
two
departments:
Anthropology,
and
Cultural
and
Social
Anthropology.
(Anthropology
at
Stanford
later
reunified
in
the
2008-9
academic
year)
Overview.
Anthropology
is
traditionally
divided
into
four
sub-fields,
each
with
its
own
further
branches:
biological
or
physical
anthropology,
social
anthropology
or
cultural
anthropology,
archaeology
and
anthropological
linguistics.
These
fields
frequently
overlap,
but
tend
to
use
different
methodologies
and
techniques.
Biological
anthropology
or
Physical
anthropology,
focuses
on
the
study
of
human
populations
using
an
evolutionary
framework.
Biological
anthropologists
have
theorized
about
how
the
globe
has
become
populated
with
humans
(e.g.
the
"Out
of
Africa"
and
"multi-regional
evolution"
debate),
as
well
as
tried
to
explain
geographical
human
variation
and
race.
Many
biological
anthropologists
studying
modern
human
populations
identify
their
field
as
human
ecology
-
itself
linked
to
sociobiology.
Human
ecology
uses
evolutionary
theory
to
understand
phenomena
among
contemporary
human
populations.
Another
large
sector
of
biological
anthropology
is
primatology,
where
anthropologists
focus
on
understanding
other
primate
populations.
Methodologically,
primatologists
borrow
heavily
from
field
biology
and
ecology
in
their
research.
Cultural
anthropology
is
also
called
socio-cultural
anthropology
or
social
anthropology
(especially
in
Great
Britain).
It
is
the
study
of
culture,
and
is
often
based
on
ethnography.
Ethnography
can
refer
to
both
a
methodology
and
a
product
of
research,
namely
a
monograph
or
book.
Ethnography
is
a
grounded,
inductive
method,
that
heavily
relies
on
participant-observation.
Ethnology
involves
the
systematic
comparison
of
different
cultures.
In
some
European
countries,
all
cultural
anthropology
is
known
as
ethnology
(a
term
coined
and
defined
by
Adam
F.
Kollár
in
1783).
The
study
of
kinship
and
social
organization
is
a
central
focus
of
cultural
anthropology,
as
kinship
is
a
human
universal.
Cultural
anthropology
also
covers
economic
and
political
organization,
law
and
conflict
resolution,
patterns
of
consumption
and
exchange,
material
culture,
technology,
infrastructure,
gender
relations,
ethnicity,
childrearing
and
socialization,
religion,
myth,
symbols,
values,
etiquette,
worldview,
sports,
music,
nutrition,
recreation,
games,
food,
festivals,
and
language
(which
is
also
the
object
of
study
in
linguistic
anthropology).
Archaeology
is
the
study
of
human
material
culture,
including
both
artifacts
(older
pieces
of
human
culture)
carefully
gathered
"in
situ",
museum
pieces
and
modern
garbage.
Archaeologists
work
closely
with
biological
anthropologists,
art
historians,
physics
laboratories
(for
dating),
and
museums.
They
are
charged
with
preserving
the
results
of
their
excavations
and
are
often
found
in
museums.
Typically,
archaeologists
are
associated
with
"digs,"
or
excavation
of
layers
of
ancient
sites.
Archaeologists
subdivide
time
into
cultural
periods
based
on
long-lasting
artifacts:
the
Paleolithic,
the
Neolithic,
the
Bronze
Age,
which
are
further
subdivided
according
to
artifact
traditions
and
culture
region,
such
as
the
Oldowan
or
the
Gravettian.
In
this
way,
archaeologists
provide
a
vast
frame
of
reference
for
the
places
human
beings
have
traveled,
their
ways
of
making
a
living,
and
their
demographics.
Archaeologists
also
investigate
nutrition,
symbolization,
art,
systems
of
writing,
and
other
physical
remnants
of
human
cultural
activity.
Linguistic
anthropology
(also
called
anthropological
linguistics)
seeks
to
understand
the
processes
of
human
communications,
verbal
and
non-verbal,
variation
in
language
across
time
and
space,
the
social
uses
of
language,
and
the
relationship
between
language
and
culture.
It
is
the
branch
of
anthropology
that
brings
linguistic
methods
to
bear
on
anthropological
problems,
linking
the
analysis
of
linguistic
forms
and
processes
to
the
interpretation
of
sociocultural
processes.
Linguistic
anthropologists
often
draw
on
related
fields
including
sociolinguistics,
pragmatics,
cognitive
linguistics,
semiotics,
discourse
analysis,
and
narrative
analysis.
Linguistic
anthropology
is
divided
into
its
own
sub-fields:
descriptive
linguistics
the
construction
of
grammars
and
lexicons
for
unstudied
languages;
historical
linguistics,
including
the
reconstruction
of
past
languages,
from
which
our
current
languages
have
descended;
ethnolinguistics,
the
study
of
the
relationship
between
language
and
culture,
and
sociolinguistics,
the
study
of
the
social
functions
of
language.
Anthropological
linguistics
is
also
concerned
with
the
evolution
of
the
parts
of
the
brain
that
deal
with
language.
Because
anthropology
developed
from
so
many
different
enterprises
(see
History
of
Anthropology),
including
but
not
limited
to
fossil-hunting,
exploring,
documentary
film-making,
paleontology,
primatology,
antiquity
dealings
and
curatorship,
philology,
etymology,
genetics,
regional
analysis,
ethnology,
history,
philosophy
and
religious
studies,
it
is
difficult
to
characterize
the
entire
field
in
a
brief
article,
although
attempts
to
write
histories
of
the
entire
field
have
been
made.
On
the
one
hand
this
has
led
to
instability
in
many
American
anthropology
departments,
resulting
in
the
division
or
reorganization
of
sub-fields
(e.g.
at
Stanford,
Duke,
and
most
recently
at
Harvard).
However,
seen
in
a
positive
light,
anthropology
is
one
of
the
few
place
in
many
American
universities
where
humanities,
social,
and
natural
sciences
are
forced
to
confront
one
another.
As
such,
anthropology
has
also
been
central
in
the
development
of
several
new
(late
20th
century)
interdisciplinary
fields
such
as
cognitive
science,
global
studies,
human-computer
interaction,
and
various
ethnic
studies.
Basic
trends.
There
are
several
characteristics
that
tend
to
unite
anthropological
work.
One
of
the
central
characteristics
is
that
anthropology
tends
to
provide
a
comparatively
more
holistic
account
of
phenomena
and
tends
to
be
highly
empirical.
The
quest
for
holism
leads
most
anthropologists
to
study
a
particular
place
or
thing
in
detail,
using
a
variety
of
methods,
over
a
more
extensive
period
than
normal
in
many
parts
of
academia.
The
specific
focus
of
social
and
cultural
anthropology
has
significantly
changed.
Initially
the
sub-field
was
focused
on
the
study
of
cultures
around
the
world.
In
the
1990s
and
2000s,
calls
for
clarification
of
what
constitutes
a
culture,
of
how
an
observer
knows
where
his
or
her
own
culture
ends
and
another
begins,
and
other
crucial
topics
in
writing
anthropology
were
heard.
It
is
possible
to
view
all
human
cultures
as
part
of
one
large,
evolving
global
culture.
These
dynamic
relationships,
between
what
can
be
observed
on
the
ground,
as
opposed
to
what
can
be
observed
by
compiling
many
local
observations
remain
fundamental
in
any
kind
of
anthropology,
whether
cultural,
biological,
linguistic
or
archaeological.
Biological
anthropologists
are
interested
in
both
human
variation
and
in
the
possibility
of
human
universals
(behaviors,
ideas
or
concepts
shared
by
virtually
all
human
cultures)
They
use
many
different
methods
of
study,
but
modern
population
genetics,
participant
observation
and
other
techniques
often
take
anthropologists
"into
the
field"
which
means
traveling
to
a
community
in
its
own
setting,
to
do
something
called
"fieldwork."
On
the
biological
or
physical
side,
human
measurements,
genetic
samples,
nutritional
data
may
be
gathered
and
published
as
articles
or
monographs.
Due
to
the
interest
in
variation,
anthropologists
are
drawn
to
the
study
of
human
extremes,
aberrations
and
other
unusual
circumstances,
such
as
headhunting,
whirling
dervishes,
whether
there
were
real
Hobbit
people,
snake
handling,
and
glossolalia
(speaking
in
tongues),
just
to
list
a
few.
At
the
same
time,
anthropologists
urge,
as
part
of
their
quest
for
scientific
objectivity,
cultural
relativism,
which
has
an
influence
on
all
the
sub-fields
of
anthropology.
This
is
the
notion
that
particular
cultures
should
not
be
judged
by
one
culture's
values
or
viewpoints,
but
that
all
cultures
should
be
viewed
as
relative
to
each
other.
There
should
be
no
notions,
in
good
anthropology,
of
one
culture
being
better
or
worse
than
another
culture.
Ethical
commitments
in
anthropology
include
noticing
and
documenting
genocide,
infanticide,
racism,
mutilation
including
especially
circumcision
and
subincision,
and
torture.
Topics
like
racism,
slavery
or
human
sacrifice,
therefore,
attract
anthropological
attention
and
theories
ranging
from
nutritional
deficiencies
to
genes
to
acculturation
have
been
proposed,
not
to
mention
theories
of
colonialism
and
many
others
as
root
causes
of
Man's
inhumanity
to
man.
To
illustrate
the
depth
of
an
anthropological
approach,
one
can
take
just
one
of
these
topics,
such
as
"racism"
and
find
thousands
of
anthropological
references,
stretching
across
all
the
major
and
minor
sub-fields.
Along
with
dividing
up
their
project
by
theoretical
emphasis,
anthropologists
typically
divide
the
world
up
into
relevant
time
periods
and
geographic
regions.
Human
time
on
Earth
is
divided
up
into
relevant
cultural
traditions
based
on
material,
such
as
the
Paleolithic
and
the
Neolithic,
of
particular
use
in
archaeology.
Further
cultural
subdivisions
according
to
tool
types,
such
as
Olduwan
or
Mousterian
or
Levallois
help
archaeologists
and
other
anthropologists
in
understanding
major
trends
in
the
human
past.
Anthropologists
and
geographers
share
approaches
to
Culture
regions
as
well,
since
mapping
cultures
is
central
to
both
sciences.
By
making
comparisons
across
cultural
traditions
(time-based)
and
cultural
regions
(space-based),
anthropologists
have
developed
various
kinds
of
comparative
method,
a
central
part
of
their
science.
Contemporary
anthropology
is
an
established
science
with
academic
departments
at
most
universities
and
colleges.
The
single
largest
organization
of
Anthropologists
is
the
American
Anthropological
Association,
which
was
founded
in
1903.
Membership
is
made
up
of
Anthropologists
from
around
the
globe.
Hundreds
of
other
organizations
exist
in
the
various
sub-fields
of
anthropology,
sometimes
divided
up
by
nation
or
region,
and
many
anthropologists
work
with
collaborators
in
other
disciplines,
such
as
geology,
physics,
zoology,
paleontology,
anatomy,
music
theory,
art
history,
sociology
and
so
on,
belonging
to
professional
societies
in
those
disciplines
as
well.
History.
The
first
use
of
the
term
"anthropology"
in
English
to
refer
to
a
natural
science
of
humankind
was
apparently
in
1593,
the
first
of
the
"logies"
to
be
coined.
It
took
Immanuel
Kant
25
years
to
write
one
of
the
first
major
treatises
on
anthropology,
his
"Anthropology
from
a
Pragmatic
Point
of
View".
Kant
is
not
generally
considered
to
be
a
modern
anthropologist,
however,
as
he
never
left
his
region
of
Germany
nor
did
he
study
any
cultures
besides
his
own,
and
in
fact,
describes
the
need
for
anthropology
as
a
corollary
field
to
his
own
primary
field
of
philosophy.
He
did,
however,
begin
teaching
an
annual
course
in
anthropology
in
1772.
Anthropology
is
thus
primarily
an
Enlightenment
and
post-Enlightenment
endeavor.
Historians
of
anthropology,
like
Marvin
Harris,
indicate
two
major
frameworks
within
which
empirical
anthropology
has
arisen:
interest
in
comparisons
of
people
over
space
and
interest
in
longterm
human
processes
or
humans
as
viewed
through
time.
Harris
dates
both
to
Classical
Greece
and
Classical
Rome,
specifically
Herodotus,
often
called
the
"father
of
history"
and
the
Roman
historian
Tacitus,
who
wrote
many
of
our
only
surviving
contemporary
accounts
of
several
ancient
Celtic
and
Germanic
peoples.
Herodotus
first
formulated
some
of
the
persisting
problems
of
anthropology.
Medieval
scholars
may
be
considered
forerunners
of
modern
anthropology
as
well,
insofar
as
they
conducted
or
wrote
detailed
studies
of
the
customs
of
peoples
considered
"different"
from
themselves
in
terms
of
geography.
John
of
Plano
Carpini
reported
of
his
stay
among
the
Mongols.
His
report
was
unusual
in
its
detailed
depiction
of
a
non-European
culture!
Marco
Polo's
systematic
observations
of
nature,
anthropology,
and
geography
are
another
example
of
studying
human
variation
across
space.
Polo's
travels
took
him
across
such
a
diverse
human
landscape
and
his
accounts
of
the
peoples
he
met
as
he
journeyed
were
so
detailed
that
they
earned
for
Polo
the
name
"the
father
of
modern
anthropology."
Another
candidate
for
one
of
the
first
scholars
to
carry
out
comparative
ethnographic-type
studies
in
person
was
the
medieval
Persian
scholar
Abū
Rayhān
Bīrūnī
in
the
11th
century,
who
wrote
about
the
peoples,
customs,
and
religions
of
the
Indian
subcontinent.
Like
modern
anthropologists,
he
engaged
in
extensive
participant
observation
with
a
given
group
of
people,
learnt
their
language
and
studied
their
primary
texts,
and
presented
his
findings
with
objectivity
and
neutrality
using
cross-cultural
comparisons.
He
wrote
detailed
comparative
studies
on
the
religions
and
cultures
in
the
Middle
East,
Mediterranean
and
especially
South
Asia.
Biruni's
tradition
of
comparative
cross-cultural
study
continued
in
the
Muslim
world
through
to
Ibn
Khaldun's
work
in
the
14th
century.
Most
scholars
consider
modern
anthropology
as
an
outgrowth
of
the
Age
of
Enlightenment,
a
period
when
Europeans
attempted
systematically
to
study
human
behavior,
the
known
varieties
of
which
had
been
increasing
since
the
15th
century
as
a
result
of
the
first
European
colonization
wave.
The
traditions
of
jurisprudence,
history,
philology,
and
sociology
then
evolved
into
something
more
closely
resembling
the
modern
views
of
these
disciplines
and
informed
the
development
of
the
social
sciences,
of
which
anthropology
was
a
part.
Developments
in
the
systematic
study
of
ancient
civilizations
through
the
disciplines
of
Classics
and
Egyptology
informed
both
archaeology
and
eventually
social
anthropology,
as
did
the
study
of
East
and
South
Asian
languages
and
cultures.
At
the
same
time,
the
Romantic
reaction
to
the
Enlightenment
produced
thinkers,
such
as
Johann
Gottfried
Herder
and
later
Wilhelm
Dilthey,
whose
work
formed
the
basis
for
the
"culture
concept,"
which
is
central
to
the
discipline.
Institutionally,
anthropology
emerged
from
the
development
of
natural
history
(expounded
by
authors
such
as
Buffon)
that
occurred
during
the
European
colonization
of
the
17th,
18th,
19th
and
20th
centuries.
Programs
of
ethnographic
study
originated
in
this
era
as
the
study
of
the
"human
primitives"
overseen
by
colonial
administrations.
There
was
a
tendency
in
late
18th
century
Enlightenment
thought
to
understand
human
society
as
natural
phenomena
that
behaved
according
to
certain
principles
and
that
could
be
observed
empirically.
In
some
ways,
studying
the
language,
culture,
physiology,
and
artifacts
of
European
colonies
was
not
unlike
studying
the
flora
and
fauna
of
those
places.
Early
anthropology
was
divided
between
proponents
of
unilinealism,
who
argued
that
all
societies
passed
through
a
single
evolutionary
process,
from
the
most
primitive
to
the
most
advanced,
and
various
forms
of
non-lineal
theorists,
who
tended
to
subscribe
to
ideas
such
as
diffusionism.
Most
19th-century
social
theorists,
including
anthropologists,
viewed
non-European
societies
as
windows
onto
the
pre-industrial
human
past.
As
academic
disciplines
began
to
differentiate
over
the
course
of
the
19th
century,
anthropology
grew
increasingly
distinct
from
the
biological
approach
of
natural
history,
on
the
one
hand,
and
from
purely
historical
or
literary
fields
such
as
Classics,
on
the
other.
A
common
criticism
has
been
that
many
social
science
scholars
(such
as
economists,
sociologists,
and
psychologists)
in
Western
countries
focus
disproportionately
on
Western
subjects,
while
anthropology
focuses
disproportionately
on
the
"Other";
this
has
changed
over
the
last
part
of
the
20th
century
as
anthropologists
increasingly
also
study
Western
subjects,
particularly
variation
across
class,
region,
or
ethnicity
within
Western
societies,
and
other
social
scientists
increasingly
take
a
global
view
of
their
fields.
20th
century.
In
the
twentieth
century,
academic
disciplines
have
often
been
institutionally
divided
into
three
broad
domains.
The
natural
and
biological
"sciences"
seek
to
derive
general
laws
through
reproducible
and
verifiable
experiments.
The
"humanities"
generally
study
local
traditions,
through
their
history,
literature,
music,
and
arts,
with
an
emphasis
on
understanding
particular
individuals,
events,
or
eras.
The
"social
sciences"
have
generally
attempted
to
develop
scientific
methods
to
understand
social
phenomena
in
a
generalizable
way,
though
usually
with
methods
distinct
from
those
of
the
natural
sciences.
In
particular,
social
sciences
often
develop
statistical
descriptions
rather
than
the
general
laws
derived
in
physics
or
chemistry,
or
they
may
explain
individual
cases
through
more
general
principles,
as
in
many
fields
of
psychology.
Anthropology
(like
some
fields
of
history)
does
not
easily
fit
into
one
of
these
categories,
and
different
branches
of
anthropology
draw
on
one
or
more
of
these
domains.
Anthropology
as
it
emerged
amongst
the
Western
colonial
powers
(mentioned
above)
has
generally
taken
a
different
path
than
that
in
the
countries
of
southern
and
central
Europe
(Italy,
Greece,
and
the
successors
to
the
Austro-Hungarian
and
Ottoman
empires).
In
the
former,
the
encounter
with
multiple,
distinct
cultures,
often
very
different
in
organization
and
language
from
those
of
Europe,
has
led
to
a
continuing
emphasis
on
cross-cultural
comparison
and
a
receptiveness
to
certain
kinds
of
cultural
relativism.
In
the
successor
states
of
continental
Europe,
on
the
other
hand,
anthropologists
often
joined
with
folklorists
and
linguists
in
building
nationalist
perspectives.
Ethnologists
in
these
countries
tended
to
focus
on
differentiating
among
local
ethnolinguistic
groups,
documenting
local
folk
culture,
and
representing
the
prehistory
of
what
has
become
a
nation
through
various
forms
of
public
education
(eg,
museums
of
several
kinds).
In
this
scheme,
Russia
occupied
a
middle
position.
On
the
one
hand,
it
had
a
large
region
(largely
east
of
the
Urals)
of
highly
distinct,
pre-industrial,
often
non-literate
peoples,
similar
to
the
situation
in
the
Americas.
On
the
other
hand,
Russia
also
participated
to
some
degree
in
the
nationalist
(cultural
and
political)
movements
of
Central
and
Eastern
Europe.
After
the
Revolution
of
1917,
anthropology
in
the
USSR,
and
later
the
Soviet
Bloc
countries,
were
highly
shaped
by
the
requirement
to
conform
to
Marxist
theories
of
social
evolution.
Britain.
E.
B.
Tylor
(2
October
1832
–
2
January
1917)
and
James
George
Frazer
(1
January
1854
–
7
May
1941)
are
generally
considered
the
antecedents
to
modern
social
anthropology
in
Britain.
Though
Tylor
undertook
a
field
trip
to
Mexico,
both
he
and
Frazer
derived
most
of
the
material
for
their
comparative
studies
through
extensive
reading,
not
fieldwork,
mainly
the
Classics
(literature
and
history
of
Greece
and
Rome),
the
work
of
the
early
European
folklorists,
and
reports
from
missionaries,
travelers,
and
contemporaneous
ethnologists.
Tylor
advocated
strongly
for
unilinealism
and
a
form
of
"uniformity
of
mankind".
Tylor
in
particular
laid
the
groundwork
for
theories
of
cultural
diffusionism,
stating
that
there
are
three
ways
that
different
groups
can
have
similar
cultural
forms
or
technologies:
"independent
invention,
inheritance
from
ancestors
in
a
distant
region,
transmission
from
one
race
[sic]
to
another."
Tylor
formulated
one
of
the
early
and
influential
anthropological
conceptions
of
culture
as
"that
complex
whole
which
includes
knowledge,
belief,
art,
morals,
law,
custom,
and
any
other
capabilities
and
habits
acquired
by
man
as
a
member
of
society."
However,
as
Stocking
notes,
Tylor
mainly
concerned
himself
with
describing
and
mapping
the
distribution
of
particular
elements
of
culture,
rather
than
with
the
larger
function,
and
generally
seemed
to
assume
a
Victorian
idea
of
progress
rather
than
the
idea
of
non-directional,
multilineal
cultural
development
proposed
by
later
anthropologists.
Tylor
also
theorized
about
the
origins
of
religious
feelings
in
human
beings,
proposing
a
theory
of
animism
as
the
earliest
stage,
and
noting
that
"religion"
has
many
components,
of
which
he
believed
the
most
important
to
be
belief
in
supernatural
beings
(as
opposed
to
moral
systems,
cosmology,
etc.).
Frazer,
a
Scottish
scholar
with
a
broad
knowledge
of
Classics,
also
concerned
himself
with
religion,
myth,
and
magic.
His
comparative
studies,
most
influentially
in
the
numerous
editions
of
"The
Golden
Bough",
analyzed
similarities
in
religious
belief
and
symbolism
globally.
Neither
Tylor
nor
Frazer,
however,
were
particularly
interested
in
fieldwork,
nor
were
they
interested
in
examining
how
the
cultural
elements
and
institutions
fit
together.
Toward
the
turn
of
the
twentieth
century,
a
number
of
anthropologists
became
dissatisfied
with
this
categorization
of
cultural
elements;
historical
reconstructions
also
came
to
seem
increasingly
speculative.
Under
the
influence
of
several
younger
scholars,
a
new
approach
came
to
predominate
among
British
anthropologists,
concerned
with
analyzing
how
societies
held
together
in
the
present
(synchronic
analysis,
rather
than
diachronic
or
historical
analysis),
and
emphasizing
long-term
(one
to
several
years)
immersion
fieldwork.
Cambridge
University
financed
a
multidisciplinary
expedition
to
the
Torres
Strait
Islands
in
1898,
organized
by
Alfred
Court
Haddon
and
including
a
physician-anthropologist,
William
Rivers,
as
well
as
a
linguist,
a
botanist,
other
specialists.
The
findings
of
the
expedition
set
new
standards
for
ethnographic
description.
A
decade
and
a
half
later,
Polish
anthropology
student
Bronisław
Malinowski
(1884–1942)
was
beginning
what
he
expected
to
be
a
brief
period
of
fieldwork
in
the
old
model,
collecting
lists
of
cultural
items,
when
the
outbreak
of
the
First
World
War
stranded
him
in
New
Guinea.
As
a
subject
of
the
Austro-Hungarian
Empire
resident
on
a
British
colonial
possession,
he
was
effectively
confined
to
New
Guinea
for
several
years.
He
made
use
of
the
time
by
undertaking
far
more
intensive
fieldwork
than
had
been
done
by
"British"
anthropologists,
and
his
classic
ethnography,
"Argonauts
of
the
Western
Pacific"
(1922)
advocated
an
approach
to
fieldwork
that
became
standard
in
the
field:
getting
"the
native's
point
of
view"
through
participant
observation.
Theoretically,
he
advocated
a
functionalist
interpretation,
which
examined
how
social
institutions
functioned
to
satisfy
individual
needs.
British
social
anthropology
had
an
expansive
moment
in
the
Interwar
period,
with
key
contributions
coming
from
the
Polish-British
Bronisław
Malinowski
and
Meyer
Fortes
A.
R.
Radcliffe-Brown
also
published
a
seminal
work
in
1922.
He
had
carried
out
his
initial
fieldwork
in
the
Andaman
Islands
in
the
old
style
of
historical
reconstruction.
However,
after
reading
the
work
of
French
sociologists
Émile
Durkheim
and
Marcel
Mauss,
Radcliffe-Brown
published
an
account
of
his
research
(entitled
simply
"The
Andaman
Islanders")
that
paid
close
attention
to
the
meaning
and
purpose
of
rituals
and
myths.
Over
time,
he
developed
an
approach
known
as
structural-functionalism,
which
focused
on
how
institutions
in
societies
worked
to
balance
out
or
create
an
equilibrium
in
the
social
system
to
keep
it
functioning
harmoniously.
(This
contrasted
with
Malinowski's
functionalism,
and
was
quite
different
from
the
later
French
structuralism,
which
examined
the
conceptual
structures
in
language
and
symbolism.)
Malinowski
and
Radcliffe-Brown's
influence
stemmed
from
the
fact
that
they,
like
Boas,
actively
trained
students
and
aggressively
built
up
institutions
that
furthered
their
programmatic
ambitions.
This
was
particularly
the
case
with
Radcliffe-Brown,
who
spread
his
agenda
for
"Social
Anthropology"
by
teaching
at
universities
across
the
British
Commonwealth.
From
the
late
1930s
until
the
postwar
period
appeared
a
string
of
monographs
and
edited
volumes
that
cemented
the
paradigm
of
British
Social
Anthropology
(BSA).
Famous
ethnographies
include
"The
Nuer,"
by
Edward
Evan
Evans-Pritchard,
and
"The
Dynamics
of
Clanship
Among
the
Tallensi,"
by
Meyer
Fortes;
well-known
edited
volumes
include
"African
Systems
of
Kinship
and
Marriage"
and
"African
Political
Systems."
Max
Gluckman,
together
with
many
of
his
colleagues
at
the
Rhodes-Livingstone
Institute
and
students
at
Manchester
University,
collectively
known
as
the
Manchester
School,
took
BSA
in
new
directions
through
their
introduction
of
explicitly
Marxist-informed
theory,
their
emphasis
on
conflicts
and
conflict
resolution,
and
their
attention
to
the
ways
in
which
individuals
negotiate
and
make
use
of
the
social
structural
possibilities.
In
Britain,
anthropology
had
a
great
intellectual
impact,
it
"contributed
to
the
erosion
of
Christianity,
the
growth
of
cultural
relativism,
an
awareness
of
the
survival
of
the
primitive
in
modern
life,
and
the
replacement
of
diachronic
modes
of
analysis
with
synchronic,
all
of
which
are
central
to
modern
culture."
Later
in
the
1960s
and
1970s,
Edmund
Leach
and
his
students
Mary
Douglas
and
Nur
Yalman,
among
others,
introduced
French
structuralism
in
the
style
of
Lévi-Strauss;
while
British
anthropology
has
continued
to
emphasize
social
organization
and
economics
over
purely
symbolic
or
literary
topics,
differences
among
British,
French,
and
American
sociocultural
anthropologies
have
diminished
with
increasing
dialogue
and
borrowing
of
both
theory
and
methods.
Today,
social
anthropology
in
Britain
engages
internationally
with
many
other
social
theories
and
has
branched
in
many
directions.
In
countries
of
the
British
Commonwealth,
social
anthropology
has
often
been
institutionally
separate
from
physical
anthropology
and
primatology,
which
may
be
connected
with
departments
of
biology
or
zoology;
and
from
archaeology,
which
may
be
connected
with
departments
of
Classics,
Egyptology,
and
the
like.
In
other
countries
(and
in
some,
particularly
smaller,
British
and
North
American
universities),
anthropologists
have
also
found
themselves
institutionally
linked
with
scholars
of
folklore,
museum
studies,
human
geography,
sociology,
social
relations,
ethnic
studies,
cultural
studies,
and
social
work.
19th
Century
to
1940s.
From
its
beginnings
in
the
early
19th
century
through
the
early
20th
century,
anthropology
in
the
United
States
was
influenced
by
the
presence
of
Native
American
societies.
Cultural
anthropology
in
the
United
States
was
influenced
greatly
by
the
ready
availability
of
Native
American
societies
as
ethnographic
subjects.
The
field
was
pioneered
by
staff
of
the
Bureau
of
Indian
Affairs
and
the
Smithsonian
Institution's
Bureau
of
American
Ethnology,
men
such
as
John
Wesley
Powell
and
Frank
Hamilton
Cushing.
Lewis
Henry
Morgan
(1818–1881),
a
lawyer
from
Rochester,
New
York,
became
an
advocate
for
and
ethnological
scholar
of
the
Iroquois.
His
comparative
analyses
of
religion,
government,
material
culture,
and
especially
kinship
patterns
proved
to
be
influential
contributions
to
the
field
of
anthropology.
Like
other
scholars
of
his
day
(such
as
Edward
Tylor),
Morgan
argued
that
human
societies
could
be
classified
into
categories
of
cultural
evolution
on
a
scale
of
progression
that
ranged
from
"savagery",
to
"barbarism",
to
"civilization".
Generally,
Morgan
used
technology
(such
as
bowmaking
or
pottery)
as
an
indicator
of
position
on
this
scale.
Boasian
anthropology.
Franz
Boas
established
academic
anthropology
in
the
United
States
in
opposition
to
this
sort
of
evolutionary
perspective.
His
approach
was
empirical,
skeptical
of
overgeneralizations,
and
eschewed
attempts
to
establish
universal
laws.
For
example,
Boas
studied
immigrant
children
to
demonstrate
that
biological
race
was
not
immutable,
and
that
human
conduct
and
behavior
resulted
from
nurture,
rather
than
nature.
Influenced
by
the
German
tradition,
Boas
argued
that
the
world
was
full
of
distinct
"cultures,"
rather
than
societies
whose
evolution
could
be
measured
by
how
much
or
how
little
"civilization"
they
had.
He
believed
that
each
culture
has
to
be
studied
in
its
particularity,
and
argued
that
cross-cultural
generalizations,
like
those
made
in
the
natural
sciences,
were
not
possible.
In
doing
so,
he
fought
discrimination
against
immigrants,
blacks,
and
indigenous
peoples
of
the
Americas.
Many
American
anthropologists
adopted
his
agenda
for
social
reform,
and
theories
of
race
continue
to
be
popular
subjects
for
anthropologists
today.
The
so-called
"Four
Field
Approach"
has
its
origins
in
Boasian
Anthropology,
dividing
the
discipline
in
the
four
crucial
and
interrelated
fields
of
sociocultural,
biological,
linguistic,
and
archaic
anthropology
(e.g.
archaeology).
Anthropology
in
the
United
States
continues
to
be
deeply
influenced
by
the
Boasian
tradition,
especially
its
emphasis
on
culture.
Boas
used
his
positions
at
Columbia
University
and
the
American
Museum
of
Natural
History
to
train
and
develop
multiple
generations
of
students.
His
first
generation
of
students
included
Alfred
Kroeber,
Robert
Lowie,
Edward
Sapir
and
Ruth
Benedict,
who
each
produced
richly
detailed
studies
of
indigenous
North
American
cultures.
They
provided
a
wealth
of
details
used
to
attack
the
theory
of
a
single
evolutionary
process.
Kroeber
and
Sapir's
focus
on
Native
American
languages
helped
establish
linguistics
as
a
truly
general
science
and
free
it
from
its
historical
focus
on
Indo-European
languages.
The
publication
of
Alfred
Kroeber's
textbook,
"Anthropology,"
marked
a
turning
point
in
American
anthropology.
After
three
decades
of
amassing
material,
Boasians
felt
a
growing
urge
to
generalize.
This
was
most
obvious
in
the
'Culture
and
Personality'
studies
carried
out
by
younger
Boasians
such
as
Margaret
Mead
and
Ruth
Benedict.
Influenced
by
psychoanalytic
psychologists
including
Sigmund
Freud
and
Carl
Jung,
these
authors
sought
to
understand
the
way
that
individual
personalities
were
shaped
by
the
wider
cultural
and
social
forces
in
which
they
grew
up.
Though
such
works
as
"Coming
of
Age
in
Samoa"
and
"The
Chrysanthemum
and
the
Sword"
remain
popular
with
the
American
public,
Mead
and
Benedict
never
had
the
impact
on
the
discipline
of
anthropology
that
some
expected.
Boas
had
planned
for
Ruth
Benedict
to
succeed
him
as
chair
of
Columbia's
anthropology
department,
but
she
was
sidelined
by
Ralph
Linton,
and
Mead
was
limited
to
her
offices
at
the
AMNH.
Canada.
Canadian
anthropology
began,
as
in
other
parts
of
the
Colonial
world,
as
ethnological
data
in
the
records
of
travellers
and
missionaries.
In
Canada,
Jesuit
missionaries
such
as
Fathers
LeClercq,
Le
Jeune
and
Sagard,
in
the
1600s,
provide
the
oldest
ethnographic
records
of
native
tribes
in
what
was
then
the
Domain
of
Canada.
True
anthropology
began
with
a
Government
department:
the
Geological
Survey
of
Canada,
and
George
Mercer
Dawson
(director
in
1895).
Dawson's
support
for
anthropology
created
impetus
for
the
profession
in
Canada.
This
was
expanded
upon
by
Prime
Minister
Wilfrid
Laurier,
who
established
a
Division
of
Anthropology
within
the
Geological
Survey
in
1910.
Anthropologists
were
recruited
from
England
and
the
USA,
setting
the
foundation
for
the
unique
Canadian
style
of
anthropology.
Scholars
include
the
linguist
and
Boasian
Edward
Sapir.
France.
Anthropology
in
France
has
a
less
clear
genealogy
than
the
British
and
American
traditions,
in
part
because
many
French
writers
influential
in
anthropology
have
been
trained
or
held
faculty
positions
in
sociology,
philosophy,
or
other
fields
rather
than
in
anthropology.
Most
commentators
consider
Marcel
Mauss
(1872–1950),
nephew
of
the
influential
sociologist
Émile
Durkheim
to
be
the
founder
of
the
French
anthropological
tradition.
Mauss
belonged
to
Durkheim's
Année
Sociologique
group;
and
while
Durkheim
and
others
examined
the
state
of
modern
societies,
Mauss
and
his
collaborators
(such
as
Henri
Hubert
and
Robert
Hertz)
drew
on
ethnography
and
philology
to
analyze
societies
which
were
not
as
'differentiated'
as
European
nation
states.
Two
works
by
Mauss
in
particular
proved
to
have
enduring
relevance:
"Essay
on
the
Gift"
a
seminal
analysis
of
exchange
and
reciprocity,
and
his
Huxley
lecture
on
the
notion
of
the
person,
the
first
comparative
study
of
notions
of
person
and
selfhood
cross-culturally.
Throughout
the
interwar
years,
French
interest
in
anthropology
often
dovetailed
with
wider
cultural
movements
such
as
surrealism
and
primitivism
which
drew
on
ethnography
for
inspiration.
Marcel
Griaule
and
Michel
Leiris
are
examples
of
people
who
combined
anthropology
with
the
French
avant-garde.
During
this
time
most
of
what
is
known
as
"ethnologie"
was
restricted
to
museums,
such
as
the
Musée
de
l'Homme
founded
by
Paul
Rivet,
and
anthropology
had
a
close
relationship
with
studies
of
folklore.
Above
all,
however,
it
was
Claude
Lévi-Strauss
who
helped
institutionalize
anthropology
in
France.
Along
with
the
enormous
influence
his
structuralism
exerted
across
multiple
disciplines,
Lévi-Strauss
established
ties
with
American
and
British
anthropologists.
At
the
same
time
he
established
centers
and
laboratories
within
France
to
provide
an
institutional
context
within
anthropology
while
training
influential
students
such
as
Maurice
Godelier
and
Françoise
Héritier
who
would
prove
influential
in
the
world
of
French
anthropology.
Much
of
the
distinct
character
of
France's
anthropology
today
is
a
result
of
the
fact
that
most
anthropology
is
carried
out
in
nationally
funded
research
laboratories
(CNRS)
rather
than
academic
departments
in
universities.
Other
influential
writers
in
the
1970s
include
Pierre
Clastres,
who
explains
in
his
books
on
the
Guayaki
tribe
in
Paraguay
that
"primitive
societies"
actively
oppose
the
institution
of
the
state.
Therefore,
these
stateless
societies
are
not
less
evolved
than
societies
with
states,
but
took
the
active
choice
of
conjuring
the
institution
of
authority
as
a
separate
function
from
society.
The
leader
is
only
a
spokesperson
for
the
group
when
it
has
to
deal
with
other
groups
("international
relations")
but
has
no
inside
authority,
and
may
be
violently
removed
if
he
attempts
to
abuse
this
position.
The
most
important
French
social
theorist
since
Foucault
and
Lévi-Strauss
is
Pierre
Bourdieu,
who
trained
formally
in
philosophy
and
sociology
and
eventually
held
the
Chair
of
Sociology
at
the
Collège
de
France.
Like
Mauss
and
others
before
him,
however,
he
worked
on
topics
both
in
sociology
and
anthropology.
His
fieldwork
among
the
Kabyles
of
Algeria
places
him
solidly
in
anthropology,
while
his
analysis
of
the
function
and
reproduction
of
fashion
and
cultural
capital
in
European
societies
places
him
as
solidly
in
sociology.
Other
countries.
Anthropology
in
Greece
and
Portugal
is
much
influenced
by
British
anthropology.
In
Greece,
there
was
since
the
19th
century
a
science
of
the
folklore
called
"laographia"
(laography),
in
the
form
of
"a
science
of
the
interior",
although
theoretically
weak;
but
the
connotation
of
the
field
deeply
changed
after
World
War
II,
when
a
wave
of
Anglo-American
anthropologists
introduced
a
science
"of
the
outside".
In
Italy,
the
development
of
ethnology
and
related
studies
did
not
receive
as
much
attention
as
other
branches
of
learning.
Germany
and
Norway
are
the
countries
that
showed
the
most
division
and
conflict
between
scholars
focusing
on
domestic
socio-cultural
issues
and
scholars
focusing
on
"other"
societies.
Post-World
War
II.
Before
WWII
British
'social
anthropology'
and
American
'cultural
anthropology'
were
still
distinct
traditions.
After
the
war,
enough
British
and
American
anthropologists
borrowed
ideas
and
methodological
approaches
from
one
another
that
some
began
to
speak
of
them
collectively
as
'sociocultural'
anthropology.
In
the
1950s
and
mid-1960s
anthropology
tended
increasingly
to
model
itself
after
the
natural
sciences.
Some
anthropologists,
such
as
Lloyd
Fallers
and
Clifford
Geertz,
focused
on
processes
of
modernization
by
which
newly
independent
states
could
develop.
Others,
such
as
Julian
Steward
and
Leslie
White,
focused
on
how
societies
evolve
and
fit
their
ecological
niche—an
approach
popularized
by
Marvin
Harris.
Economic
anthropology
as
influenced
by
Karl
Polanyi
and
practiced
by
Marshall
Sahlins
and
George
Dalton
challenged
standard
neoclassical
economics
to
take
account
of
cultural
and
social
factors,
and
employed
Marxian
analysis
into
anthropological
study.
In
England,
British
Social
Anthropology's
paradigm
began
to
fragment
as
Max
Gluckman
and
Peter
Worsley
experimented
with
Marxism
and
authors
such
as
Rodney
Needham
and
Edmund
Leach
incorporated
Lévi-Strauss's
structuralism
into
their
work.
Structuralism
also
influenced
a
number
of
developments
in
1960s
and
1970s,
including
cognitive
anthropology
and
componential
analysis.
Authors
such
as
David
Schneider,
Clifford
Geertz,
and
Marshall
Sahlins
developed
a
more
fleshed-out
concept
of
culture
as
a
web
of
meaning
or
signification,
which
proved
very
popular
within
and
beyond
the
discipline.
In
keeping
with
the
times,
much
of
anthropology
became
politicized
through
the
Algerian
War
of
Independence
and
opposition
to
the
Vietnam
War;
Marxism
became
an
increasingly
popular
theoretical
approach
in
the
discipline.
By
the
1970s
the
authors
of
volumes
such
as
"Reinventing
Anthropology"
worried
about
anthropology's
relevance.
Since
the
1980s
issues
of
power,
such
as
those
examined
in
Eric
Wolf's
"Europe
and
the
People
Without
History",
have
been
central
to
the
discipline.
In
the
80s
books
like
"Anthropology
and
the
Colonial
Encounter"
pondered
anthropology's
ties
to
colonial
inequality,
while
the
immense
popularity
of
theorists
such
as
Antonio
Gramsci
and
Michel
Foucault
moved
issues
of
power
and
hegemony
into
the
spotlight.
Gender
and
sexuality
became
popular
topics,
as
did
the
relationship
between
history
and
anthropology,
influenced
by
Marshall
Sahlins
(again),
who
drew
on
Lévi-Strauss
and
Fernand
Braudel
to
examine
the
relationship
between
social
structure
and
individual
agency.
Also
influential
in
these
issues
were
Nietzsche,
Heidegger,
the
critical
theory
of
the
Frankfurt
School,
Derrida
and
Lacan.
In
the
late
1980s
and
1990s
authors
such
as
George
Marcus
and
James
Clifford
pondered
ethnographic
authority,
particularly
how
and
why
anthropological
knowledge
was
possible
and
authoritative.
They
were
reflecting
trends
in
research
and
discourse
initiated
by
Feminists
in
the
academy,
although
they
excused
themselves
from
commenting
specifically
on
those
pioneering
critics.
Nevertheless,
key
aspects
of
feminist
theorizing
and
methods
became
"de
rigueur"
as
part
of
the
'post-modern
moment'
in
anthropology:
Ethnographies
became
more
reflexive,
explicitly
addressing
the
author's
methodology,
cultural,
gender
and
racial
positioning,
and
their
influence
on
his
or
her
ethnographic
analysis.
This
was
part
of
a
more
general
trend
of
postmodernism
that
was
popular
contemporaneously.
Currently
anthropologists
pay
attention
to
a
wide
variety
of
issues
pertaining
to
the
contemporary
world,
including
globalization,
medicine
and
biotechnology,
indigenous
rights,
virtual
communities,
and
the
anthropology
of
industrialized
societies.
Controversies
about
its
history.
Anthropologists,
like
other
researchers
(especially
historians
and
scientists
engaged
in
field
research),
have
over
time
assisted
state
policies
and
projects,
especially
colonialism.
Military.
Anthropologists'
involvement
with
the
U.S.
government,
in
particular,
has
caused
bitter
controversy
within
the
discipline.
Franz
Boas
publicly
objected
to
US
participation
in
World
War
I,
and
after
the
war
he
published
a
brief
expose
and
condemnation
of
the
participation
of
several
American
archaeologists
in
espionage
in
Mexico
under
their
cover
as
scientists.
But
by
the
1940s,
many
of
Boas'
anthropologist
contemporaries
were
active
in
the
allied
war
effort
against
the
"Axis"
(Nazi
Germany,
Fascist
Italy,
and
Imperial
Japan).
Many
served
in
the
armed
forces
but
others
worked
in
intelligence
(for
example,
Office
of
Strategic
Services
(OSS)
and
the
Office
of
War
Information).
At
the
same
time,
David
H.
Price's
work
on
American
anthropology
during
the
Cold
War
provides
detailed
accounts
of
the
pursuit
and
dismissal
of
several
anthropologists
from
their
jobs
for
communist
sympathies.
Attempts
to
accuse
anthropologists
of
complicity
with
the
CIA
and
government
intelligence
activities
during
the
Vietnam
War
years
have
turned
up
surprisingly
little
(although
anthropologist
Hugo
Nutini
was
active
in
the
stillborn
Project
Camelot).
Many
anthropologists
(students
and
teachers)
were
active
in
the
antiwar
movement
and
a
great
many
resolutions
condemning
the
war
in
all
its
aspects
were
passed
overwhelmingly
at
the
annual
meetings
of
the
American
Anthropological
Association
(AAA).
In
the
decades
since
the
Vietnam
war
the
tone
of
cultural
and
social
anthropology,
at
least,
has
been
increasingly
politicized,
with
the
dominant
liberal
tone
of
earlier
generations
replaced
with
one
more
radical,
a
mix
of,
and
varying
degrees
of,
Marxist,
feminist,
anarchist,
post-colonial,
post-modern,
Saidian,
Foucauldian,
identity-based,
and
more.
Professional
anthropological
bodies
often
object
to
the
use
of
anthropology
for
the
benefit
of
the
state.
Their
codes
of
ethics
or
statements
may
proscribe
anthropologists
from
giving
secret
briefings.
The
Association
of
Social
Anthropologists
of
the
UK
and
Commonwealth
(ASA)
has
called
certain
scholarships
ethically
dangerous.
The
AAA's
current
'Statement
of
Professional
Responsibility'
clearly
states
that
"in
relation
with
their
own
government
and
with
host
governments...
no
secret
research,
no
secret
reports
or
debriefings
of
any
kind
should
be
agreed
to
or
given."
However,
anthropologists,
along
with
other
social
scientists,
are
again
being
used
in
warfare
as
part
of
the.
The
Christian
Science
Monitor
reports
that
"Counterinsurgency
efforts
focus
on
better
grasping
and
meeting
local
needs"
in
Afghanistan,
under
the
rubric
of
"Human
Terrain
Team"
(HTT).
Focus
on
other
cultures===.
Some
authors
argue
that
anthropology
originated
and
developed
as
the
study
of
"other
cultures",
both
in
terms
of
time
(past
societies)
and
space
(non-European/non-Western
societies).
For
example,
the
classic
of
urban
anthropology,
Ulf
Hannerz
in
the
introduction
to
his
seminal
"Exploring
the
City:
Inquiries
Toward
an
Urban
Anthropology"
mentions
that
the
"Third
World"
had
habitually
received
most
of
attention;
anthropologists
who
traditionally
specialized
in
"other
cultures"
looked
for
them
far
away
and
started
to
look
"across
the
tracks"
only
in
late
1960s.
Now
there
exist
many
works
focusing
on
peoples
and
topics
very
close
to
the
author's
"home".
It
is
also
argued
that
other
fields
of
study,
like
History
and
Sociology,
on
the
contrary
focus
disproportionately
on
the
West.
In
France,
the
study
of
existing
contemporary
society
has
been
traditionally
left
to
sociologists,
but
this
is
increasingly
changing,
starting
in
the
1970s
from
scholars
like
Isac
Chiva
and
journals
like
"Terrain"
("fieldwork"),
and
developing
with
the
center
founded
by
Marc
Augé
("Le
Centre
d'anthropologie
des
mondes
contemporains",
the
Anthropological
Research
Center
of
Contemporary
Societies).
The
same
approach
of
focusing
on
"modern
world"
topics
by
"Terrain",
was
also
present
in
the
British
Manchester
School
of
the
1950s.
---END.OF.DOCUMENT---
Agricultural
science.
Agricultural
science
is
a
broad
multidisciplinary
field
that
encompasses
the
parts
of
exact,
natural,
economic
and
social
sciences
that
are
used
in
the
practice
and
understanding
of
agriculture.
(Veterinary
science,
but
not
animal
science,
is
often
excluded
from
the
definition.)
Agricultural
science:
a
local
science.
With
the
exception
of
theoretical
agronomy,
research
in
agronomy,
more
than
in
any
other
field,
is
strongly
related
to
local
areas.
It
can
be
considered
a
science
of
ecoregions,
because
it
is
closely
linked
to
soil
properties
and
climate,
which
are
never
exactly
the
same
from
one
place
to
another.
Many
people
think
an
agricultural
production
system
relying
on
local
weather,
soil
characteristics,
and
specific
crops
has
to
be
studied
locally.
Others
feel
a
need
to
know
and
understand
production
systems
in
as
many
areas
as
possible,
and
the
human
dimension
of
interaction
with
nature.
History
of
agricultural
science.
Agricultural
science
began
with
Gregor
Mendel's
genetic
work,
but
in
modern
terms
might
be
better
dated
from
the
chemical
fertilizer
outputs
of
plant
physiological
understanding
in
eighteenth
century
Germany.
In
the
United
States,
a
scientific
revolution
in
agriculture
began
with
the
Hatch
Act
of
1887,
which
used
the
term
"agricultural
science".
The
Hatch
Act
was
driven
by
farmers'
interest
in
knowing
the
constituents
of
early
artificial
fertilizer.
The
Smith-Hughes
Act
of
1917
shifted
agricultural
education
back
to
its
vocational
roots,
but
the
scientific
foundation
had
been
built.
After
1906,
public
expenditures
on
agricultural
research
in
the
US
exceeded
private
expenditures
for
the
next
44
years.
Intensification
of
agriculture
since
the
1960s
in
developed
and
developing
countries,
often
referred
to
as
the
Green
Revolution,
was
closely
tied
to
progress
made
in
selecting
and
improving
crops
and
animals
for
high
productivity,
as
well
as
to
developing
additional
inputs
such
as
artificial
fertilizers
and
phytosanitary
products.
As
the
oldest
and
largest
human
intervention
in
nature,
the
environmental
impact
of
agriculture
in
general
and
more
recently
intensive
agriculture,
industrial
development,
and
population
growth
have
raised
many
questions
among
agricultural
scientists
and
have
led
to
the
development
and
emergence
of
new
fields.
These
include
technological
fields
that
assume
the
solution
to
technological
problems
lies
in
better
technology,
such
as
integrated
pest
management,
waste
treatment
technologies,
landscape
architecture,
genomics,
and
agricultural
philosophy
fields
that
include
references
to
food
production
as
something
essentially
different
from
non-essential
economic
'goods'.
In
fact,
the
interaction
between
these
two
approaches
provide
a
fertile
field
for
deeper
understanding
in
agricultural
science.
New
technologies,
such
as
biotechnology
and
computer
science
(for
data
processing
and
storage),
and
technological
advances
have
made
it
possible
to
develop
new
research
fields,
including
genetic
engineering,
agrophysics,
improved
statistical
analysis,
and
precision
farming.
Balancing
these,
as
above,
are
the
natural
and
human
sciences
of
agricultural
science
that
seek
to
understand
the
human-nature
interactions
of
traditional
agriculture,
including
interaction
of
religion
and
agriculture,
and
the
non-material
components
of
agricultural
production
systems.
Agricultural
science
and
agriculture
crisis.
Agriculture
sciences
seek
to
feed
the
world's
population
while
preventing
biosafety
problems
that
may
affect
human
health
and
the
environment.
This
requires
promoting
good
management
of
natural
resources
and
respect
for
the
environment,
and
increasingly
concern
for
the
psychological
wellbeing
of
all
concerned
in
the
food
production
and
consumption
system.
Economic,
environmental,
and
social
aspects
of
agriculture
sciences
are
subjects
of
ongoing
debate.
Recent
crises
(such
as
avian
influenza,
mad
cow
disease
and
issues
such
as
the
use
of
genetically
modified
organisms)
illustrate
the
complexity
and
importance
of
this
debate.
---END.OF.DOCUMENT---
Alchemy.
Alchemy,
originally
derived
from
the
Ancient
Greek
word
"khemia"
(Χημία)
meaning
"art
of
transmuting
metals",
later
arabicized
as
"al-kimia"
(الكيمياء),
is
both
a
philosophy
and
an
ancient
practice
focused
on
the
attempt
to
change
base
metals
into
gold,
investigating
the
preparation
of
the
"elixir
of
longevity",
and
achieving
ultimate
wisdom,
involving
the
improvement
of
the
alchemist
as
well
as
the
making
of
several
substances
described
as
possessing
unusual
properties.
The
practical
aspect
of
alchemy
generated
the
basics
of
modern
inorganic
chemistry,
namely
concerning
procedures,
equipment
and
the
identification
and
use
of
many
current
substances.
Alchemy
has
been
practiced
in
Mesopotamia
(comprising
much
of
today's
Iraq),
Egypt,
Persia
(today's
Iran),
India,
China,
Japan,
Korea
and
in
Classical
Greece
and
Rome,
in
the
Post-Islamic
Persia,
and
then
in
Europe
up
to
the
20th
century,
in
a
complex
network
of
schools
and
philosophical
systems
spanning
at
least
2500
years.
Etymology.
The
word
alchemy
derives
in
turn
from
the
Old
French
"alkemie";
from
the
Medieval
Latin
"alchimia";
from
the
Arabic
"al-kimia"
(الكيمياء);
and
ultimately
from
the
Ancient
Greek
"khemia"
(Χημία)
meaning
"art
of
transmuting
metals".
During
the
seventeenth
century
chemistry
as
a
separate
science
was
derived
from
Alchemy,
with
the
work
of
Robert
Boyle,
sometimes
known
as
"The
father
of
Chemistry",
who
in
his
book
"The
Skeptical
Chymist"
attacked
Paracelsus
and
the
old
Aristotelian
concepts
of
the
elements
and
laid
down
the
foundations
of
modern
chemistry.
Alchemy
as
a
philosophical
and
spiritual
discipline.
Alchemy
became
known
as
the
"spagyric
art"
after
Greek
words
meaning
"to
separate"
and
"to
join
together"
in
the
16th
century,
the
word
probably
being
coined
by
Paracelsus.
Compare
this
with
one
of
the
dictums
of
Alchemy
in
Latin:
Solve
et
Coagula —
"Separate,
and
Join
Together"
(or
"dissolve
and
coagulate").
The
best-known
goals
of
the
alchemists
were
the
transmutation
of
common
metals
into
gold
(called
chrysopoeia)
or
silver
(less
well
known
is
plant
alchemy,
or
"spagyric");
the
creation
of
a
"panacea",
or
the
elixir
of
life,
a
remedy
that,
it
was
supposed,
would
cure
all
diseases
and
prolong
life
indefinitely;
and
the
discovery
of
a
universal
solvent.
Although
these
were
not
the
only
uses
for
the
discipline,
they
were
the
ones
most
documented
and
well-known.
Certain
Hermetic
schools
argue
that
the
transmutation
of
lead
into
gold
is
analogical
for
the
transmutation
of
the
physical
body
(Saturn
or
lead)
into
(Gold)
with
the
goal
of
attaining
immortality.
This
is
described
as
Internal
Alchemy.
Starting
with
the
Middle
Ages,
Persian
and
European
alchemists
invested
much
effort
in
the
search
for
the
"philosopher's
stone",
a
legendary
substance
that
was
believed
to
be
an
essential
ingredient
for
either
or
both
of
those
goals.
Pope
John
XXII
issued
a
bull
against
alchemical
counterfeiting,
and
the
Cistercians
banned
the
practice
amongst
their
members.
In
1403,
Henry
IV
of
England
banned
the
practice
of
Alchemy.
In
the
late
14th
century,
Piers
the
Ploughman
and
Chaucer
both
painted
unflattering
pictures
of
Alchemists
as
thieves
and
liars.
By
contrast,
Rudolf
II,
Holy
Roman
Emperor,
in
the
late
16th
century,
sponsored
various
alchemists
in
their
work
at
his
court
in
Prague.
It
is
a
popular
belief
that
Alchemists
made
mundane
contributions
to
the
"chemical"
industries
of
the
day—ore
testing
and
refining,
metalworking,
production
of
gunpowder,
ink,
dyes,
paints,
cosmetics,
leather
tanning,
ceramics,
glass
manufacture,
preparation
of
extracts,
liquors,
and
so
on
(it
seems
that
the
preparation
of
"aqua
vitae",
the
"water
of
life",
was
a
fairly
popular
"experiment"
among
European
alchemists).
In
reality,
although
Alchemists
contributed
distillation
to
Western
Europe,
they
did
little
for
any
known
industry.
Long
before
Alchemists
appeared,
goldsmiths
knew
how
to
tell
what
was
good
gold
or
fake,
and
industrial
technology
grew
by
the
work
of
the
artisans
themselves,
rather
than
any
Alchemical
helpers.
The
double
origin
of
Alchemy
in
Greek
philosophy
as
well
as
in
Egyptian
and
Mesopotamian
technology
set,
from
the
start,
a
double
approach:
the
technological,
operative
one,
which
Marie-Louise
von
Franz
call
extravert,
and
the
mystic,
contemplative,
psychological
one,
which
von
Franz
names
as
introvert.
These
are
not
mutually
exclusive,
but
complementary
instead,
as
meditation
requires
practice
in
the
real
world,
and
conversely.
Several
early
alchemists,
such
as
Zosimos
of
Panopolis,
are
recorded
as
viewing
alchemy
as
a
spiritual
discipline,
and,
in
the
Middle
Ages,
metaphysical
aspects,
substances,
physical
states,
and
molecular
material
processes
as
mere
metaphors
for
spiritual
entities,
spiritual
states,
and,
ultimately,
transformations.
In
this
sense,
the
literal
meanings
of
'Alchemical
Formulas'
were
a
blind,
hiding
their
true
spiritual
philosophy,
which
being
at
odds
with
the
Medieval
Christian
Church
was
a
necessity
that
could
have
otherwise
led
them
to
the
"stake
and
rack"
of
the
Inquisition
under
charges
of
heresy.
Thus,
both
the
transmutation
of
common
metals
into
gold
and
the
universal
panacea
symbolized
evolution
from
an
imperfect,
diseased,
corruptible,
and
ephemeral
state
towards
a
perfect,
healthy,
incorruptible,
and
everlasting
state;
and
the
philosopher's
stone
then
represented
a
mystic
key
that
would
make
this
evolution
possible.
Applied
to
the
alchemist
himself,
the
twin
goal
symbolized
his
evolution
from
ignorance
to
enlightenment,
and
the
stone
represented
a
hidden
spiritual
truth
or
power
that
would
lead
to
that
goal.
In
texts
that
are
written
according
to
this
view,
the
cryptic
alchemical
symbols,
diagrams,
and
textual
imagery
of
late
alchemical
works
typically
contain
multiple
layers
of
meanings,
allegories,
and
references
to
other
equally
cryptic
works;
and
must
be
laboriously
"decoded"
in
order
to
discover
their
true
meaning.
Q.
When
the
Philosophers
speak
of
gold
and
silver,
from
which
they
extract
their
matter,
are
we
to
suppose
that
they
refer
to
the
vulgar
gold
and
silver?
A.
By
no
means;
vulgar
silver
and
gold
are
dead,
while
those
of
the
Philosophers
are
full
of
life.
Psychology.
Alchemical
symbolism
has
been
occasionally
used
by
psychologists
and
philosophers.
Carl
Jung
reexamined
alchemical
symbolism
and
theory
and
began
to
show
the
inner
meaning
of
alchemical
work
as
a
spiritual
path.
Alchemical
philosophy,
symbols
and
methods
have
enjoyed
something
of
a
renaissance
in
post-modern
contexts.
Jung
saw
alchemy
as
a
Western
proto-psychology
dedicated
to
the
achievement
of
individuation.
In
his
interpretation,
alchemy
was
the
vessel
by
which
Gnosticism
survived
its
various
purges
into
the
Renaissance,
a
concept
also
followed
by
others
such
as
Stephan
A.
Hoeller.
In
this
sense,
Jung
viewed
alchemy
as
comparable
to
a
Yoga
of
the
East,
and
more
adequate
to
the
Western
mind
than
Eastern
religions
and
philosophies.
The
practice
of
Alchemy
seemed
to
change
the
mind
and
spirit
of
the
Alchemist.
Conversely,
spontaneous
changes
on
the
mind
of
Western
people
undergoing
any
important
stage
in
individuation
seems
to
produce,
on
occasion,
imagery
known
to
Alchemy
and
relevant
to
the
person's
situation.
His
interpretation
of
Chinese
alchemical
texts
in
terms
of
his
analytical
psychology
also
served
the
function
of
comparing
Eastern
and
Western
alchemical
imagery
and
core
concepts
and
hence
its
possible
inner
sources
(archetypes).
Marie-Louise
von
Franz,
a
disciple
of
Jung,
continued
Jung's
studies
on
Alchemy
and
its
psychological
meaning.
Magnum
opus.
After
the
15th
century,
many
writers
tended
to
compress
"citrinitas"
into
"rubedo"
and
consider
only
three
stages.
However,
it
is
in
citrinitas
that
the
Chemical
Wedding
takes
place,
generating
the
Philosophical
Mercury
without
which
the
Philosopher's
Stone,
triumph
of
the
Work,
could
never
be
accomplished.
Within
the
Magnum
Opus
was
the
creation
of
the
Sanctum
Moleculae,
that
is
the
'Sacred
Masses'
that
were
derived
from
the
Sacrum
Particulae,
that
is
the
'Sacred
Particles',
needed
to
complete
the
process
of
achieving
the
Magnum
Opus.
Alchemy
as
a
subject
of
historical
research.
The
history
of
alchemy
has
become
a
vigorous
academic
field.
As
the
obscure
hermetic
language
of
the
alchemists
is
gradually
being
"deciphered",
historians
are
becoming
more
aware
of
the
intellectual
connections
between
that
discipline
and
other
facets
of
Western
cultural
history,
such
as
the
sociology
and
psychology
of
the
intellectual
communities,
kabbalism,
spiritualism,
Rosicrucianism,
and
other
mystic
movements,
cryptography,
witchcraft,
and
the
evolution
of
science
and
philosophy.
History.
In
a
historical
sense,
Alchemy
is
the
pursuit
of
transforming
common
metals
into
valuable
gold.
According
to
Marie-Louise
von
Franz,
the
initial
basis
for
alchemy
were
Egyptian
metal
technology
and
mummification,
Mesopotamian
technology
and
astrology,
and
Pre-Socratic
Greek
philosophers
such
as
Empedocles,
Thales
of
Miletus
and
Heraclitus.
The
origins
of
Western
alchemy
are
traceable
back
to
ancient
Egypt.
The
Leyden
papyrus
X
and
the
Stockholm
papyrus
along
with
the
Greek
magical
papyri
comprise
the
first
"book"
on
alchemy
still
existent.
Babylonian,
Greek
and
Indian
philosophers
theorized
that
there
were
only
four
classical
elements
(rather
than
today's
117
chemical
elements,
a
useful
analogy
is
with
the
highly
similar
states
of
matter);
Earth,
Fire,
Water,
and
Air.
The
Greek
philosophers,
in
order
to
prove
their
point,
burned
a
log:
The
log
was
the
earth,
the
flames
burning
it
was
fire,
the
smoke
being
released
was
air,
and
the
smoldering
soot
at
the
bottom
was
bubbling
water.
Because
of
this,
the
belief
that
these
four
"elements"
were
at
the
heart
of
everything
soon
spread,
only
later
being
replaced
in
the
Middle
Ages
by
Geber's
theory
of
seven
elements,
which
was
then
replaced
by
the
modern
theory
of
chemical
elements
during
the
early
modern
period.
Alchemy
encompasses
several
philosophical
traditions
spanning
four
millennia
and
three
continents.
These
traditions'
general
penchant
for
cryptic
and
symbolic
language
makes
it
hard
to
trace
their
mutual
influences
and
"genetic"
relationships.
Alchemy
starts
becoming
much
clearer
in
the
8th
century
with
the
works
of
the
Islamic
alchemist,
Jabir
ibn
Hayyan
(known
as
"Geber"
in
Europe),
who
introduced
a
methodical
and
experimental
approach
to
scientific
research
based
in
the
laboratory,
in
contrast
to
the
ancient
Greek
and
Egyptian
alchemists
whose
works
were
mainly
allegorical.
Other
famous
alchemists
include
Rhazes,
Avicenna
and
Imad
ul-din
in
Persia;
Wei
Boyang
in
Chinese
alchemy;
and
Nagarjuna
in
Indian
alchemy;
and
Albertus
Magnus
and
Pseudo-Geber
in
European
alchemy;
as
well
as
the
anonymous
author
of
the
"Mutus
Liber",
published
in
France
in
the
late
17th
century,
which
was
a
'wordless
book'
that
claimed
to
be
a
guide
to
making
the
philosopher's
stone,
using
a
series
of
15
symbols
and
illustrations.
The
philosopher's
stone
was
an
object
that
was
thought
to
be
able
to
amplify
one's
power
in
alchemy
and,
if
possible,
grant
the
user
ageless
immortality,
unless
he
fell
victim
to
burnings
or
drowning;
the
common
belief
was
that
fire
and
water
were
the
two
greater
elements
that
were
implemented
into
the
creation
of
the
stone.
In
the
case
of
the
Chinese
and
European
alchemists,
there
was
a
difference
between
the
two.
The
European
alchemists
tried
to
transmute
lead
into
gold,
and,
no
matter
how
futile
or
toxic
the
element,
would
continue
trying
until
it
was
royally
outlawed
later
into
the
century.
The
Chinese,
however,
paid
no
heed
to
the
philosopher's
stone
or
transmutation
of
lead
to
gold;
they
focused
more
on
medicine
for
the
greater
good.
During
Enlightenment,
these
"elixirs"
were
a
strong
cure
for
sicknesses,
unless
it
was
a
test
medicine.
In
general,
most
tests
were
fatal,
but
stabilized
elixirs
served
great
purposes.
On
the
other
hand,
the
Islamic
alchemists
were
interested
in
alchemy
for
a
variety
of
reasons,
whether
it
was
for
the
transmutation
of
metals
or
artificial
creation
of
life,
or
for
practical
uses
such
as
medicine.
Modern
connections
to
alchemy.
Persian
alchemy
was
a
forerunner
of
modern
scientific
chemistry.
Alchemists
used
many
of
the
same
laboratory
tools
that
are
used
today.
These
tools
were
not
usually
sturdy
or
in
good
condition,
especially
during
the
medieval
period
of
Europe.
Many
transmutation
attempts
failed
when
alchemists
unwittingly
made
unstable
chemicals.
This
was
made
worse
by
the
unsafe
conditions
in
which
the
alchemists
worked.
Up
to
the
16th
century,
alchemy
was
considered
serious
science
in
Europe;
for
instance,
Isaac
Newton
devoted
considerably
more
of
his
writing
to
the
study
of
alchemy
(see
Isaac
Newton's
occult
studies)
than
he
did
to
either
optics
or
physics,
for
which
he
is
famous.
Other
eminent
alchemists
of
the
Western
world
are
Roger
Bacon,
Saint
Thomas
Aquinas,
Tycho
Brahe,
Thomas
Browne,
and
Parmigianino.
The
decline
of
alchemy
began
in
the
18th
century
with
the
birth
of
modern
chemistry,
which
provided
a
more
precise
and
reliable
framework
for
matter
transmutations
and
medicine,
within
a
new
grand
design
of
the
universe
based
on
rational
materialism.
Alchemy
in
traditional
medicine.
Traditional
medicines
involve
transmutation
by
alchemy,
using
pharmacological
or
a
combination
of
pharmacological
and
spiritual
techniques.
In
Chinese
medicine
the
alchemical
traditions
of
pao
zhi
will
transform
the
nature
of
the
temperature,
taste,
body
part
accessed
or
toxicity.
In
Ayurveda
the
samskaras
are
used
to
transform
heavy
metals
and
toxic
herbs
in
a
way
that
removes
their
toxicity.
These
processes
are
actively
used
to
the
present
day.
Nuclear
transmutation.
In
1919,
Ernest
Rutherford
used
artificial
disintegration
to
convert
nitrogen
into
oxygen.
From
then
on,
this
sort
of
"scientific
transmutation"
has
been
routinely
performed
in
many
nuclear
physics-related
laboratories
and
facilities,
like
particle
accelerators,
nuclear
power
stations
and
nuclear
weapons
as
a
by-product
of
fission
and
other
physical
processes.
In
literature.
A
play
by
Ben
Jonson,
The
Alchemist,
is
a
satirical
and
skeptical
take
on
the
subject.
Part
2
of
Goethe's
Faust,
is
full
of
alchemical
symbolism.
According
to
"Hermetic
Fictions:
Alchemy
and
Irony
in
the
Novel"
(Keele
University
Press,
1995),
by
David
Meakin,
alchemy
is
also
featured
in
such
novels
and
poems
as
those
by
William
Godwin,
Percy
Bysshe
Shelley,
Emile
Zola,
Jules
Verne,
Marcel
Proust,
Thomas
Mann,
Hermann
Hesse,
James
Joyce,
Gustav
Meyrink,
Lindsay
Clarke,
Marguerite
Yourcenar,
Umberto
Eco,
Michel
Butor,
Paulo
Coelho,
Amanda
Quick,
Gabriel
García
Marquez
and
Maria
Szepes.
Hilary
Mantel,
in
her
novel
Fludd
(1989,
Penguin),
mentions
the
spagyric
art.
'After
separation,
drying
out,
moistening,
dissolving,
coagulating,
fermenting,
comes
purification,
recombination:
the
creation
of
substances
the
world
until
now
has
never
beheld.
This
is
the
opus
contra
naturem,
this
is
the
spagyric
art,
this
is
the
Alchymical
Wedding'.
(page
79)
In
Dante's
Inferno,
it
is
placed
within
the
Tenth
ring
of
the
8th
circle.
In
Angie
Sage's
Septimus
Heap
series,
Marcellus
Pye
is
an
important
Alchemist
that
first
appears
in
Physik,
the
third
book.
In
The
Secrets
of
the
Immortal
Nicholas
Flamel
series,
one
of
the
main
characters
is
a
alchemist.
The
manga
and
anime
series
Fullmetal
Alchemist
bases
itself
of
a
more
fantasised
version
of
alchemy.
In
contemporary
art.
In
the
twentieth
century
alchemy
was
a
profoundly
important
source
of
inspiration
for
the
Surrealist
artist
Max
Ernst,
who
used
the
symbolism
of
alchemy
to
inform
and
guide
his
work.
M.E.
Warlick
wrote
his
"Max
Ernst
and
Alchemy"
describing
this
relationship
in
detail.
Contemporary
artists
use
alchemy
as
inspiring
subject
matter,
like
Odd
Nerdrum,
whose
interest
has
been
noted
by
Richard
Vine,
and
the
painter
Michael
Pearce,
whose
interest
in
alchemy
dominates
his
work.
His
works
"Fama"
and
"The
Aviator's
Dream"
particularly
express
alchemical
ideas
in
a
painted
allegory.
---END.OF.DOCUMENT---
Austria.
Austria
(),
officially
the
Republic
of
Austria
(German:;
Austro-Bavarian:
Repubblik
Östareich),
is
a
landlocked
country
of
roughly
8.3
million
people
in
Central
Europe.
It
borders
Germany
and
the
Czech
Republic
to
the
north,
Slovakia
and
Hungary
to
the
east,
Slovenia
and
Italy
to
the
south,
and
Switzerland
and
Liechtenstein
to
the
west.
The
territory
of
Austria
covers,
and
has
a
temperate
and
alpine
climate.
Austria's
terrain
is
highly
mountainous
due
to
the
presence
of
the
Alps;
only
32%
of
the
country
is
below,
and
its
highest
point
is.
The
majority
of
the
population
speaks
German,
which
is
also
the
country's
official
language.
Other
local
official
languages
are
Croatian,
Hungarian
and
Slovene.
The
origins
of
Austria
date
back
to
the
time
of
the
Roman
Empire
when
a
Celtic
kingdom
was
conquered
by
the
Romans
in
approximately
15
BC,
and
later
became
Noricum,
a
Roman
province,
in
the
mid
1st
century
AD—an
area
which
mostly
encloses
today's
Austria.
In
788
AD,
the
Frankish
king
Charlemagne
conquered
the
area,
and
introduced
Christianity.
Under
the
native
Habsburg
dynasty,
Austria
became
one
of
the
great
powers
of
Europe.
In
1867,
the
Austrian
Empire
was
reformed
into
Austria-Hungary.
The
Austro-Hungarian
Empire
collapsed
in
1918
with
the
end
of
World
War
I.
After
establishing
the
First
Austrian
Republic
in
1919
Austria
was
de
facto
annexed
into
Greater
Germany
by
the
Nazi
regime
in
the
so-called
Anschluss
in
1938.
This
lasted
until
the
end
of
World
War
II
in
1945,
after
which
Austria
was
occupied
by
the
Allies.
In
1955,
the
Austrian
State
Treaty
re-established
Austria
as
a
sovereign
state,
ending
the
occupation.
In
the
same
year,
the
Austrian
Parliament
created
the
Declaration
of
Neutrality
which
declared
that
the
country
would
become
permanently
neutral.
Today,
Austria
is
a
parliamentary
representative
democracy
comprising
nine
federal
states.
The
capital—and
with
a
population
exceeding
1.6
million,
Austria's
largest
city—is
Vienna.
Austria
is
one
of
the
richest
countries
in
the
world,
with
a
nominal
per
capita
GDP
of
$43,570.
The
country
has
developed
a
high
standard
of
living,
and
in
2008
was
ranked
14th
in
the
world
for
its
Human
Development
Index.
Austria
has
been
a
member
of
the
United
Nations
since
1955,
joined
the
European
Union
in
1995,
and
is
a
founder
of
the
OECD.
Austria
also
signed
the
Schengen
Agreement
in
1995,
and
adopted
the
European
currency,
the
euro,
in
1999.
Etymology.
The
German
name
of
Austria,
derives
from
the
Old
High
German
word
Ostarrîchi
"eastern
realm",
first
attested
in
the
famous
"Ostarrîchi
document"
of
AD
996,
where
the
term
refers
to
the
Margraviate
ruled
by
the
Babenberg
Count
Henry
I
located
mostly
in
what
is
today
Lower
Austria
and
part
of
Upper
Austria.
The
name
Austria
is
a
latinisation
of
the
same
Germanic
word
for
"east",
*austrō
also
found
in
"Austrasia",
the
eastern
part
of
Merovingian
Francia.
German
"Österreich"
is
readily
analysable
as
connected
to
"östlich"
"eastern"
and
"Reich"
"realm,
dominion,
empire".
The
term
probably
originates
in
a
vernacular
translation
of
the
Medieval
Latin
name
for
the
region:,
which
translates
as
"eastern
marches"
or
"eastern
borderland",
as
it
was
situated
at
the
eastern
edge
of
the
Holy
Roman
Empire.==
However,
Friedrich
Heer,
one
of
the
most
important
Austrian
historians
in
the
20th
century,
stated
in
his
book
"Der
Kampf
um
die
österreichische
Identität"
("The
Struggle
Over
Austrian
Identity"),
that
the
Germanic
form
"Ostarrîchi"
was
not
a
translation
of
the
Latin
word,
but
both
resulted
from
a
much
older
term
originating
in
the
Celtic
languages
of
ancient
Austria:
More
than
2,500
years
ago,
the
major
part
of
the
actual
country
was
called
"Norig"
by
the
Celtic
population
(Hallstatt
culture);
"No-"
or
"Nor-"
meant
"east"
or
"eastern",
whereas
"-rig"
is
related
to
the
modern
German
"Reich";
meaning
"realm".
Accordingly,
"Norig"
would
essentially
mean
"Ostarrîchi"
and
"Österreich",
thus
"Austria".
The
Celtic
name
was
eventually
Latinised
to
"Noricum"
after
the
Romans
conquered
the
area
that
encloses
most
of
modern
day
Austria,
in
approximately
15
BC.
"Noricum"
later
became
a
Roman
province
in
the
mid
1st
century
AD.
History.
Settled
in
ancient
times,
the
Central
European
land
that
is
now
Austria
was
occupied
in
pre-Roman
times
by
various
Celtic
tribes.
The
Celtic
kingdom
of
Noricum
was
later
claimed
by
the
Roman
Empire
and
made
a
province.
Present
day
Petronell-Carnuntum
in
Eastern
Austria
was
an
important
army
camp
turned
capital
city
in
what
became
known
as
the
Upper
Pannonia
province.
Fifty
thousand
people
called
Carnuntum
home
for
nearly
400
years.
After
the
fall
of
the
Roman
Empire
the
area
was
invaded
by
Bavarians,
Slavs
and
Avars.
The
Slavic
tribe
of
the
Carantanians
migrated
into
the
Alps,
and
established
the
realm
of
Carantania,
which
covered
much
of
eastern
and
central
Austrian
territory.
Charlemagne
conquered
the
area
in
788
AD,
encouraged
colonisation
and
introduced
Christianity.
As
part
of
Eastern
Francia,
the
core
areas
that
now
encompass
Austria
were
bequeathed
to
the
house
of
Babenberg.
The
area
was
known
as
the
"marchia
Orientalis"
and
was
given
to
Leopold
of
Babenberg
in
976.
The
first
record
showing
the
name
Austria
is
from
996
where
it
is
written
as
"Ostarrîchi",
referring
to
the
territory
of
the
Babenberg
March.
In
1156
the
Privilegium
Minus
elevated
Austria
to
the
status
of
a
duchy.
In
1192,
the
Babenbergs
also
acquired
the
Duchy
of
Styria.
With
the
death
of
Frederick
II
in
1246,
the
line
of
the
Babenbergs
went
extinct.
As
a
result
Otakar
II
of
Bohemia
effectively
assumed
control
of
the
duchies
of
Austria,
Styria
and
Carinthia.
His
reign
came
to
an
end
with
his
defeat
at
Dürnkrut
at
the
hands
of
Rudolf
I
of
Germany
in
1278.
Thereafter,
until
World
War
I,
Austria's
history
was
largely
that
of
its
ruling
dynasty,
the
Habsburgs.
In
the
14th
and
15th
centuries,
the
Habsburgs
began
to
accumulate
other
provinces
in
the
vicinity
of
the
Duchy
of
Austria.
In
1438
Duke
Albert
V
of
Austria
was
chosen
as
the
successor
to
his
father-in-law,
Emperor
Sigismund.
Although
Albert
himself
only
reigned
for
a
year,
every
emperor
of
the
Holy
Roman
Empire
was
a
Habsburg,
with
only
one
exception.
The
Habsburgs
began
also
to
accumulate
lands
far
from
the
hereditary
lands.
In
1477
Archduke
Maximilian,
only
son
of
Emperor
Frederick
III,
married
the
heiress
Maria
of
Burgundy,
thus
acquiring
most
of
the
Netherlands
for
the
family.
His
son
Philip
the
Fair
married
the
heiress
of
Castile
and
Aragon,
and
thus
acquired
Spain
and
its
Italian,
African
and
New
World
appendages
for
the
Habsburgs.
In
1526
following
the
Battle
of
Mohács,
Bohemia
and
the
part
of
Hungary
not
occupied
by
the
Ottomans
came
under
Austrian
rule.
Ottoman
expansion
into
Hungary
led
to
frequent
conflicts
between
the
two
empires,
particularly
evident
in
the
so-called
Long
War
of
1593
to
1606.
During
the
long
reign
of
Leopold
I
(1657–1705)
and
following
the
successful
defense
of
Vienna
in
1683
(under
the
command
of
the
King
of
Poland,
John
III
Sobieski),
a
series
of
campaigns
resulted
in
bringing
all
of
Hungary
to
Austrian
control
by
the
Treaty
of
Carlowitz
in
1699.
Emperor
Charles
VI
relinquished
many
of
the
fairly
impressive
gains
the
empire
made
in
the
previous
years,
largely
due
to
his
apprehensions
at
the
imminent
extinction
of
the
House
of
Habsburg.
Charles
was
willing
to
offer
concrete
advantages
in
territory
and
authority
in
exchange
for
other
powers'
worthless
recognitions
of
the
Pragmatic
Sanction
that
made
his
daughter
Maria
Theresa
his
heir.
With
the
rise
of
Prussia
the
Austrian–Prussian
dualism
began
in
Germany.
Austria
participated,
together
with
Prussia
and
Russia,
in
the
first
and
the
third
of
the
three
Partitions
of
Poland
(in
1772
and
1795).
Austria
later
became
engaged
in
a
war
with
Revolutionary
France,
at
the
beginning
highly
unsuccessful,
with
successive
defeats
at
the
hands
of
Napoleon
meaning
the
end
of
the
old
Holy
Roman
Empire
in
1806.
Two
years
earlier,
in
1804,
the
Empire
of
Austria
was
founded.
In
1814
Austria
was
part
of
the
Allied
forces
that
invaded
France
and
brought
to
an
end
the
Napoleonic
wars.
It
thus
emerged
from
the
Congress
of
Vienna
in
1815
as
one
of
four
of
the
continent's
dominant
powers
and
a
recognised
great
power.
The
same
year,
the
German
Confederation,
()
was
founded
under
the
presidency
of
Austria.
Because
of
unsolved
social,
political
and
national
conflicts
the
German
lands
were
shaken
by
the
1848
revolution
aiming
to
create
a
unified
Germany.
A
unified
Germany
would
have
been
possible
either
as
a
Greater
Germany,
or
a
Greater
Austria
or
just
the
German
Confederation
without
Austria
at
all.
As
Austria
was
not
willing
to
relinquish
its
German-speaking
territories
to
what
would
become
the
German
Empire
of
1848,
the
crown
of
the
newly-formed
empire
was
offered
to
the
Prussian
King
Friedrich
Wilhelm
IV.
In
1864
Austria
and
Prussia
fought
together
against
Denmark,
and
successfully
freed
the
independent
duchies
of
Schleswig
and
Holstein.
Nevertheless
as
they
could
not
agree
on
a
solution
to
the
administration
of
the
two
duchies,
they
fought
in
1866
the
Austro-Prussian
War.
Defeated
by
Prussia
in
the
Battle
of
Königgrätz,
Austria
had
to
leave
the
German
Confederation
and
subsequently
no
longer
took
part
in
German
politics.
The
Austro-Hungarian
Compromise
of
1867,
the
"Ausgleich",
provided
for
a
dual
sovereignty,
the
Austrian
Empire
and
the
Kingdom
of
Hungary,
under
Franz
Joseph
I.
The
Austrian-Hungarian
rule
of
this
diverse
empire
included
various
Slavic
groups
including
Croats,
Czechs,
Poles,
Rusyns,
Serbs,
Slovaks,
Slovenes
and
Ukrainians,
as
well
as
large
Italian
and
Romanian
communities.
As
a
result,
ruling
Austria–Hungary
became
increasingly
difficult
in
an
age
of
emerging
nationalist
movements.
Yet
the
government
of
Austria
tried
its
best
to
be
accommodating
in
some
respects:
The
"Reichsgesetzblatt",
publishing
the
laws
and
ordinances
of
Cisleithania,
was
issued
in
eight
languages,
all
national
groups
were
entitled
to
schools
in
their
own
language
and
to
the
use
of
their
mothertongue
at
state
offices,
for
example.
The
government
of
Hungary
to
the
contrary
tried
to
magyarise
other
ethnic
entities.
Thus
the
wishes
of
ethnic
groups
dwelling
in
both
parts
of
the
dual
monarchy
hardly
could
be
solved.
The
assassination
of
Archduke
Franz
Ferdinand
in
Sarajevo
in
1914
by
Gavrilo
Princip
(a
member
of
the
Serbian
nationalist
group
the
Black
Hand)
was
used
by
leading
Austrian
and
Hungarian
politicians
and
generals
to
persuade
the
emperor
to
declare
war
on
Serbia,
thereby
risking
and
prompting
the
outbreak
of
World
War
I
which
led
to
the
dissolution
of
the
Austro-Hungarian
Empire.
Over
one
million
Austro-Hungarian
soldiers
died
in
World
War
I.
On
October
21,
1918,
the
elected
German
members
of
the
"Reichsrat"
(parliament
of
Imperial
Austria)
met
in
Vienna
as
the
Provisional
National
Assembly
for
German
Austria
("Provisorische
Nationalversammlung
für
Deutschösterreich").
On
October
30
the
assembly
founded
the
State
of
German
Austria
by
appointing
a
government,
called
"Staatsrat".
This
new
government
was
invited
by
the
emperor
to
take
part
in
the
decision
on
the
planned
armistice
with
Italy,
but
refrained
from
this
business;
this
left
the
responsibility
for
the
end
of
the
war
on
November
3,
1918,
solely
to
the
emperor
and
his
government.
On
November
11
the
emperor,
counseled
by
ministers
of
the
old
and
the
new
government,
declared
he
would
not
take
part
in
state
business
any
more;
on
November
12
German
Austria,
by
law,
declared
itself
to
be
a
democratic
republic
and
part
of
the
new
German
republic.
The
constitution,
renaming
"Staatsrat"
to
"Bundesregierung"
(federal
government)
and
"Nationalversammlung"
to
"Nationalrat"
(national
council)
was
passed
on
November
10,
1920.
The
Treaty
of
Saint-Germain
of
1919
(for
Hungary
the
Treaty
of
Trianon
of
1920)
confirmed
and
consolidated
the
new
order
of
Central
Europe
which
to
a
great
part
had
been
established
in
November
1918,
creating
new
states
and
resizing
others.
Over
3-million
German
Austrians
found
themselves
living
outside
of
the
newborn
Austrian
Republic
in
the
respective
states
of
Czechoslovakia,
Yugoslavia,
Hungary
and
Italy.
Between
1918
and
1919
Austria
was
officially
known
as
the
State
of
German
Austria
().
Not
only
did
the
Entente
powers
forbid
German
Austria
to
unite
with
Germany,
they
also
ignored
the
name
German
Austria
in
the
peace
treaty
to
be
signed;
it
was
therefore
changed
to
Republic
of
Austria
in
late
1919.
After
the
war
inflation
began
to
devaluate
the
"Krone",
still
Austria's
currency.
In
the
autumn
of
1922
Austria
was
granted
an
international
loan
supervised
by
the
League
of
Nations.
The
purpose
of
the
loan
was
to
avert
bankruptcy,
stabilise
the
currency
and
improve
its
general
economic
condition.
With
the
granting
of
the
loan,
Austria
passed
from
an
independent
state
to
the
control
exercised
by
the
League
of
Nations.
In
1925
the
"Schilling",
replacing
the
"Krone"
by
10,000:1,
was
introduced.
Later
it
was
called
the
Alpine
dollar
due
to
its
stability.
From
1925
to
1929
the
economy
enjoyed
a
short
high
before
nearly
crashing
after
Black
Friday.
The
First
Austrian
Republic
lasted
until
1933
when
Chancellor
Engelbert
Dollfuss,
gladly
using
what
he
called
"self-switch-off
of
Parliament"
(),
established
an
autocratic
regime
tending
toward
Italian
fascism.
The
two
big
parties
at
this
time,
the
Social
Democrats
and
the
Conservatives,
had
paramilitary
armies;
the
Social
Democrats'
"Schutzbund"
was
now
declared
illegal
but
still
operative
as
civil
war
broke
out.
In
February
1934
several
members
of
the
"Schutzbund"
were
executed,
the
Social
Democratic
party
was
outlawed
and
many
of
its
members
were
imprisoned
or
emigrated.
On
1
May
1934,
the
Austrofascists
imposed
a
new
constitution
("Maiverfassung")
which
cemented
Dollfuss's
power
but
on
25
July
he
was
assassinated
in
a
Nazi
coup
attempt.
His
successor,
Kurt
Schuschnigg,
struggled
to
keep
Austria
independent
as
"the
better
German
state",
but
on
12
March
1938,
German
troops
occupied
the
country
while
Austrian
Nazis
took
over
government.
On
13
March
1938,
the
"Anschluss"
of
Austria
was
officially
declared.
Two
days
later
Hitler,
a
native
of
Austria,
proclaimed
the
re-unification
of
his
home
country
with
the
rest
of
Germany
on
Vienna's
Heldenplatz.
He
established
a
plebiscite
confirming
union
with
Germany
in
April
1938.
Austria
was
incorporated
into
the
Third
Reich
and
ceased
to
exist
as
an
independent
state.
The
Aryanisation
of
the
wealth
of
Jewish
Austrians
started
immediately
mid-March
with
a
so
called
"wild"
(i.e.
extra-legal)
phase
but
soon
was
structured
legally
and
bureaucratically
to
strip
Jewish
citizens
of
any
asset
they
may
have
possessed.
The
Nazis
called
Austria
"Ostmark"
until
1942
when
it
was
again
renamed
and
called
"Alpen-Donau-Reichsgaue".
Vienna
fell
on
13
April
1945,
during
the
Soviet
Vienna
Offensive
just
before
the
total
collapse
of
the
Third
Reich.
Karl
Renner
and
Adolf
Schärf
(Socialist
Party
of
Austria
[Social
Democrats
and
Revolutionary
Socialists]),
Leopold
Kunschak
(Austria's
People's
Party
[former
Christian
Social
People's
Party])
and
Johann
Koplenig
(Communist
Party
of
Austria)
declared
Austria's
secession
from
the
Third
Reich
by
the
Declaration
of
Independence
on
27
April
1945,
and
set
up
a
provisional
government
in
Vienna
under
state
Chancellor
Renner
the
same
day,
with
the
approval
of
the
victorious
Red
Army
and
backed
by
Stalin.
(The
date
is
officially
named
the
birthday
of
the
second
republic.)
At
the
end
of
April,
most
of
Western
and
Southern
Austria
still
was
under
Nazi
rule.
On
May
1,
1945,
the
federal
constitution
of
1929
was
put
into
validity
again,
which
had
been
terminated
by
dictator
Dollfuss
on
May
1,
1934.
Total
military
deaths
from
1939–1945
are
estimated
at
260,000.
Jewish
Holocaust
victims
totaled
65,000.
About
140,000
Jewish
Austrians
had
fled
the
country
in
1938–39.
Thousands
of
Austrians
had
taken
part
in
serious
Nazi
crimes,
a
fact
officially
recognised
by
Chancellor
Franz
Vranitzky
in
1992.
Much
like
Germany,
Austria
was
divided
into
a
British,
a
French,
a
Soviet
and
a
U.S.
zone
and
governed
by
the
Allied
Commission
for
Austria.
As
forecast
in
the
Moscow
Declaration
in
1943,
there
was
a
subtle
difference
in
the
treatment
of
Austria
by
the
Allies.
The
Austrian
Government,
consisting
of
Social
Democrats,
Conservatives
and
Communists
(until
1947)
and
residing
in
Vienna,
which
was
surrounded
by
the
Soviet
zone,
was
recognised
by
the
Western
Allies
in
October
1945
after
some
doubts
that
Renner
could
be
Stalin's
puppet.
Thereby
the
creation
of
a
separate
Western
Austrian
government
and
the
division
of
the
country
could
be
avoided.
Austria,
in
general,
was
treated
as
though
it
had
been
originally
invaded
by
Germany
and
liberated
by
the
Allies.
On
15
May
1955,
after
talks
which
lasted
for
years
and
were
influenced
by
the
Cold
War
Austria
regained
full
independence
by
concluding
the
Austrian
State
Treaty
with
the
Four
Occupying
Powers.
On
26
October
1955,
after
all
occupation
troops
had
left,
Austria
declared
its
"permanent
neutrality"
by
an
act
of
Parliament,
which
remains
to
this
day
but
has
been
implicitly
overlapped
by
constitutional
amendments
concerning
Austria
as
member
of
the
European
Union
from
1995
onward.
The
political
system
of
the
Second
Republic
is
based
on
the
constitution
of
1920
and
1929,
which
was
reintroduced
in
1945.
The
system
came
to
be
characterised
by
"Proporz",
meaning
that
most
posts
of
political
importance
were
split
evenly
between
members
of
the
Social
Democrats
and
the
People's
Party.
Interest
group
"chambers"
with
mandatory
membership
(e.g.
for
workers,
business
people,
farmers)
grew
to
considerable
importance
and
were
usually
consulted
in
the
legislative
process,
so
that
hardly
any
legislation
was
passed
that
did
not
reflect
widespread
consensus.
Since
1945
a
single-party
government
took
place
only
1966–1970
(Conservatives)
and
1970–1983
(Social
Democrats).
During
all
other
legislative
periods,
either
a
grand
coalition
of
Conservatives
and
Social
Democrats
or
a
"small
coalition"
(one
of
these
two
and
a
smaller
party)
ruled
the
country.
Following
a
referendum
in
1994,
at
which
consent
reached
a
majority
of
two
thirds,
the
country
became
a
member
of
the
European
Union
on
1
January
1995.
According
to
its
economic
success,
Austria
is
one
of
the
"net
contributors"
of
the
union.
The
major
parties
SPÖ
and
ÖVP
have
contrary
opinions
about
the
future
status
of
Austria's
military
non-alignment:
While
the
SPÖ
in
public
supports
a
neutral
role,
the
ÖVP
argues
for
stronger
integration
into
the
EU's
security
policy;
even
a
future
NATO
membership
is
not
ruled
out
by
some
ÖVP
politicians.
In
reality,
Austria
is
taking
part
in
the
EU's
Common
Foreign
and
Security
Policy,
participates
in
the
so-called
Petersburg
Agenda
(including
peace
keeping
and
peace
creating
tasks)
and
has
become
member
of
NATO's
"Partnership
for
Peace";
the
constitution
has
been
amended
accordingly.
The
term
"neutrality"
is
only
used
to
tranquilise
voters
afraid
of
change.
Since
2008,
due
to
the
Schengen
Agreement,
the
only
neighbouring
country
performing
border
controls
towards
Austria
is
Liechtenstein.
Political
system.
The
Parliament
of
Austria
is
located
in
Vienna,
the
country's
largest
city
and
capital.
Austria
became
a
federal,
parliamentarian,
democratic
republic
through
the
Federal
Constitution
of
1920.
It
was
reintroduced
in
1945
to
the
nine
states
of
the
Federal
Republic.
The
head
of
state
is
the
Federal
President
("Bundespräsident"),
who
is
directly
elected
by
popular
vote.
The
chairman
of
the
Federal
Government
is
the
Federal
Chancellor,
who
is
appointed
by
the
president.
The
government
can
be
removed
from
office
by
either
a
presidential
decree
or
by
vote
of
no
confidence
in
the
lower
chamber
of
parliament,
the
Nationalrat.
Voting
for
the
federal
president
and
for
the
Parliament
used
to
be
compulsory
in
Austria,
but
this
was
abolished
in
steps
from
1982
to
2004.
The
Parliament
of
Austria
consists
of
two
chambers.
The
composition
of
the
Nationalrat
(183
seats)
is
determined
every
five
years
(or
whenever
the
Nationalrat
has
been
dissolved
by
the
federal
president
on
a
motion
by
the
federal
chancellor,
or
by
Nationalrat
itself)
by
a
general
election
in
which
every
citizen
over
16
years
(since
2007)
has
voting
rights.
While
there
is
a
general
threshold
of
4
percent
for
all
parties
at
federal
elections
(Nationalratswahlen),
there
remains
the
possibility
to
gain
a
direct
seat,
or,
in
one
of
the
43
regional
election
districts.
The
Nationalrat
is
the
dominant
chamber
in
the
formation
of
legislation
in
Austria.
However,
the
upper
house
of
parliament,
the
Bundesrat,
has
a
limited
right
of
veto
(the
Nationalrat
can—in
almost
all
cases—ultimately
pass
the
respective
bill
by
voting
a
second
time.
This
is
referred
to
as
Beharrungsbeschluss",
lit.
"vote
of
persistence").
A
convention,
called
the
was
convened
in
June
30,
2003
to
decide
upon
suggestions
to
reform
the
constitution,
but
failed
to
produce
a
proposal
that
would
receive
the
two-thirds
of
votes
in
the
Nationalrat
necessary
for
constitutional
amendments
and/or
reform.
With
legislative
and
executive,
the
courts
are
the
third
column
of
Austrian
state
powers.
Notably
the
Constitutional
Court
("Verfassungsgerichtshof")
may
exert
considerable
influence
on
the
political
system
by
ruling
out
laws
and
ordinances
not
in
compliance
with
the
constitution.
Since
1995,
the
European
Court
of
Justice
may
overrule
Austrian
decisions
in
all
matters
defined
in
laws
of
the
European
Union.
Concerning
human
rights,
Austria
also
is
implementing
the
decisions
of
the
European
Court
of
Human
Rights,
since
the
European
Convention
on
Human
Rights
is
part
of
the
Austrian
constitution.
Recent
developments.
After
general
elections
held
in
October
2006,
the
Social
Democrats
emerged
as
the
largest
party,
whereas
the
People's
Party
lost
about
8%
in
votes.
Political
realities
prohibited
any
of
the
two
major
parties
from
forming
a
coalition
with
smaller
parties.
In
January
2007
the
People's
Party
and
Social
Democrats
formed
a
grand
coalition
with
the
social
democrat
Alfred
Gusenbauer
as
Chancellor.
This
coalition
broke
up
in
June
2008.
Elections
in
September
2008
further
weakened
both
major
parties
(Social
Democrats
and
People's
Party)
but
together
they
still
held
more
than
50%
of
the
votes
with
the
Social
Democrats
holding
the
majority.
They
formed
a
coalition
with
Werner
Faymann
from
the
Social
Democrats
as
Chancellor.
The
positions
of
the
Freedom
Party
and
the
deceased
Jörg
Haider's
new
party
Alliance
for
the
Future
of
Austria,
both
right-wing
parties,
were
strengthened
during
the
election.
Foreign
policy.
The
1955
Austrian
State
Treaty
ended
the
occupation
of
Austria
following
World
War
II
and
recognised
Austria
as
an
independent
and
sovereign
state.
On
26
October
1955,
the
Federal
Assembly
passed
a
constitutional
article
in
which
"Austria
declares
of
her
own
free
will
her
perpetual
neutrality".
The
second
section
of
this
law
stated
that
"in
all
future
times
Austria
will
not
join
any
military
alliances
and
will
not
permit
the
establishment
of
any
foreign
military
bases
on
her
territory".
Since
then,
Austria
has
shaped
its
foreign
policy
on
the
basis
of
neutrality,
but
rather
different
from
the
neutrality
of
Switzerland.
Austria
began
to
reassess
its
definition
of
neutrality
following
the
fall
of
the
Soviet
Union,
granting
overflight
rights
for
the
UN-sanctioned
action
against
Iraq
in
1991,
and,
since
1995,
it
has
developed
participation
in
the
EU's
Common
Foreign
and
Security
Policy
(CFSP).
Also
in
1995,
it
joined
the
Partnership
for
Peace
and
subsequently
participated
in
peacekeeping
missions
in
Bosnia.
Meanwhile,
the
only
part
of
the
Constitutional
Law
on
Neutrality
of
1955
still
valid
fully
is
not
to
allow
foreign
military
bases
in
Austria.
Austria
attaches
great
importance
to
participation
in
the
Organisation
for
Economic
Co-operation
and
Development
and
other
international
economic
organisations,
and
it
has
played
an
active
role
in
the
Organization
for
Security
and
Cooperation
in
Europe
(OSCE).
Energy
politics.
In
1972,
the
country
began
construction
of
a
nuclear-powered
electricity-generation
station
at
Zwentendorf
on
the
River
Danube,
following
a
unanimous
vote
in
parliament.
However,
in
1978,
a
referendum
voted
approximately
50.5%
against
nuclear
power,
49.5%
for,
and
parliament
subsequently
unanimously
passed
a
law
forbidding
the
use
of
nuclear
power
to
generate
electricity.
Austria
currently
produces
more
than
half
of
its
electricity
by
hydropower.
Together
with
other
renewable
energy
sources
such
as
wind,
solar
and
biomass
powerplants,
the
electricity
supply
from
renewable
energy
amounts
to
62.89%
of
total
use
in
Austria,
with
the
rest
being
produced
by
gas
and
oil
powerplants.
Military.
The
manpower
of
the
Austrian
Armed
Forces
()
mainly
relies
on
conscription.
All
males
who
have
reached
the
age
of
eighteen
and
are
found
fit
have
to
serve
a
six
months
military
service,
followed
by
an
eight
year
reserve
obligation.
Both
males
and
females
at
the
age
of
sixteen
are
eligible
for
voluntary
service.
Conscientious
objection
is
legally
acceptable
and
those
who
claim
this
right
are
obliged
to
serve
an
institutionalised
nine
months
civilian
service
instead.
Since
1998,
women
volunteers
have
been
allowed
to
become
professional
soldiers.
The
main
sectors
of
the
Bundesheer
are
Joint
Forces
(Streitkräfteführungskommando,
SKFüKdo)
which
consist
of
Land
Forces
(Landstreitkräfte),
Air
Forces
(Luftstreitkräfte),
International
Missions
(Internationale
Einsätze)
and
Special
Forces
(Spezialeinsatzkräfte),
next
to
Mission
Support
(Kommando
Einsatzunterstützung;
KdoEU)
and
Command
Support
(Kommando
Führungsunterstützung;
KdoFüU).
Being
a
landlocked
country,
Austria
has
no
navy.
In
2004,
Austria's
defence
expenditures
corresponded
to
approximately
0.9%
of
its
GDP.
The
Army
currently
has
about
45,000
soldiers,
of
whom
about
half
are
conscripts.
As
head
of
state,
Austrian
President
(currently
Heinz
Fischer)
is
nominally
the
Commander-in-Chief
of
the
Bundesheer.
In
practical
reality,
however,
command
of
the
Austrian
Armed
Forces
is
almost
exclusively
exercised
by
the
Minister
of
Defense,
currently
Norbert
Darabos.
Since
the
end
of
the
Cold
War,
and
more
importantly
the
removal
of
the
former
heavily
guarded
"Iron
Curtain"
separating
Austria
and
Hungary,
the
Austrian
military
has
been
assisting
Austrian
border
guards
in
trying
to
prevent
border
crossings
by
illegal
immigrants.
This
assistance
came
to
an
end
when
Hungary
joined
the
EU
Schengen
area
in
2008,
for
all
intents
and
purposes
abolishing
"internal"
border
controls
between
treaty
states.
Some
politicians
have
called
for
a
prolongation
of
this
mission,
but
the
legality
of
this
is
heavily
disputed.
In
accordance
with
the
Austrian
constitution,
armed
forces
may
only
be
deployed
in
a
limited
number
of
cases,
mainly
to
defend
the
country
and
aid
in
cases
of
national
emergency,
such
as
in
the
wake
of
natural
disasters.
They
may
generally
not
be
used
as
auxiliary
police
forces.
Within
its
self-declared
status
of
permanent
neutrality,
Austria
has
a
long
and
proud
tradition
of
engaging
in
UN-led
peacekeeping
and
other
humanitarian
missions.
The
Austrian
Forces
Disaster
Relief
Unit
(AFDRU),
in
particular,
an
all-volunteer
unit
with
close
ties
to
civilian
specialists
(e.g.
rescue
dog
handlers)
enjoys
a
reputation
as
a
quick
(standard
deployment
time
is
10
hours)
and
efficient
SAR
unit.
Currently,
larger
contingents
of
Austrian
forces
are
deployed
in
Bosnia,
Kosovo
and,
since
1974,
in
the
Golan
Heights.
States.
As
a
federal
republic,
Austria
is
divided
into
nine
states
().
These
states
are
then
divided
into
districts
()
and
statutory
cities
().
Districts
are
subdivided
into
municipalities
().
Statutory
Cities
have
the
competencies
otherwise
granted
to
both
districts
and
municipalities.
The
states
are
not
mere
administrative
divisions
but
have
some
legislative
authority
distinct
from
the
federal
government,
e.g.
in
matters
of
culture,
social
care,
youth
and
nature
protection,
hunting,
building,
and
zoning
ordinances.
In
recent
years,
it
has
been
discussed
whether
today
it
is
appropriate
for
a
small
country
to
maintain
ten
parliaments.
Geography.
Austria
is
a
largely
mountainous
country
due
to
its
location
in
the
Alps.
The
Central
Eastern
Alps,
Northern
Limestone
Alps
and
Southern
Limestone
Alps
are
all
partly
in
Austria.
Of
the
total
area
of
Austria
(),
only
about
a
quarter
can
be
considered
low
lying,
and
only
32%
of
the
country
is
below.
The
Alps
of
western
Austria
give
way
somewhat
into
low
lands
and
plains
in
the
eastern
part
of
the
country.
Austria
can
be
divided
into
five
areas,
the
biggest
being
the
Eastern
Alps,
which
constitute
62%
of
nation's
total
area.
The
Austrian
foothills
at
the
base
of
the
Alps
and
the
Carpathians
account
for
around
12%
and
the
foothills
in
the
east
and
areas
surrounding
the
periphery
of
the
Pannoni
low
country
amount
to
about
12%
of
the
total
landmass.
The
second
greater
mountain
area
(much
lower
than
the
Alps)
is
situated
in
the
north.
Known
as
the
Austrian
granite
plateau,
it
is
located
in
the
central
area
of
the
Bohemian
Mass,
and
accounts
for
10%
of
Austria.
The
Austrian
portion
of
the
Vienna
basin
comprises
the
remaining
4%.
Phytogeographically,
Austria
belongs
to
the
Central
European
province
of
the
Circumboreal
Region
within
the
Boreal
Kingdom.
According
to
the
WWF,
the
territory
of
Austria
can
be
subdivided
into
four
ecoregions:
the
Central
European
mixed
forests,
Pannonian
mixed
forests,
Alps
conifer
and
mixed
forests
and
Western
European
broadleaf
forests.
Climate.
The
greater
part
of
Austria
lies
in
the
cool/temperate
climate
zone
in
which
humid
westerly
winds
predominate.
With
over
half
of
the
country
dominated
by
the
Alps,
the
alpine
climate
is
the
predominant
one.
In
the
east—in
the
Pannonian
Plain
and
along
the
Danube
valley—the
climate
shows
continental
features
with
less
rain
than
the
alpine
areas.
Although
Austria
is
cold
in
the
winter,
summer
temperatures
can
be
relatively
warm—reaching
temperatures
of
around
20
–
40
°C.
Economy.
Austria
is
one
of
the
12
richest
countries
in
the
world
in
terms
of
GDP
(Gross
domestic
product)
per
capita,
has
a
well-developed
social
market
economy,
and
a
high
standard
of
living.
Until
the
1980s,
many
of
Austria's
largest
industry
firms
were
nationalised;
in
recent
years,
however,
privatisation
has
reduced
state
holdings
to
a
level
comparable
to
other
European
economies.
Labour
movements
are
particularly
strong
in
Austria
and
have
large
influence
on
labour
politics.
Next
to
a
highly
developed
industry,
international
tourism
is
the
most
important
part
of
the
national
economy.
Germany
has
historically
been
the
main
trading
partner
of
Austria,
making
it
vulnerable
to
rapid
changes
in
the
German
economy.
However,
since
Austria
became
a
member
state
of
the
European
Union
it
has
gained
closer
ties
to
other
European
Union
economies,
reducing
its
economic
dependence
on
Germany.
In
addition,
membership
in
the
EU
has
drawn
an
influx
of
foreign
investors
attracted
by
Austria's
access
to
the
single
European
market
and
proximity
to
the
aspiring
economies
of
the
European
Union.
Growth
in
GDP
accelerated
in
recent
years
and
reached
3.3%
in
2006.
Currency.
In
Austria,
the
euro
was
introduced
as
an
accounting
currency
on
1
January
1999,
and
euro
coins
and
banknotes
entered
circulation
on
1
January
2002.
As
a
preparation
for
this
date,
the
minting
of
the
new
euro
coins
started
as
early
as
1999,
however
all
Austrian
euro
coins
introduced
in
2002
have
this
year
on
it;
unlike
other
countries
of
the
Eurozone
where
mint
year
is
minted
in
the
coin.
Eight
different
designs,
one
per
face
value,
were
selected
for
the
Austrian
coins.
In
2007,
to
adopt
the
new
common
map
like
the
rest
of
the
Eurozone
countries,
Austria
changed
the
common
side
of
its
coins.
Before
adopting
the
Euro
in
2002
Austria
had
maintained
use
of
the
Austrian
schilling
which
was
first
established
in
December
1924.
The
Schilling
was
abolished
in
the
wake
of
the
Anschluss
in
1938
and
has
been
reintroduced
after
the
end
of
the
World
War
II
in
November
1945.
Austria
has
one
of
the
richest
collection
of
collectors'
coins
in
the
Eurozone,
with
face
value
ranging
from
10
to
100
euro
(although
a
100,000
euro
coin
was
exceptionally
minted
in
2004).
These
coins
are
a
legacy
of
an
old
national
practice
of
minting
of
silver
and
gold
coins.
Unlike
normal
issues,
these
coins
are
not
legal
tender
in
all
the
eurozone.
For
instance,
a
€5
Austrian
commemorative
coin
cannot
be
used
in
any
other
country.
Education.
Responsibility
for
educational
oversight
in
Austria
is
entrusted
partly
to
the
Austrian
states
(Bundesländer),
and
partly
to
the
federal
government.
School
attendance
is
compulsory
for
nine
years,
i.e.
usually
to
the
age
of
fifteen.
Kindergarten
education,
free
in
most
states,
is
provided
for
all
children
between
the
ages
of
three
and
six
years
and,
whilst
optional,
is
considered
a
normal
part
of
a
child's
education,
due
to
its
high
takeup
rate.
Maximum
class
size
is
around
30,
each
class
normally
being
cared
for
by
one
qualified
teacher
and
one
assistant.
Standard
attendance
times
are
8am
to
12am,
with
extra
afternoon
care
also
frequently
provided
for
a
fee.
Primary
education,
or
Volksschule,
lasts
for
four
years,
starting
at
age
six.
Maximum
class
size
is
30,
but
may
be
as
low
as
15.
It
is
generally
expected
that
a
class
will
be
taught
by
one
teacher
for
the
entire
four
years
and
the
stable
bond
between
teacher
and
pupil
is
considered
important
for
a
child's
well-being.
The
"3Rs"
dominate
lesson
time,
with
less
time
allotted
to
project
work
than
in
the
UK.
Children
work
individually
and
all
members
of
a
class
follow
the
same
plan
of
work.
There
is
no
streaming.
Lessons
begin
at
8am
and
last
until
noon
or
1pm
with
hourly
five-
or
ten-minute
breaks.
Children
are
given
homework
daily
from
the
first
year.
Historically
there
has
been
no
lunch
hour,
children
returning
home
to
eat.
However,
due
to
a
rise
in
the
number
of
mothers
in
work,
primary
schools
are
increasingly
offering
pre-lesson
and
afternoon
care.
As
in
Germany,
secondary
education
consists
of
two
main
types
of
schools,
attendance
at
which
is
based
on
a
pupil's
ability
as
determined
by
grades
from
the
primary
school.
The
Gymnasium
caters
for
the
more
able
children,
in
the
final
year
of
which
the
Matura
examination
is
taken,
which
is
a
requirement
for
access
to
university.
The
Hauptschule
prepares
pupils
for
vocational
education
but
also
for
various
types
of
further
education
(HTL
=
institution
of
higher
technical
education;
HAK
=
commercial
academy;
HBLA
=
institution
of
higher
education
for
economic
business;
etc.).
Attendance
at
one
of
these
further
education
institutes
also
leads
to
the
Matura.
Some
schools
aim
to
combine
the
education
available
at
the
Gymnasium
and
the
Hauptschule,
and
are
known
as
Gesamtschulen.
In
addition,
a
recognition
of
the
importance
of
learning
English
has
led
some
Gymnasiums
to
offer
a
bilingual
stream,
in
which
pupils
deemed
able
in
languages
follow
a
modified
curriculum,
a
portion
of
the
lesson
time
being
conducted
in
English.
As
at
primary
school,
lessons
at
Gymnasium
begin
at
8am,
and
continue
with
short
intervals
until
lunchtime
or
early
afternoon,
with
children
returning
home
to
a
late
lunch.
Older
pupils
often
attend
further
lessons
after
a
break
for
lunch,
generally
eaten
at
school.
As
at
primary
level,
all
pupils
follow
the
same
plan
of
work.
Great
emphasis
is
placed
on
homework
and
frequent
testing.
Satisfactory
marks
in
the
end-of-the-year
report
("Zeugnis")
are
a
prerequisite
for
moving
up
("aufsteigen")
to
the
next
class.
Pupils
who
do
not
meet
the
required
standard
re-sit
their
tests
at
the
end
of
the
summer
holidays;
those
whose
marks
are
still
not
satisfactory
are
required
to
re-sit
the
year
("sitzenbleiben").
It
is
not
uncommon
for
a
pupil
to
re-sit
more
than
one
year
of
school.
After
completing
the
first
two
years,
pupils
choose
between
one
of
two
strands,
known
as
"Gymnasium"
(slightly
more
emphasis
on
arts)
or
"Realgymnasium"
(slightly
more
emphasis
on
science).
Whilst
many
schools
offer
both
strands,
some
do
not,
and
as
a
result,
some
children
move
schools
for
a
second
time
at
age
12.
At
age
14,
pupils
may
choose
to
remain
in
one
of
these
two
strands,
or
to
change
to
a
vocational
course,
possibly
with
a
further
change
of
school.
The
Austrian
university
system
had
been
open
to
any
student
who
passed
the
Matura
examination
until
recently.
A
2006
bill
allowed
the
introduction
of
entrance
exams
for
studies
such
as
Medicine.
In
2001,
an
obligatory
tuition
fee
("Studienbeitrag")
of
€363.36
per
term
was
introduced
for
all
public
universities.
Since
2008,
for
all
EU
students
the
studies
are
free
of
charge,
as
long
as
a
certain
time-limit
is
not
exceeded
(the
expected
duration
of
the
study
plus
usually
two
terms
tolerance).
When
the
time-limit
is
exceeded,
the
fee
of
around
€363.36
per
term
is
charged.
Some
further
exceptions
to
the
fee
apply,
e.g.
for
students
with
a
year's
salary
of
more
than
about
€5000.
In
all
cases,
an
obligatory
fee
of
€15.50
for
the
student
union
and
insurance
is
charged.
Demographics.
Austria's
population
estimate
in
January
2009
was
8,356,707.
The
population
of
the
capital,
Vienna,
exceeds
1.6
million
(2.2
million
including
the
suburbs),
representing
about
a
quarter
of
the
country's
population.
It
is
known
for
its
vast
cultural
offerings
and
high
standard
of
living.
Vienna
is
by
far
the
country's
largest
city.
Graz
is
second
in
size,
with
250,099
inhabitants,
followed
by
Linz
(188,968),
Salzburg
(150,000),
and
Innsbruck
(117,346).
All
other
cities
have
fewer
than
100,000
inhabitants.
Language.
German,
Austria's
official
language,
is
spoken
natively
by
88.6%
of
the
population—followed
by
Turkish
(2.3%),
Serbian
(2.2%),
Croatian
(1.6%),
Hungarian
(0.5%),
and
Bosnian
(0.4%).
The
Austrian
federal
states
of
Carinthia
and
Styria
are
home
to
a
significant
indigenous
Slovene-speaking
minority
with
around
14,000
members
(Austrian
census;
unofficial
numbers
of
Slovene
groups
speak
of
up
to
50,000).
In
the
eastermost
state,
Burgenland
(formerly
part
of
the
Hungarian
portion
of
Austria–Hungary),
about
20,000
Austrian
citizens
speak
Hungarian
and
30,000
speak
Croatian.
Of
the
remaining
number
of
Austria's
people
that
are
of
non-Austrian
descent,
many
come
from
surrounding
countries,
especially
from
the
former
East
Bloc
nations.
So-called
guest
workers
"(Gastarbeiter)"
and
their
descendants,
as
well
as
refugees
from
the
Yugoslav
wars
and
other
conflicts,
also
form
an
important
minority
group
in
Austria.
Since
1994
the
Roma–Sinti
(gypsies)
are
an
officially
recognised
ethnic
minority
in
Austria.
According
to
census
information
published
by
Statistik
Austria
for
2001
there
were
a
total
of
710,926
foreign
nationals
living
in
Austria.
Of
these,
124,392
speak
German
as
their
mother
tongue
(mainly
immigrants
from
Germany,
some
from
Switzerland
and
South
Tyrol,
Italy)
The
next
largest
populations
of
linguistic
and
ethnic
groups
are
240,863
foreign
nationals
from
the
former
Yugoslavia
(Serbs
being
the
largest
number
of
these
at
135,376,
followed
by
Croatian
at
105,487);
123,417
Turkish
nationals;
25,155
whose
native
tongue
is
English;
24,446
Albanian;
17,899
Polish;
14,699
Hungarian;
12,216
Romanian;
7,982
Arabs;
6,902
Slovenes
(not
including
the
autochthonous
minority);
6,891
Slovaks;
6,707
Czech;
5,916
Persian;
5,677
Italian;
5,466
Russian;
5,213
French;
4,938
Chinese;
4,264
Spanish;
3,503
Bulgarian.
The
populations
of
the
rest
fall
off
sharply
below
3,000.
Between
200,000
and
300,000
ethnic
Turks
(including
minority
of
Turkish
Kurds)
currently
live
in
Austria.
They
are
the
largest
single
immigrant
group
in
Austria,
closely
followed
by
the
Serbs.
Austria's
mountainous
terrain
led
to
the
development
of
many
distinct
German
dialects.
All
of
the
dialects
in
the
country,
however,
belong
to
Austro-Bavarian
groups
of
German
dialects,
with
the
exception
of
the
dialect
spoken
in
its
western-most
Bundesland,
Vorarlberg,
which
belongs
to
the
group
of
Alemannic
dialects.
There
is
also
a
distinct
grammatical
standard
for
Austrian
German
with
a
few
differences
to
the
German
spoken
in
Germany.
As
of
2006,
some
of
the
Austrian
states
introduced
standardised
tests
for
new
citizens,
to
assure
their
language
ability,
cultural
knowledge
and
accordingly
their
ability
to
integrate
into
the
Austrian
society.
For
the
national
rules,
see
Austrian
nationality
law
–
Naturalisation.
Ethnic
groups
().
An
estimated
13,000
to
40,000
Slovenes
in
the
Austrian
state
of
Carinthia
(the
Carinthian
Slovenes)
as
well
as
Croats
(around
30,000)
and
Hungarians
in
Burgenland
were
recognised
as
a
minority
and
have
enjoyed
special
rights
following
the
Austrian
State
Treaty
()
of
1955.
The
Slovenes
in
the
Austrian
state
of
Styria
(estimated
at
a
number
between
1,600
and
5,000)
are
not
recognised
as
a
minority
and
do
not
enjoy
special
rights,
although
the
State
Treaty
of
July
27,
1955
states
otherwise.
The
right
for
bilingual
topographic
signs
for
the
regions
where
Slovene-
and
Croat-Austrians
live
alongside
the
German
speaking
population
(as
required
by
the
1955
State
Treaty)
is
still
to
be
fully
implemented.
Many
Carinthians
are
afraid
of
Slovenian
territorial
claims,
pointing
to
the
fact
that
Yugoslav
troops
entered
the
state
after
each
of
the
two
World
Wars
and
considering
that
some
official
Slovenian
atlases
show
parts
of
Carinthia
as
Slovene
cultural
territory.
The
recently
deceased
governor,
Jörg
Haider,
has
made
this
fact
a
matter
of
public
argument
in
autumn
2005
by
refusing
to
increase
the
number
of
bilingual
topographic
signs
in
Carinthia.
A
poll
by
the
Kärntner
Humaninstitut
conducted
in
January
2006
states
that
65%
of
Carinthians
are
not
in
favour
of
an
increase
of
bilingual
topographic
signs,
since
the
original
requirements
set
by
the
State
Treaty
of
1955
have
already
been
fulfilled
according
to
their
point
of
view.
Another
interesting
phenomenon
is
the
so
called
"Windischen-Theorie"
stating
that
the
Slovenes
can
be
split
in
two
groups:
actual
Slovenes
and
"Windische"
(a
traditional
German
name
for
Slavs),
based
on
differences
in
language
between
Austrian
Slovenes,
who
were
taught
Slovene
standard
language
in
school
and
those
Slovenes
who
spoke
their
local
Slovene
dialect
but
went
to
German
schools.
The
term
"Windische"
was
applied
to
the
latter
group
as
a
means
of
distinction.
This
politically
influenced
theory,
dividing
Slovene
Austrians
into
the
"loyal
Windische"
and
the
"national
Slovenes",
was
never
generally
accepted
and
fell
out
of
use
some
decades
ago.
Religion.
At
the
end
of
the
twentieth
century,
about
74%
of
Austria's
population
were
registered
as
Roman
Catholic,
while
about
5%
considered
themselves
Protestants.
Austrian
Christians
are
obliged
to
pay
a
mandatory
membership
fee
(calculated
by
income—about
1%)
to
their
church;
this
payment
is
called
"Kirchenbeitrag"
("Ecclesiastical/Church
contribution").
Since
the
second
half
of
the
20th
century,
the
number
of
adherents
and
churchgoers
has
dropped.
Data
for
the
end
of
2005
from
the
Austrian
Roman
Catholic
church
lists
5,662,782
members
or
68.5%
of
the
total
Austrian
population,
and
a
Sunday
church
attendance
of
753,701
or
9%
of
the
total
Austrian
population.
Data
for
the
end
of
2008
published
by
the
Austrian
Roman
Catholic
church
shows
a
further
reduction
to
5,579,493
members
or
66.8%
of
the
total
Austrian
population,
and
a
Sunday
church
attendance
of
698,527
or
8%
of
the
total
Austrian
population.
The
Lutheran
church
also
recorded
a
large
drop
in
adherents
between
2001
and
2008.
About
12%
of
the
population
declared
that
they
have
no
religion.
in
2001.
Of
the
remaining
people,
around
340,000
are
registered
as
members
of
various
Muslim
communities,
mainly
due
to
the
influx
from
Turkey,
Bosnia-Herzegovina
and
Albania.
About
180,000
are
members
of
Eastern
Orthodox
Churches,
more
than
20,000
are
active
Jehovah's
Witnesses
and
about
8,100
are
Jewish.
The
Austrian
Jewish
Community
of
1938—Vienna
alone
counted
more
than
200,000—was
reduced
to
around
4,500
during
the
Second
World
War,
with
approximately
65,000
Jewish
Austrians
killed
in
the
Holocaust
and
130,000
emigrating.
The
large
majority
of
the
current
Jewish
population
are
post-war
immigrants,
particularly
from
eastern
Europe
and
central
Asia
(including
Bukharan
Jews).
Buddhism
was
legally
recognised
as
a
religion
in
Austria
in
1983.
According
to
the
most
recent
Eurobarometer
Poll
2005,
While
northern
and
central
Germany
was
the
origin
of
the
Reformation,
Austria
and
Bavaria
were
the
heart
of
the
Counter-Reformation
in
the
sixteenth
and
seventeenth
centuries,
when
the
absolute
monarchy
of
Habsburg
imposed
a
strict
regime
to
restore
Catholicism's
power
and
influence
among
Austrians.
The
Habsburgs
for
a
long
time
viewed
themselves
as
the
vanguard
of
Catholicism
and
all
other
confessions
and
religions
were
repressed.
In
1781,
in
the
era
of
Austrian
enlightenment,
Emperor
Joseph
II
issued
a
Patent
of
Tolerance
for
Austria
that
allowed
other
confessions
a
limited
freedom
of
worship.
Religious
freedom
was
declared
a
constitutional
right
in
Cisleithania
after
the
Austro-Hungarian
"Ausgleich"
in
1867
thus
paying
tribute
to
the
fact
that
the
monarchy
was
home
of
numerous
religions
beside
Roman
Catholicism
such
as
Greek,
Serbian,
Romanian,
Russian,
and
Bulgarian
Orthodox
Christians
(Austria
neighboured
the
Ottoman
Empire
for
centuries),
Calvinist,
Lutheran
Protestants
and
Jews.
In
1912,
after
the
annexation
of
Bosnia
Hercegovina
in
1908,
Islam
was
officially
recognised
in
Austria.
Austria
remained
largely
influenced
by
Catholicism.
After
1918,
First
Republic
Catholic
leaders
such
as
Theodor
Innitzer
and
Ignaz
Seipel
took
leading
positions
within
or
close
to
Austria's
government
and
increased
their
influence
during
the
time
of
the
Austrofascism;
Catholicism
was
treated
much
like
a
state
religion
by
Engelbert
Dollfuss
and
Kurt
Schuschnigg.
Although
Catholic
(and
Protestant)
leaders
initially
welcomed
the
Germans
in
1938
during
the
Anschluss
of
Austria
into
Germany,
Austrian
Catholicism
stopped
its
support
of
Nazism
later
on
and
many
former
religious
public
figures
became
involved
with
the
resistance
during
the
Third
Reich.
After
the
end
of
World
War
II
in
1945,
a
stricter
secularism
was
imposed
in
Austria,
and
religious
influence
on
politics
declined.
Music.
Austria's
past
as
a
European
power
and
its
cultural
environment
have
generated
a
broad
contribution
to
various
forms
of
art,
most
notably
among
them
music.
Austria
has
been
the
birthplace
of
many
famous
composers
such
as
Joseph
Haydn,
Franz
Schubert,
Anton
Bruckner,
Johann
Strauss,
Sr.,
Johann
Strauss,
Jr.
and
Gustav
Mahler
as
well
as
members
of
the
Second
Viennese
School
such
as
Arnold
Schoenberg,
Anton
Webern
and
Alban
Berg.
Wolfgang
Amadeus
Mozart
was
born
in
Salzburg,
then
an
independent
Church
Principality,
though
one
that
was
culturally
closely
connected
to
Austria,
and
much
of
Mozart's
career
was
spent
in
Vienna.
Vienna
has
long
been
especially
an
important
centre
of
musical
innovation.
Eighteenth
and
nineteenth
century
composers
were
drawn
to
the
city
due
to
the
patronage
of
the
Habsburgs,
and
made
Vienna
the
European
capital
of
classical
music.
During
the
Baroque
period,
Slavic
and
Hungarian
folk
forms
influenced
Austrian
music.
Vienna's
status
began
its
rise
as
a
cultural
center
in
the
early
1500s,
and
was
focused
around
instruments
including
the
lute.
Ludwig
van
Beethoven
spent
the
better
part
of
his
life
in
Vienna.
Austria's
current
national
anthem,
attributed
to
Mozart,
was
chosen
after
World
War
II
to
replace
the
traditional
Austrian
anthem
by
Joseph
Haydn.
Austria
has
also
produced
one
notable
jazz
musician,
keyboardist
Josef
Zawinul,
who
helped
pioneer
electronic
influences
in
jazz
as
well
as
being
a
notable
composer
in
his
own
right.
The
pop
and
rock
musician
Falco
was
internationally
acclaimed
during
the
1980s,
especially
for
his
song
"Rock
Me
Amadeus"
dedicated
to
Mozart.
The
drummer
Thomas
Lang
was
born
in
Vienna
in
1967
and
is
now
world
renowned
for
his
technical
ability,
having
played
with
artists
such
as
Geri
Halliwell
and
Robbie
Williams.
Art
and
architecture.
Among
Austrian
Artists
and
architects
one
can
find
the
painters
Ferdinand
Georg
Waldmüller,
Rudolf
von
Alt,
Hans
Makart,
Gustav
Klimt,
Oskar
Kokoschka,
Egon
Schiele,
Carl
Moll,
and
Friedensreich
Hundertwasser,
the
photographers
Inge
Morath
and
Ernst
Haas,
and
architects
like
Johann
Bernhard
Fischer
von
Erlach,
Otto
Wagner,
Adolf
Loos,
and
Hans
Hollein.
Film
and
theater.
Austrian
contributions
to
the
worlds
of
film
and
theater
have
traditionally
been
strong.
Sascha
Kolowrat
was
an
Austrian
pioneer
of
filmmaking.
Billy
Wilder,
Fritz
Lang,
Josef
von
Sternberg,
and
Fred
Zinnemann
originally
came
from
Austria
before
establishing
themselves
as
internationally
relevant
movie
makers.
Willi
Forst,
Ernst
Marischka,
or
Franz
Antel
enriched
the
popular
cinema
in
German
language
speaking
countries.
Michael
Haneke
became
internationally
known
for
his
disturbing
cinematic
studies,
before
receiving
a
Golden
Globe
for
his
critically
acclaimed
film
"The
White
Ribbon"
in
2010.
The
first
Austrian
film
director
receiving
an
Academy
Award
was
Stefan
Ruzowitzky.
Many
Austrian
actors
were
able
to
pursue
a
career,
the
impact
of
which
was
sensed
beyond
national
borders.
Among
them
were
Peter
Lorre,
Curd
Jürgens,
Senta
Berger,
Oskar
Werner,
and
Klaus
Maria
Brandauer.
Hedy
Lamarr
and
Arnold
Schwarzenegger
became
American
as
well
as
international
movie
stars.
Christoph
Waltz
rose
to
international
fame
with
his
performance
in
"Inglourious
Basterds",
earning
an
Golden
Globe
Award
in
2010.
Max
Reinhardt
was
a
master
of
spectacular
and
astute
theater
productions.
Otto
Schenk
not
only
excelled
as
a
stage
actor,
but
also
as
an
opera
director.
Science,
philosophy
and
economics.
Austria
was
the
cradle
of
numerous
scientists
with
international
reputation.
Among
them
are
Ludwig
Boltzmann,
Ernst
Mach,
Victor
Franz
Hess
and
Christian
Doppler,
prominent
scientists
in
the
nineteenth
century.
In
the
twentieth
century,
contributions
by
Lise
Meitner,
Erwin
Schrödinger
and
Wolfgang
Pauli
to
nuclear
research
and
quantum
mechanics
were
key
to
these
areas'
development
during
the
1920s
and
1930s.
A
present-day
quantum
physicist
is
Anton
Zeilinger,
noted
as
the
first
scientist
to
demonstrate
quantum
teleportation.
In
addition
to
physicists,
Austria
was
the
birthplace
of
two
of
the
most
noteworthy
philosophers
of
the
twentieth
century,
Ludwig
Wittgenstein
and
Karl
Popper.
In
addition
to
them
biologists
Gregor
Mendel
and
Konrad
Lorenz
as
well
as
mathematician
Kurt
Gödel
and
engineers
such
as
Ferdinand
Porsche
and
Siegfried
Marcus
were
Austrians.
A
focus
of
Austrian
science
has
always
been
medicine
and
psychology,
starting
in
medieval
times
with
Paracelsus.
Eminent
physicians
like
Theodore
Billroth,
Clemens
von
Pirquet,
and
Anton
von
Eiselsberg
have
built
upon
the
achievements
of
the
19th
century
Vienna
School
of
Medicine.
Austria
was
home
to
psychologists
Sigmund
Freud,
Alfred
Adler,
Paul
Watzlawick
and
Hans
Asperger
and
psychiatrist
Viktor
Frankl.
The
Austrian
School
of
Economics,
which
is
prominent
as
one
of
the
main
competitive
directions
for
economic
theory,
is
related
to
Austrian
economists
Joseph
Schumpeter,
Eugen
von
Böhm-Bawerk,
Ludwig
von
Mises,
and
Friedrich
Hayek.
Other
noteworthy
Austrian-born
émigrés
include
the
management
thinker
Peter
Drucker,
scientist
Sir
Gustav
Nossal,
and
the
38th
Governor
of
California,
Arnold
Schwarzenegger.
Literature.
Complementing
its
status
as
a
land
of
artists
and
scientists,
Austria
has
always
been
a
country
of
poets,
writers,
and
novelists.
It
was
the
home
of
novelists
Arthur
Schnitzler,
Stefan
Zweig,
Thomas
Bernhard,
Franz
Kafka,
and
Robert
Musil,
of
poets
Georg
Trakl,
Franz
Werfel,
Franz
Grillparzer,
Rainer
Maria
Rilke,
Adalbert
Stifter,
Karl
Kraus
and
children's
author
Eva
Ibbotson.
Famous
contemporary
playwrights
and
novelists
are
Nobel
prize
winner
Elfriede
Jelinek,
Peter
Handke
and
Daniel
Kehlmann.
Cuisine.
Austria's
cuisine
is
derived
from
that
of
the
Austro-Hungarian
Empire.
Austrian
cuisine
is
mainly
the
tradition
of
Royal-Cuisine
("Hofküche")
delivered
over
centuries.
It
is
famous
for
its
well-balanced
variations
of
beef
and
pork
and
countless
variations
of
vegetables.
There
is
also
the
"Mehlspeisen"
Bakery,
which
created
particular
delicacies
such
as
Sachertorte,
"Krapfen"
which
are
doughnuts
usually
filled
with
apricot
marmalade
or
custard,
and
"Strudel"
such
as
"Apfelstrudel"
filled
with
apple
and
"Topfenstrudel"
filled
with
sweetened
sour
cream.
In
addition
to
native
regional
traditions,
the
cuisine
has
been
influenced
by
Hungarian,
Bohemia
Czech,
Jewish,
Italian,
Balkan
and
French
cuisine,
from
which
both
dishes
and
methods
of
food
preparation
have
often
been
borrowed.
The
Austrian
cuisine
is
therefore
one
of
the
most
multicultural
and
transcultural
in
Europe.
Typical
Austrian
dishes
include
Wiener
Schnitzel,
Schweinsbraten,
Kaiserschmarren,
Knödel,
Sachertorte
and
Tafelspitz.
There
are
also
Kärntner
Kasnudeln,
a
cooked
filled
dough-bag
with
a
type
of
cottage
cheese
and
spearmint,
and
Eierschwammerl
dishes.
The
"Eierschwammerl",
also
known
as
"Pfifferling",
are
native
yellow,
tan
mushrooms.
The
candy
Pez
was
invented
in
Austria,
as
well
as
Mannerschnitten.
Austria
is
also
famous
for
its
Mozartkugeln,
and
its
coffee
tradition.
Sports.
Due
to
the
mountainous
terrain,
alpine
skiing
is
a
prominent
sport
in
Austria.
Similar
sports
such
as
snowboarding
or
ski-jumping
are
also
widely
popular.
A
popular
team
sport
in
Austria
is
football,
which
is
governed
by
the
Austrian
Football
Association.
However,
Austria
rarely
has
international
success
in
this
discipline,
going
out
in
the
first
round
of
the
2008
UEFA
European
Football
Championship
which
was
co-hosted
by
Austria
and
Switzerland.
Besides
football,
Austria
also
has
professional
national
leagues
for
most
major
team
sports
including
the
Austrian
Hockey
League
for
ice
hockey,
and
the
Österreichische
Basketball
Bundesliga
for
basketball.
Bobsleigh,
luge,
and
skeleton
are
also
popular
events
with
a
permanent
track
located
in
Igls,
which
hosted
bobsleigh
and
luge
competitions
for
the
1964
and
1976
Winter
Olympics
held
in
Innsbruck.
The
first
Winter
Youth
Olympics
in
2012
will
be
held
in
Innsbruck
as
well.
---END.OF.DOCUMENT---
Astronomer.
An
astronomer
is
a
scientist
who
studies
celestial
bodies
such
as
planets,
stars,
and
galaxies.
Historically,
astronomy
was
more
concerned
with
the
classification
and
description
of
phenomena
in
the
sky,
while
astrophysics
attempted
to
explain
these
phenomena
and
the
differences
between
them
using
physical
laws.
Today,
that
distinction
has
mostly
disappeared.
Professional
astronomers
are
highly
educated
individuals
who
typically
have
a
PhD
in
physics
or
astronomy
and
are
employed
by
research
institutions
or
universities.
They
spend
the
majority
of
their
time
working
on
research,
although
they
quite
often
have
other
duties
such
as
teaching,
building
instruments,
or
aiding
in
the
operation
of
an
observatory.
The
number
of
professional
astronomers
in
the
United
States
is
actually
quite
small.
The
American
Astronomical
Society,
which
is
the
major
organization
of
professional
astronomers
in
North
America,
has
approximately
7,700
members.
This
number
includes
scientists
from
other
fields
such
as
physics,
geology,
and
engineering,
whose
research
interests
are
closely
related
to
astronomy.
The
International
Astronomical
Union
comprises
almost
9,259
members
from
89
different
countries
who
are
involved
in
astronomical
research
at
the
PhD
level
and
beyond.
While
the
number
of
professional
astronomers
worldwide
is
not
much
larger
than
the
population
of
a
small
town,
there
is
a
huge
community
of
amateur
astronomers.
Most
cities
have
amateur
astronomy
clubs
that
meet
on
a
regular
basis
and
often
host
star
parties
in
their
communities.
The
Astronomical
Society
of
the
Pacific
is
the
largest
general
astronomical
society
in
the
world,
comprising
both
professional
and
amateur
astronomers
as
well
as
educators
from
70
different
nations.
Like
any
hobby,
most
people
who
think
of
themselves
as
amateur
astronomers
may
devote
a
few
hours
a
month
to
stargazing
and
reading
the
latest
developments
in
research.
However,
amateurs
span
the
range
from
so-called
"armchair
astronomers"
to
the
very
ambitious,
who
own
science-grade
telescopes
and
instruments
with
which
they
are
able
to
make
their
own
discoveries
and
assist
professional
astronomers
in
research.
Calculation
in
astronomy.
(Arabic:
زيج
"astronomical
tables
of
Sind
and
Hind")
is
a
work
consisting
of
approximately
37
chapters
on
calendrical
and
astronomical
calculations
and
116
tables
with
calendrical,
astronomical
and
astrological
data,
as
well
as
a
table
of
sine
values.
This
is
the
first
of
many
Arabic
"Zijes"
based
on
the
Indian
astronomical
methods
known
as
the
"sindhind".
The
work
contains
tables
for
the
movements
of
the
sun,
the
moon
and
the
five
planets
known
at
the
time.
This
work
marked
the
turning
point
in
Islamic
astronomy.
Hitherto,
Muslim
astronomers
had
adopted
a
primarily
research
approach
to
the
field,
translating
works
of
others
and
learning
already
discovered
knowledge.
Al-Khwarizmi's
work
marked
the
beginning
of
non-traditional
methods
of
study
and
calculations.
The
original
Arabic
version
(written
c.
820)
is
lost,
but
a
version
by
the
Spanish
astronomer
Maslamah
Ibn
Ahmad
al-Majriti
(c.
1000)
has
survived
in
a
Latin
translation,
presumably
by
Adelard
of
Bath
(January
26,
1126).
The
four
surviving
manuscripts
of
the
Latin
translation
are
kept
at
the
Bibliothèque
publique
(Chartres),
the
Bibliothèque
Mazarine
(Paris),
the
Bibliotheca
Nacional
(Madrid)
and
the
Bodleian
Library
(Oxford).
Modern
astronomers.
Contrary
to
the
classical
image
of
an
old
astronomer
peering
through
a
telescope
through
the
dark
hours
of
the
night,
it
is
very
rare
for
a
modern
professional
astronomer
to
use
an
eyepiece
on
a
larger
telescope.
It
is
far
more
common
to
use
a
charge-coupled
device
camera
to
record
a
long,
deep
exposure,
allowing
a
more
sensitive
image
to
be
created
because
the
light
is
added
over
time.
Before
CCDs,
photographic
plates
were
a
common
method
of
observation.
Modern
astronomers
spend
relatively
little
time
at
telescopes
-
most
spend
a
few
weeks
per
year
observing,
and
the
rest
of
their
time
reducing
the
data
(changing
it
from
raw
data
to
processed
images)
and
analyzing
it.
Many
astronomers
work
entirely
from
astronomical
survey
or
space
observatory
data.
Others
work
with
radio
telescopes
like
the
Very
Large
Array,
which
is
entirely
automated,
although
it
is
maintained
by
telescope
operators.
Astronomers
who
serve
as
faculty
spend
much
of
their
time
teaching
undergraduate
and
graduate
classes.
Most
universities
also
have
outreach
programs
including
public
telescope
time
and
sometime
planetariums
as
a
public
service
and
to
encourage
interest
in
the
field.
---END.OF.DOCUMENT---
Amoeboid.
Amoeboids
are
single-celled
life-forms
characterized
by
an
irregular
shape.
"Amoeboid"
and
"amoeba"
are
often
used
interchangeably
even
by
biologists,
and
especially
refers
to
a
creature
moving
by
using
pseudopods.
Most
references
to
"amoebas"
or
"amoebae"
are
to
amoeboids
in
general
rather
than
to
the
specific
genus
"Amoeba".
The
genus
"Amoeba"
and
amoeboids
in
general
both
derive
their
names
from
the
ancient
Greek
word
for
change.
Structure.
The
superclass
Rhizopoda
has
naked
and
shelled
amoebas.
They
tend
to
ingest
food
and
creep
toward
food.
They
move
using
pseudopodia,
which
are
bulges
of
cytoplasm.
Naked
amoebas
use
all
of
their
body
for
pseudopod
movement.
Usually
half
is
used
in
shelled
amoebas;
the
other
half
grips
the
shell.
Amoebas
breathe
using
their
entire
cell
membrane
that
is
constantly
immersed
in
water.
Because
they
live
in
water,
they
have
no
problem
keeping
it
wet.
But
it
does
have
one
drawback.
Excess
water
often
crosses
into
the
cytosol.
All
have
one
contractile
vacuole
that
expels
excess
water.
This
contraction
is
powered
by
contracting
cytosol.
The
nuclei
of
amoebas
do
not
have
chromosomes.
They
also
persist
during
cell
division.
The
nuclear
membrane
pinches
in
two
during
telophase.
Amoebas
also
do
not
go
through
meiosis.
Amoebas
have
one
or
more
nucleoli.
The
food
scources
vary
in
rhizopoda.
Some
consume
bacteria.
Others
are
detritivores
and
eat
dead
organic
material.
Still
others
eat
other
protists.
They
extend
a
pair
of
pseudopodia
around
food.
They
fuse
to
make
a
food
vacuole
which
then
fuses
with
a
lysosome
to
add
digestive
chemicals.
Undigestied
food
is
expelled
at
the
cell
membrane.
Amoebas
use
pseudopodia
to
move
and
feed.
They
are
powered
by
flexible
microfilaments
near
the
membrane.
Microfilaments
are
at
least
50%
of
the
cytoskeleton.
The
other
parts
are
more
stiff
and
are
composed
of
intermediate
filaments
and
macrotubules.
These
are
not
used
in
amoeboid
movement,
but
are
stiff
skeletons
on
which
organelles
are
supported
or
can
move
on.
The
shells
of
amoebas
are
often
composed
of
calcium
ar
other
materials.
The
proteins
or
materials
are
synthesised
in
the
cell
and
exported
just
outside
the
cell
membrane.
The
shells
are
stiff
and
flexible,
but
not
too
much
of
either
trait.
Amoebas
seem
to
have
connections
with
two
phyla
of
the
lineage
funguslike
protists.
The
two
phyla
are
myxomycota
(plasmodial
slime
molds),
and
acrasiomycota
(cellular
slime
molds).
These
two
phyla
use
amoeboid
movement
in
their
feeding
stage.
One
is
basically
a
giant
multinucleate
amoeba,
while
the
other
lives
solitary
until
food
runs
out;
in
which
a
colony
of
these
functions
as
a
unit.
Myxomycotes
use
amoeboid
gametes,
as
well.
They
definitely
have
a
connection.
Diversity.
They
have
appeared
in
a
number
of
different
groups.
Some
cells
in
multicellular
animals
may
be
amoeboid,
for
instance
human
white
blood
cells,
which
consume
pathogens.
Many
protists
also
exist
as
individual
amoeboid
cells,
or
take
such
a
form
at
some
point
in
their
life-cycle.
The
most
famous
such
organism
is
"Amoeba
proteus";
the
name
amoeba
is
variously
used
to
describe
its
close
relatives,
other
organisms
similar
to
it,
or
the
amoeboids
in
general.
As
amoebas
themselves
are
polyphyletic
and
subject
to
some
imprecision
in
definition,
the
term
"Amoeboid"
does
not
provide
identification
of
an
organism,
and
is
better
understood
as
description
of
locomotion.
When
used
in
the
broader
sense,
the
term
can
include
the
following
groups:
Acanthamoeba,
Acrasis,
Adelphamoeba,
Amoeba,
Astramoeba,
Balamuthia,
Cashia,
Chaos,
Clydonella,
Dactylamoeba,
Dientamoeba,
Dinamoeba,
Discamoeba,
Echinamoeba,
Endamoeba,
Entamoeba,
Filamoeba,
Flabelulla,
Flagellipodium,
Flamella,
Gephyramoeba,
Gibbodiscus,
Glaeseria,
Gocevia,
Gruberella,
Gyromitus,
Hartmannella,
Heteramoeba,
Hollandella,
Histomonas,
Hyalodiscus,
Hydramoeba,
Hyperamoeba,
Iodamoeba,
Korotnevella,
Labyrinthula,
Learamoeba,
Leptomyxa,
Lingulamoeba,
Macropharyngomonas,
Malamoeba,
Mastigamoeba,
Mastigella,
Mastigina,
Mayorella,
Metachaos,
Micronuclearia,
Monopylocystis,
Naegleria,
Neoparamoeba,
Neovahlkampfia,
Nollandia,
Nuclearia,
Oscillosignum,
Paragocevia,
Paramoeba,
Paratetramitus,
Paravahlkampfia,
Parvamoeba,
Pelomyxa,
Pernina,
Pfiesteria,
Polychaos,
Pontifex,
Phreatamoeba,
Platyamoeba,
Protoacanthamoeba,
Protonaegleria,
Psalteriomonas,
Pseudomastigamoeba,
Plaesiobystra,
Rhizamoeba,
Rosculus,
Rugipes,
Saccamoeba,
Sappinia,
Sawyeria,
Stachyamoeba,
Stereomyxa,
Striamoeba,
Striolatus,
Stygamoeba,
Subulamoeba,
Tetramitus,
Thecamoeba,
Theratromyxa,
Trichamoeba,
Trichosphaerium,
Trienamoeba,
Trimastigamoeba,
Unda,
Vahlkampfia,
Vampyrella,
Vampyrellium,
Vannella,
Vexillifera,
and
Willaertia.
Classification.
Amoeboids
may
be
divided
into
several
morphological
categories
based
on
the
form
and
structure
of
the
pseudopods.
Those
where
the
pseudopods
are
supported
by
regular
arrays
of
microtubules
are
called
actinopods,
and
forms
where
they
are
not
are
called
rhizopods,
further
divided
into
lobose,
filose,
and
reticulose
amoebae.
There
is
also
a
strange
group
of
giant
marine
amoeboids,
the
xenophyophores,
that
do
not
fall
into
any
of
these
categories.
Most
amoeboid
are
now
grouped
in
Amoebozoa
or
Rhizaria.
---END.OF.DOCUMENT---
ASCII.
The
American
Standard
Code
for
Information
Interchange
(acronym:
ASCII;,)
is
a
character-encoding
scheme
based
on
the
ordering
of
the
English
alphabet.
ASCII
codes
represent
text
in
computers,
communications
equipment,
and
other
devices
that
use
text.
Most
modern
character-encoding
schemes
are
based
on
ASCII,
though
they
support
many
more
characters
than
did
ASCII.
US-ASCII
is
the
IANA
preferred
charset
name
for
ASCII.
Historically,
ASCII
developed
from
telegraphic
codes.
Its
first
commercial
use
was
as
a
seven-bit
teleprinter
code
promoted
by
Bell
data
services.
Work
on
ASCII
formally
began
October
6,
1960,
with
the
first
meeting
of
the
American
Standards
Association's
(ASA)
X3.2
subcommittee.
The
first
edition
of
the
standard
was
published
during
1963,
a
major
revision
during
1967,
and
the
most
recent
update
during
1986.
Compared
to
earlier
telegraph
codes,
the
proposed
Bell
code
and
ASCII
were
both
ordered
for
more
convenient
sorting
(i.e.,
alphabetization)
of
lists,
and
added
features
for
devices
other
than
teleprinters.
ASCII
includes
definitions
for
128
characters:
33
are
non-printing
control
characters
(now
mostly
obsolete)
that
affect
how
text
and
space
is
processed;
94
are
printable
characters,
and
the
space
is
considered
an
invisible
graphic.
The
most
commonly
used
character
encoding
on
the
World
Wide
Web
was
US-ASCII
until
December
2007,
when
it
was
surpassed
by
UTF-8.
History.
The
American
Standard
Code
for
Information
Interchange
(ASCII)
was
developed
under
the
auspices
of
a
committee
of
the
American
Standards
Association,
called
the
X3
committee,
by
its
X3.2
(later
X3L2)
subcommittee,
and
later
by
that
subcommittee's
X3.2.4
working
group.
The
ASA
became
the
United
States
of
America
Standards
Institute
or
USASI
and
ultimately
the
American
National
Standards
Institute.
The
X3.2
subcommittee
designed
ASCII
based
on
earlier
teleprinter
encoding
systems.
Like
other
character
encodings,
ASCII
specifies
a
correspondence
between
digital
bit
patterns
and
character
symbols
(i.e.
graphemes
and
control
characters).
This
allows
digital
devices
to
communicate
with
each
other
and
to
process,
store,
and
communicate
character-oriented
information
such
as
written
language.
Before
ASCII
was
developed,
the
encodings
in
use
included
26
alphabetic
characters,
10
numerical
digits,
and
from
11
to
25
special
graphic
symbols.
To
include
all
these,
and
control
characters
compatible
with
the
Comité
Consultatif
International
Téléphonique
et
Télégraphique
standard,
Fieldata,
and
early
EBCDIC,
more
than
64
codes
were
required
for
ASCII.
The
committee
debated
the
possibility
of
a
shift
key
function
(like
the
Baudot
code),
which
would
allow
more
than
64
codes
to
be
represented
by
six
bits.
In
a
shifted
code,
some
character
codes
determine
choices
between
options
for
the
following
character
codes.
It
allows
compact
encoding,
but
is
less
reliable
for
data
transmission;
an
error
in
transmitting
the
shift
code
typically
makes
a
long
part
of
the
transmission
unreadable.
The
standards
committee
decided
against
shifting,
and
so
ASCII
required
at
least
a
seven-bit
code.
The
committee
considered
an
eight-bit
code,
since
eight
bits
would
allow
two
four-bit
patterns
to
efficiently
encode
two
digits
with
binary
coded
decimal.
However,
it
would
require
all
data
transmission
to
send
eight
bits
when
seven
could
suffice.
The
committee
voted
to
use
a
seven-bit
code
to
minimize
costs
associated
with
data
transmission.
Since
perforated
tape
at
the
time
could
record
eight
bits
in
one
position,
it
also
allowed
for
a
parity
bit
for
error
checking
if
desired.
Machines
with
octets
as
the
native
data
type
that
did
not
use
parity
checking
typically
set
the
eighth
bit
to
"0".
The
code
itself
was
patterned
so
that
most
control
codes
were
together,
and
all
graphic
codes
were
together.
The
first
two
columns
(32
positions)
were
reserved
for
control
characters.
The
"space"
character
had
to
come
before
graphics
to
make
sorting
algorithms
easy,
so
it
became
position
0x20.
The
committee
decided
it
was
important
to
support
upper
case
64-character
alphabets,
and
chose
to
pattern
ASCII
so
it
could
be
reduced
easily
to
a
usable
64-character
set
of
graphic
codes.
Lower
case
letters
were
therefore
not
interleaved
with
upper
case.
To
keep
options
available
for
lower
case
letters
and
other
graphics,
the
special
and
numeric
codes
were
arranged
before
the
letters,
and
the
letter
'A'
was
placed
in
position
0x41
to
match
the
draft
of
the
corresponding
British
standard.
The
digits
0–9
were
arranged
so
they
correspond
to
values
in
binary
prefixed
with
011,
making
conversion
with
binary-coded
decimal
straightforward.
Many
of
the
non-alphanumeric
characters
were
positioned
to
correspond
to
their
shifted
position
on
typewriters.
Thus
#,
$
and
%
were
placed
to
correspond
to
3,
4,
and
5
in
the
adjacent
column.
The
parentheses
could
not
correspond
to
9
and
0,
however,
because
the
place
corresponding
to
0
was
taken
by
the
space
character.
Since
many
European
typewriters
placed
the
parentheses
with
8
and
9,
those
corresponding
positions
were
chosen
for
the
parentheses.
The
@
symbol
was
not
used
in
continental
Europe
and
the
committee
expected
it
would
be
replaced
by
an
accented
À
in
the
French
variation,
so
the
@
was
placed
in
position
0x40
next
to
the
letter
A.
The
control
codes
felt
essential
for
data
transmission
were
the
start
of
message
(SOM),
end
of
address
(EOA),
end
of
message
(EOM),
end
of
transmission
(EOT),
"who
are
you?"
(WRU),
"are
you?"
(RU),
a
reserved
device
control
(DC0),
synchronous
idle
(SYNC),
and
acknowledge
(ACK).
These
were
positioned
to
maximize
the
Hamming
distance
between
their
bit
patterns.
With
the
other
special
characters
and
control
codes
filled
in,
ASCII
was
published
as
ASA
X3.4-1963,
leaving
28
code
positions
without
any
assigned
meaning,
reserved
for
future
standardization,
and
one
unassigned
control
code.
It
now
seems
obvious
that
these
positions
should
have
been
assigned
to
the
lower
case
alphabet,
but
there
was
some
debate
at
the
time
whether
there
should
be
more
control
characters
instead.
The
indecision
did
not
last
long:
during
May
1963
the
CCITT
Working
Party
on
the
New
Telegraph
Alphabet
proposed
to
assign
lower
case
characters
to
columns
6
and
7,
and
International
Organization
for
Standardization
TC
97
SC
2
voted
during
October
to
incorporate
the
change
into
its
draft
standard.
The
X3.2.4
task
group
voted
its
approval
for
the
change
to
ASCII
at
its
May
1963
meeting.
Locating
the
lowercase
letters
in
columns
6
and
7
caused
the
characters
to
differ
in
bit
pattern
from
the
upper
case
by
a
single
bit,
which
simplified
case-insensitive
character
matching
and
the
construction
of
keyboards
and
printers.
The
X3
committee
made
other
changes,
including
other
new
characters
(the
brace
and
vertical
line
characters),
renaming
some
control
characters
(SOM
became
start
of
header
(SOH))
and
moving
or
removing
others
(RU
was
removed).
ASCII
was
subsequently
updated
as
USASI
X3.4-1967,
then
USASI
X3.4-1968,
ANSI
X3.4-1977,
and
finally,
ANSI
X3.4-1986
(the
first
two
are
occasionally
retronamed
ANSI
X3.4-1967,
and
ANSI
X3.4-1968).
The
X3
committee
also
addressed
how
ASCII
should
be
transmitted
(least
significant
bit
first),
and
how
it
should
be
recorded
on
perforated
tape.
They
proposed
a
9-track
standard
for
magnetic
tape,
and
attempted
to
deal
with
some
forms
of
punched
card
formats.
ASCII
itself
was
first
used
commercially
during
1963
as
a
seven-bit
teleprinter
code
for
American
Telephone
&
Telegraph's
TWX
(Teletype
Wide-area
eXchange)
network.
TWX
originally
used
the
earlier
five-bit
Baudot
code,
which
was
also
used
by
the
competing
Telex
teleprinter
system.
Bob
Bemer
introduced
features
such
as
the
escape
sequence.
His
British
colleague
Hugh
McGregor
Ross
helped
to
popularize
this
work—according
to
Bemer,
"so
much
so
that
the
code
that
was
to
become
ASCII
was
first
called
the
Bemer-Ross
Code
in
Europe".
Because
of
his
extensive
work
on
ASCII,
Bemer
has
been
called
"the
father
of
ASCII."
I
have
also
approved
recommendations
of
the
Secretary
of
Commerce
regarding
standards
for
recording
the
Standard
Code
for
Information
Interchange
on
magnetic
tapes
and
paper
tapes
when
they
are
used
in
computer
operations.
All
computers
and
related
equipment
configurations
brought
into
the
Federal
Government
inventory
on
and
after
July
1,
1969,
must
have
the
capability
to
use
the
Standard
Code
for
Information
Interchange
and
the
formats
prescribed
by
the
magnetic
tape
and
paper
tape
standards
when
these
media
are
used.
Other
international
standards
bodies
have
ratified
character
encodings
such
as
IEC
646
that
are
identical
or
nearly
identical
to
ASCII,
with
extensions
for
characters
outside
the
English
alphabet
and
symbols
used
outside
the
United
States,
such
as
the
symbol
for
the
United
Kingdom's
pound
sterling
(£).
Almost
every
country
needed
an
adapted
version
of
ASCII
since
ASCII
only
suited
the
needs
of
the
USA
and
a
few
other
countries.
For
example,
Canada
had
its
own
version
that
supported
French
characters.
Other
adapted
encodings
include
ISCII
(India),
VISCII
(Vietnam),
and
YUSCII
(Yugoslavia).
Although
these
encodings
are
sometimes
referred
to
as
ASCII,
true
ASCII
is
defined
strictly
only
by
ANSI
standard.
ASCII
was
incorporated
into
the
Unicode
character
set
as
the
first
128
symbols,
so
the
ASCII
characters
have
the
same
numeric
codes
in
both
sets.
This
allows
UTF-8
to
be
backward
compatible
with
ASCII,
a
significant
advantage.
ASCII
control
characters.
ASCII
reserves
the
first
32
codes
(numbers
0–31
decimal)
for
control
characters:
codes
originally
intended
not
to
represent
printable
information,
but
rather
to
control
devices
(such
as
printers)
that
make
use
of
ASCII,
or
to
provide
meta-information
about
data
streams
such
as
those
stored
on
magnetic
tape.
For
example,
character
10
represents
the
"line
feed"
function
(which
causes
a
printer
to
advance
its
paper),
and
character
8
represents
"backspace".
RFC
2822
refers
to
control
characters
that
do
not
include
carriage
return,
line
feed
or
white
space
as
non-whitespace
control
characters.
Except
for
the
control
characters
that
prescribe
elementary
line-oriented
formatting,
ASCII
does
not
define
any
mechanism
for
describing
the
structure
or
appearance
of
text
within
a
document.
Other
schemes,
such
as
markup
languages,
address
page
and
document
layout
and
formatting.
The
original
ASCII
standard
used
only
short
descriptive
phrases
for
each
control
character.
The
ambiguity
this
caused
was
sometimes
intentional
(where
a
character
would
be
used
slightly
differently
on
a
terminal
link
than
on
a
data
stream)
and
sometimes
accidental
(such
as
what
"delete"
means).
Probably
the
most
influential
single
device
on
the
interpretation
of
these
characters
was
the
ASR-33
Teletype
series,
which
was
a
printing
terminal
with
an
available
paper
tape
reader/punch
option.
Paper
tape
was
a
very
popular
medium
for
long-term
program
storage
through
the
1980s,
less
costly
and
in
some
ways
less
fragile
than
magnetic
tape.
In
particular,
the
Teletype
33
machine
assignments
for
codes
17
(Control-Q,
DC1,
also
known
as
XON),
19
(Control-S,
DC3,
also
known
as
XOFF),
and
127
(DELete)
became
de
facto
standards.
Because
the
keytop
for
the
O
key
also
showed
a
left-arrow
symbol
(from
ASCII-1963,
which
had
this
character
instead
of
underscore),
a
noncompliant
use
of
code
15
(Control-O,
Shift
In)
interpreted
as
"delete
previous
character"
was
also
adopted
by
many
early
timesharing
systems
but
eventually
became
neglected.
The
use
of
Control-S
(XOFF,
an
abbreviation
for
transmit
off)
as
a
"handshaking"
signal
warning
a
sender
to
stop
transmission
because
of
impending
overflow,
and
Control-Q
(XON,
"transmit
on")
to
resume
sending,
persists
to
this
day
in
many
systems
as
a
manual
output
control
technique.
On
some
systems
Control-S
retains
its
meaning
but
Control-Q
is
replaced
by
a
second
Control-S
to
resume
output.
Code
127
is
officially
named
"delete"
but
the
Teletype
label
was
"rubout".
Since
the
original
standard
did
not
give
detailed
interpretation
for
most
control
codes,
interpretations
of
this
code
varied.
The
original
Teletype
meaning,
and
the
intent
of
the
standard,
was
to
make
it
an
ignored
character,
the
same
as
NUL
(all
zeroes).
This
was
useful
specifically
for
paper
tape,
because
punching
the
all-ones
bit
pattern
on
top
of
an
existing
mark
would
obliterate
it.
Tapes
designed
to
be
"hand
edited"
could
even
be
produced
with
spaces
of
extra
NULs
(blank
tape)
so
that
a
block
of
characters
could
be
"rubbed
out"
and
then
replacements
put
into
the
empty
space.
As
video
terminals
began
to
replace
printing
ones,
the
value
of
the
"rubout"
character
was
lost.
DEC
systems,
for
example,
interpreted
"Delete"
to
mean
"remove
the
character
before
the
cursor,"
and
this
interpretation
also
became
common
in
Unix
systems.
Most
other
systems
used
"Backspace"
for
that
meaning
and
used
"Delete"
to
mean
"remove
the
character
at
the
cursor".
That
latter
interpretation
is
the
most
common
now.
Many
more
of
the
control
codes
have
been
given
meanings
quite
different
from
their
original
ones.
The
"escape"
character
(code
27),
for
example,
was
intended
originally
to
allow
sending
other
control
characters
as
literals
instead
of
invoking
their
meaning.
This
is
the
same
meaning
of
"escape"
encountered
in
URL
encodings,
C
language
strings,
and
other
systems
where
certain
characters
have
a
reserved
meaning.
Over
time
this
meaning
has
been
co-opted
and
has
eventually
been
changed.
In
modern
use,
an
ESC
sent
to
the
terminal
usually
indicates
the
start
of
a
command
sequence,
usually
in
the
form
of
a
so-called
"ANSI
escape
code"
(or,
more
properly,
a
"Control
Sequence
Introducer")
beginning
with
ESC
followed
by
a
"["
(left-bracket)
character.
An
ESC
sent
from
the
terminal
is
most
often
used
as
an
out-of-band
character
used
to
terminate
an
operation,
as
in
the
TECO
and
vi
text
editors.
The
inherent
ambiguity
of
many
control
characters,
combined
with
their
historical
usage,
created
problems
when
transferring
"plain
text"
files
between
systems.
The
best
example
of
this
is
the
newline
problem
on
various
operating
systems.
Teletypes
required
that
a
line
of
text
be
terminated
with
both
"Carriage
Return"
and
"Linefeed".
The
first
returns
the
printing
carriage
to
the
beginning
of
the
line
and
the
second
advances
to
the
next
line
without
moving
the
carriage.
However,
requiring
two
characters
to
mark
the
end
of
a
line
introduced
unnecessary
complexity
and
questions
as
to
how
to
interpret
each
character
when
encountered
alone.
To
simplify
matters,
plain
text
files
on
Unix
and
Amiga
systems
use
line
feeds
alone
to
separate
lines.
Similarly,
older
Macintosh
systems,
among
others,
use
only
carriage
returns
in
plain
text
files.
Various
DEC
operating
systems
used
both
characters
to
mark
the
end
of
a
line,
perhaps
for
compatibility
with
teletypes.
This
de
facto
standard
was
copied
into
M
and
then
into
MS-DOS
and
eventually
into
Microsoft
Windows.
Transmission
of
text
over
the
Internet,
for
protocols
as
E-mail
and
the
World
Wide
Web,
uses
both
characters.
The
pre-VMS
DEC
operating
systems,
along
with
CP/M,
tracked
file
length
only
in
units
of
disk
blocks
and
used
Control-Z
(SUB)
to
mark
the
end
of
the
actual
text
in
the
file
(also
done
for
CP/M
compatibility
in
some
cases
in
MS-DOS,
though
MS-DOS
2
added
the
ability
to
record
exact
file
lengths
and
this
is
usually
relied
on
today).
Text
strings
ending
with
the
null
character
are
known
as
ASCIZ
or
C
strings.
ASCII
printable
characters.
Code
0x20,
the
space
character,
denotes
the
space
between
words,
as
produced
by
the
space-bar
of
a
keyboard.
The
space
character
is
considered
an
invisible
graphic
rather
than
a
control
character.
Codes
0x21
to
0x7E,
known
as
the
printable
characters,
represent
letters,
digits,
punctuation
marks,
and
a
few
miscellaneous
symbols.
Aliases.
Of
these,
the
IANA
encourages
use
of
the
name
"US-ASCII"
for
Internet
uses
of
ASCII.
One
often
finds
this
in
the
optional
"charset"
parameter
in
the
Content-Type
header
of
some
MIME
messages,
in
the
equivalent
"meta"
element
of
some
HTML
documents,
and
in
the
encoding
declaration
part
of
the
prologue
of
some
XML
documents.
Variants.
As
computer
technology
spread
throughout
the
world,
different
standards
bodies
and
corporations
developed
many
variations
of
ASCII
to
facilitate
the
expression
of
non-English
languages
that
used
Roman-based
alphabets.
One
could
class
some
of
these
variations
as
"ASCII
extensions",
although
some
misuse
that
term
to
represent
all
variants,
including
those
that
do
not
preserve
ASCII's
character-map
in
the
7-bit
range.
The
PETSCII
code
Commodore
International
used
for
their
8-bit
systems
is
probably
unique
among
post-1970
codes
in
being
based
on
ASCII-1963,
instead
of
the
more
common
ASCII-1967,
such
as
found
on
the
ZX
Spectrum
computer.
Atari
and
Galaksija
computers
also
used
ASCII
variants.
Incompatibility
vs
interoperability.
From
early
in
its
development,
ASCII
was
intended
to
be
just
one
of
several
national
variants
of
an
international
character
code
standard,
ultimately
published
as
IEC
646
(1972),
which
would
share
most
characters
in
common
but
assign
other
locally-useful
characters
to
several
code
points
reserved
for
"national
use."
However,
the
four
years
that
elapsed
between
the
publication
of
ASCII-1963
and
ISO's
first
acceptance
of
an
international
recommendation
during
1967
caused
ASCII's
choices
for
the
national
use
characters
to
seem
to
be
de
facto
standards
for
the
world,
causing
confusion
and
incompatibility
once
other
countries
did
begin
to
make
their
own
assignments
to
these
code
points.
ISO/IEC
646,
like
ASCII,
was
a
7-bit
character
set.
It
did
not
make
any
additional
codes
available,
so
the
same
code
points
encoded
different
characters
in
different
countries.
Escape
codes
were
defined
to
indicate
which
national
variant
applied
to
a
piece
of
text,
but
they
were
rarely
used,
so
it
was
often
impossible
to
know
what
variant
to
work
with
and
therefore
which
character
a
code
represented,
and
text-processing
systems
could
generally
cope
with
only
one
variant
anyway.
Because
the
bracket
and
brace
characters
of
ASCII
were
assigned
to
"national
use"
code
points
that
were
used
for
accented
letters
in
other
national
variants
of
ISO/IEC
646,
a
German,
French,
or
Swedish,
etc.,
programmer
had
to
get
used
to
reading
and
writing
codice_1
codice_2
C
trigraphs
were
created
to
solve
this
problem
for
ANSI
C,
although
their
late
introduction
and
inconsistent
implementation
in
compilers
limited
their
use.
Eventually,
as
8-,
16-,
and
32-bit
computers
began
to
replace
18-
and
36-bit
computers
as
the
norm,
it
became
common
to
use
an
8-bit
byte
to
store
each
character
in
memory,
providing
an
opportunity
for
extended,
8-bit,
relatives
of
ASCII,
with
the
128
additional
characters
providing
room
to
avoid
most
of
the
ambiguity
that
had
been
necessary
in
7-bit
codes.
For
example,
IBM
developed
8-bit
code
pages,
such
as
code
page
437,
which
replaced
the
control-characters
with
graphic
symbols
such
as
smiley
faces,
and
mapped
additional
graphic
characters
to
the
upper
128
positions.
Operating
systems
such
as
DOS
supported
these
code-pages,
and
manufacturers
of
IBM
PCs
supported
them
in
hardware.
Digital
Equipment
Corporation
developed
the
Multinational
Character
Set
(DEC-MCS)
for
use
in
the
popular
VT220
terminal.
Eight-bit
standards
such
as
IEC
8859
(derived
from
the
DEC-MCS)
and
Mac
OS
Roman
developed
as
true
extensions
of
ASCII,
leaving
the
original
character-mapping
intact,
but
adding
additional
character
definitions
after
the
first
128
(i.e.,
7-bit)
characters.
This
enabled
representation
of
characters
used
in
a
broader
range
of
languages.
Because
there
were
several
competing
8-bit
code
standards,
they
continued
to
suffer
from
incompatibilities
and
limitations.
Still,
ISO-8859-1
(Latin
1),
its
variant
Windows-1252
(often
mislabeled
as
ISO-8859-1),
and
the
original
7-bit
ASCII
remain
the
most
common
character
encodings
in
use
today.
Unicode.
Unicode
and
the
ISO/IEC
10646
Universal
Character
Set
(UCS)
have
a
much
wider
array
of
characters,
and
their
various
encoding
forms
have
begun
to
supplant
ISO/IEC
8859
and
ASCII
rapidly
in
many
environments.
While
ASCII
is
limited
to
128
characters,
Unicode
and
the
UCS
support
more
characters
by
separating
the
concepts
of
unique
identification
(using
natural
numbers
called
"code
points")
and
encoding
(to
8-,
16-
or
32-bit
binary
formats,
called
UTF-8,
UTF-16
and
UTF-32).
To
allow
backward
compatibility,
the
128
ASCII
and
256
ISO-8859-1
(Latin
1)
characters
are
assigned
Unicode/UCS
code
points
that
are
the
same
as
their
codes
in
the
earlier
standards.
Therefore,
ASCII
can
be
considered
a
7-bit
encoding
scheme
for
a
very
small
subset
of
Unicode/UCS,
and,
conversely,
the
UTF-8
encoding
forms
are
binary-compatible
with
ASCII
for
code
points
below
128,
meaning
all
ASCII
is
valid
UTF-8.
The
other
encoding
forms
resemble
ASCII
in
how
they
represent
the
first
128
characters
of
Unicode,
but
use
16
or
32
bits
per
character,
so
they
require
conversion
for
compatibility.
(similarly
UCS-2
is
upwards
compatible
with
UTF-16)
Order.
The
slang
expression
"ASCIIbetical"
is
sometimes
used
for
this
order.
This
ordering
can
be
refined
by
converting
uppercase
letters
to
lowercase
before
comparing
ASCII
values,
or
for
more
sophisticated
purposes,
applying
a
collation
map
to
bring
accented
characters
into
the
correct
positions.
---END.OF.DOCUMENT---
Animation.
The
bouncing
ball
animation
(below)
consists
of
these
6
frames.
This
animation
moves
at
10
frames
per
second.
Animation
is
the
rapid
display
of
a
sequence
of
images
of
2-D
or
3-D
artwork
or
model
positions
in
order
to
create
an
illusion
of
movement.
It
is
an
optical
illusion
of
motion
due
to
the
phenomenon
of
persistence
of
vision,
and
can
be
created
and
demonstrated
in
a
number
of
ways.
The
most
common
method
of
presenting
animation
is
as
a
motion
picture
or
video
program,
although
several
other
forms
of
presenting
animation
also
exist.
Early
examples.
Early
examples
of
attempts
to
capture
the
phenomenon
of
motion
drawing
can
be
found
in
paleolithic
cave
paintings,
where
animals
are
depicted
with
multiple
legs
in
superimposed
positions,
clearly
attempting
to
convey
the
perception
of
motion.
A
5,200
year
old
earthen
bowl
found
in
Iran
in
Shahr-i
Sokhta
has
five
images
of
a
goat
painted
along
the
sides.
This
has
been
claimed
to
be
an
example
of
early
animation.
However,
since
no
equipment
existed
to
show
the
images
in
motion,
such
a
series
of
images
cannot
be
called
animation
in
a
true
sense
of
the
word.
The
phenakistoscope,
praxinoscope,
as
well
as
the
common
flip
book
were
early
popular
animation
devices
invented
during
the
1800s,
while
a
Chinese
zoetrope-type
device
was
invented
already
in
180
AD.
These
devices
produced
movement
from
sequential
drawings
using
technological
means,
but
animation
did
not
really
develop
much
further
until
the
advent
of
cinematography.
There
is
no
single
person
who
can
be
considered
the
"creator"
of
the
art
of
film
animation,
as
there
were
several
people
doing
several
projects
which
could
be
considered
various
types
of
animation
all
around
the
same
time.
Georges
Méliès
was
a
creator
of
special-effect
films;
he
was
generally
one
of
the
first
people
to
use
animation
with
his
technique.
He
discovered
a
technique
by
accident
which
was
to
stop
the
camera
rolling
to
change
something
in
the
scene,
and
then
continue
rolling
the
film.
This
idea
was
later
known
as
stop-motion
animation.
Méliès
discovered
this
technique
accidentally
when
his
camera
broke
down
while
shooting
a
bus
driving
by.
When
he
had
fixed
the
camera,
a
hearse
happened
to
be
passing
by
just
as
Méliès
restarted
rolling
the
film,
his
end
result
was
that
he
had
managed
to
make
a
bus
transform
into
a
hearse.
This
was
just
one
of
the
great
contributors
to
animation
in
the
early
years.
The
earliest
surviving
stop-motion
advertising
film
was
an
English
short
by
Arthur
Melbourne-Cooper
called
"Matches:
An
Appeal"
(1899).
Developed
for
the
Bryant
and
May
Matchsticks
company,
it
involved
stop-motion
animation
of
wired-together
matches
writing
a
patriotic
call
to
action
on
a
blackboard.
J.
Stuart
Blackton
was
possibly
the
first
American
filmmaker
to
use
the
techniques
of
stop-motion
and
hand-drawn
animation.
Introduced
to
filmmaking
by
Edison,
he
pioneered
these
concepts
at
the
turn
of
the
20th
century,
with
his
first
copyrighted
work
dated
1900.
Several
of
his
films,
among
them
"The
Enchanted
Drawing"
(1900)
and
"Humorous
Phases
of
Funny
Faces"
(1906)
were
film
versions
of
Blackton's
"lightning
artist"
routine,
and
utilized
modified
versions
of
Méliès'
early
stop-motion
techniques
to
make
a
series
of
blackboard
drawings
appear
to
move
and
reshape
themselves.
'Humorous
Phases
of
Funny
Faces'
is
regularly
cited
as
the
first
true
animated
film,
and
Blackton
is
considered
the
first
true
animator.
Another
French
artist,
Émile
Cohl,
began
drawing
cartoon
strips
and
created
a
film
in
1908
called
"Fantasmagorie".
The
film
largely
consisted
of
a
stick
figure
moving
about
and
encountering
all
manner
of
morphing
objects,
such
as
a
wine
bottle
that
transforms
into
a
flower.
There
were
also
sections
of
live
action
where
the
animator’s
hands
would
enter
the
scene.
The
film
was
created
by
drawing
each
frame
on
paper
and
then
shooting
each
frame
onto
negative
film,
which
gave
the
picture
a
blackboard
look.
This
makes
"Fantasmagorie"
the
first
animated
film
created
using
what
came
to
be
known
as
traditional
(hand-drawn)
animation.
Following
the
successes
of
Blackton
and
Cohl,
many
other
artists
began
experimenting
with
animation.
One
such
artist
was
Winsor
McCay,
a
successful
newspaper
cartoonist,
who
created
detailed
animations
that
required
a
team
of
artists
and
painstaking
attention
for
detail.
Each
frame
was
drawn
on
paper;
which
invariably
required
backgrounds
and
characters
to
be
redrawn
and
animated.
Among
McCay's
most
noted
films
are
"Little
Nemo"
(1911),
"Gertie
the
Dinosaur"
(1914)
and
"The
Sinking
of
the
Lusitania"
(1918).
The
production
of
animated
short
films,
typically
referred
to
as
"cartoons",
became
an
industry
of
its
own
during
the
1910s,
and
cartoon
shorts
were
produced
to
be
shown
in
movie
theaters.
The
most
successful
early
animation
producer
was
John
Randolph
Bray,
who,
along
with
animator
Earl
Hurd,
patented
the
cel
animation
process
which
dominated
the
animation
industry
for
the
rest
of
the
decade.
Traditional
animation.
Traditional
animation
(also
called
cel
animation
or
hand-drawn
animation)
was
the
process
used
for
most
animated
films
of
the
20th
century.
The
individual
frames
of
a
traditionally
animated
film
are
photographs
of
drawings,
which
are
first
drawn
on
paper.
To
create
the
illusion
of
movement,
each
drawing
differs
slightly
from
the
one
before
it.
The
animators'
drawings
are
traced
or
photocopied
onto
transparent
acetate
sheets
called
cels,
which
are
filled
in
with
paints
in
assigned
colors
or
tones
on
the
side
opposite
the
line
drawings.
The
completed
character
cels
are
photographed
one-by-one
onto
motion
picture
film
against
a
painted
background
by
a
rostrum
camera.
The
traditional
cel
animation
process
became
obsolete
by
the
beginning
of
the
21st
century.
Today,
animators'
drawings
and
the
backgrounds
are
either
scanned
into
or
drawn
directly
into
a
computer
system.
Various
software
programs
are
used
to
color
the
drawings
and
simulate
camera
movement
and
effects.
The
final
animated
piece
is
output
to
one
of
several
delivery
media,
including
traditional
35
mm
film
and
newer
media
such
as
digital
video.
The
"look"
of
traditional
cel
animation
is
still
preserved,
and
the
character
animators'
work
has
remained
essentially
the
same
over
the
past
70
years.
Some
animation
producers
have
used
the
term
"tradigital"
to
describe
cel
animation
which
makes
extensive
use
of
computer
technology.
Examples
of
traditionally
animated
feature
films
include
"Pinocchio"
(United
States,
1940),
"Animal
Farm"
(United
Kingdom,
1954),
and
"Akira"
(Japan,
1988).
Traditional
animated
films
which
were
produced
with
the
aid
of
computer
technology
include
"The
Lion
King"
(US,
1994)
"Sen
to
Chihiro
no
Kamikakushi
(Spirited
Away)"
(Japan,
2001),
"Treasure
Planet"
(USA,
2002)
and
"Les
Triplettes
de
Belleville"
(2003).
Stop
motion.
Stop-motion
animation
is
used
to
describe
animation
created
by
physically
manipulating
real-world
objects
and
photographing
them
one
frame
of
film
at
a
time
to
create
the
illusion
of
movement.
There
are
many
different
types
of
stop-motion
animation,
usually
named
after
the
type
of
media
used
to
create
the
animation.
Computer
software
is
widely
available
to
create
this
type
of
animation.
Computer
animation.
Computer
animation
encompasses
a
variety
of
techniques,
the
unifying
factor
being
that
the
animation
is
created
digitally
on
a
computer.
2D
animation.
2D
animation
figures
are
created
and/or
edited
on
the
computer
using
2D
bitmap
graphics
or
created
and
edited
using
2D
vector
graphics.
This
includes
automated
computerized
versions
of
traditional
animation
techniques
such
as
of
tweening,
morphing,
onion
skinning
and
interpolated
rotoscoping.
Examples:
"Foster's
Home
for
Imaginary
Friends",
"Danny
Phantom",
Waltz
with
Bashir
3D
animation.
3D
animation
are
digitally
modeled
and
manipulated
by
an
animator.
In
order
to
manipulate
a
mesh,
it
is
given
a
digital
skeletal
structure
that
can
be
used
to
control
the
mesh.
This
process
is
called
rigging.
Various
other
techniques
can
be
applied,
such
as
mathematical
functions
(ex.
gravity,
particle
simulations),
simulated
fur
or
hair,
effects
such
as
fire
and
water
and
the
use
of
Motion
capture
to
name
but
a
few,
these
techniques
fall
under
the
category
of
3d
dynamics.
Many
3D
animations
are
very
believable
and
are
commonly
used
as
Visual
effects
for
recent
movies.
Terms.
2D
animation
techniques
tend
to
focus
on
image
manipulation
while
3D
techniques
usually
build
virtual
worlds
in
which
characters
and
objects
move
and
interact.
3D
animation
can
create
images
that
seem
real
to
the
viewer.
---END.OF.DOCUMENT---
Apollo.
In
Greek
and
Roman
mythology,
Apollo
(in
Greek,
"Ἀπόλλων"—"Apóllōn"
or
"Ἀπέλλων"—"Apellōn"),
is
one
of
the
most
important
and
diverse
of
the
Olympian
deities.
The
ideal
of
the
"kouros"
(a
beardless
youth),
Apollo
has
been
variously
recognized
as
a
god
of
light
and
the
sun;
truth
and
prophecy;
archery;
medicin,
healing
and
plague;
music,
poetry,
and
the
arts;
and
more.
Apollo
is
the
son
of
Zeus
and
Leto,
and
has
a
twin
sister,
the
chaste
huntress
Artemis.
Apollo
is
known
in
Greek-influenced
Etruscan
mythology
as
"Apulu".
Apollo
was
worshiped
in
both
ancient
Greek
and
Roman
religion,
as
well
as
in
the
modern
Greco–Roman
Neopaganism.
As
the
patron
of
Delphi
("Pythian
Apollo"),
Apollo
was
an
oracular
god
—
the
prophetic
deity
of
the
Delphic
Oracle.
Medicine
and
healing
were
associated
with
Apollo,
whether
through
the
god
himself
or
mediated
through
his
son
Asclepius,
yet
Apollo
was
also
seen
as
a
god
who
could
bring
ill-health
and
deadly
plague
as
well
as
one
who
had
the
ability
to
cure.
Amongst
the
god's
custodial
charges,
Apollo
became
associated
with
dominion
over
colonists,
and
as
the
patron
defender
of
herds
and
flocks.
As
the
leader
of
the
Muses
("Apollon
Musagetes")
and
director
of
their
choir,
Apollo
functioned
as
the
patron
god
of
music
and
poetry.
Hermes
created
the
lyre
for
him,
and
the
instrument
became
a
common
attribute
of
Apollo.
Hymns
sung
to
Apollo
were
called
paeans.
In
Hellenistic
times,
especially
during
the
third
century
BCE,
as
"Apollo
Helios"
he
became
identified
among
Greeks
with
Helios,
god
of
the
sun,
and
his
sister
Artemis
similarly
equated
with
Selene,
goddess
of
the
moon.
In
Latin
texts,
on
the
other
hand,
Joseph
Fontenrose
declared
himself
unable
to
find
any
conflation
of
Apollo
with
Sol
among
the
Augustan
poets
of
the
first
century,
not
even
in
the
conjurations
of
Aeneas
and
Latinus
in
"Aeneid"
XII
(161–215).
Apollo
and
Helios/Sol
remained
separate
beings
in
literary
and
mythological
texts
until
the
third
century
CE.
Etymology.
The
etymology
of
"Apollo"
is
uncertain.
Several
instances
of
popular
etymology
are
attested
from
ancient
authors.
Thus,
Plato
in
"Cratylus"
connects
the
name
with
"redeem",
with
"purification",
and
with
"simple",
in
particular
in
reference
to
the
Thessalian
form
of
the
name,
and
finally
with
"ever-shooting".
Hesychius
connects
the
name
Apollo
with
the
Doric
απελλα,
which
means
"assembly",
so
that
Apollo
would
be
the
god
of
political
life,
and
he
also
gives
the
explanation
σηκος
("fold"),
in
which
case
Apollo
would
be
the
god
of
flocks
and
herds.
It
is
also
possible
that
"apellai"
derives
from
an
old
form
of
Apollo
which
can
be
equated
with
Appaliunas,
an
Anatolian
god
whose
name
possibly
means
"father
lion"
or
"father
light".
The
Greeks
later
associated
Apollo's
name
with
the
Greek
verb
απολλυμι
(apollymi)
meaning
"to
destroy".
It
has
also
been
suggested
that
Apollo
comes
from
the
Hurrian
and
Hittite
divinity,
Aplu,
who
was
widely
evoked
during
the
"plague
years".
Aplu,
it
is
suggested,
comes
from
the
Akkadian
"Aplu
Enlil",
meaning
"the
son
of
Enlil",
a
title
that
was
given
to
the
god
Nergal,
who
was
linked
to
Shamash,
Babylonian
god
of
the
sun.
Origins
of
cult.
There
are
generally
two
broad
opinions
on
the
origins
of
Apollo:
one
derives
him
from
the
East,
the
other
connects
him
to
the
Dorians
and
their
apellai
(cf.
also
the
month
Apellaios).
In
any
case,
Walter
Burkert
notes
that
components
of
various
origins
are
discernible
in
his
worship:
a
Dorian
Greek,
a
Cretan-Minoan
and
a
Syro-Hittite.
According
to
the
first
opinion,
both
Greek
and
Etruscan
Apollo
came
to
the
Aegean
during
the
Iron
Age
(i.e.
from
c.1100
BCE
to
c.
800
BCE)
from
Anatolia.
Homer
pictures
him
on
the
side
of
the
Trojans,
against
the
Achaeans,
during
the
Trojan
War
and
he
has
close
affiliations
with
a
Luwian
deity,
Apaliunas,
who
in
turn
seems
to
have
traveled
west
from
further
east.
The
Late
Bronze
Age
(from
1700–1200
BCE)
Hittite
and
Hurrian
"Aplu",
like
the
Homeric
Apollo,
was
a
god
of
plagues,
and
resembles
the
mouse
god
"Apollo
Smintheus".
Here
we
have
an
apotropaic
situation,
where
a
god
originally
bringing
the
plague
was
invoked
to
end
it,
merging
over
time
through
fusion
with
the
Mycenaean
healer-god
Paeon
(PA-JA-WO
in
Linear
B);
Paeon,
in
Homer's
"Iliad",
was
the
Greek
healer
of
the
wounded
gods
Ares
and
Hades.
In
other
writers,
the
word
becomes
a
mere
epithet
of
Apollo
in
his
capacity
as
a
god
of
healing,
but
it
is
now
known
from
Linear
B
that
Paeon
was
originally
a
separate
deity.
Homer
illustrated
Paeon
the
god,
as
well
as
the
song
both
of
apotropaic
thanksgiving
or
triumph,
and
Hesiod
also
separated
the
two;
in
later
poetry
Paeon
was
invoked
independently
as
a
god
of
healing.
It
is
equally
difficult
to
separate
Paeon
or
Paean
in
the
sense
of
"healer"
from
Paean
in
the
sense
of
"song."
Such
songs
were
originally
addressed
to
Apollo,
and
afterwards
to
other
gods,
Dionysus,
Helios,
Asclepius.
About
the
fourth
century
BCE,
the
paean
became
merely
a
formula
of
adulation;
its
object
was
either
to
implore
protection
against
disease
and
misfortune,
or
to
offer
thanks
after
such
protection
had
been
rendered.
It
was
in
this
way
that
Apollo
had
become
recognised
as
the
god
of
music.
Apollo's
role
as
the
slayer
of
the
Python
led
to
his
association
with
battle
and
victory;
hence
it
became
the
Roman
custom
for
a
paean
to
be
sung
by
an
army
on
the
march
and
before
entering
into
battle,
when
a
fleet
left
the
harbour,
and
also
after
a
victory
had
been
won.
Apollo's
links
with
oracles
again
seem
to
be
associated
with
wishing
to
know
the
outcome
of
an
illness.
He
is
a
god
of
music
and
the
lyre.
Healing
belongs
to
his
realm:
he
was
the
father
of
Asclepius,
the
god
of
medicine.
The
Muses
are
part
of
his
retinue,
so
that
music,
history,
poetry
and
dance
all
belong
to
him.
Cult
sites.
Unusually
among
the
Olympic
deities,
Apollo
had
two
cult
sites
that
had
widespread
influence:
Delos
and
Delphi.
In
cult
practice,
Delian
Apollo
and
Pythian
Apollo
(the
Apollo
of
Delphi)
were
so
distinct
that
they
might
both
have
shrines
in
the
same
locality.
Theophoric
names
such
as
"Apollodorus"
or
"Apollonios"
and
cities
named
Apollonia
are
met
with
throughout
the
Greek
world.
Apollo's
cult
was
already
fully
established
when
written
sources
commenced,
about
650
BCE.
Oracular
shrines.
Apollo
had
a
famous
oracle
in
Delphi,
and
other
notable
ones
in
Clarus
and
Branchidae.
His
oracular
shrine
in
Abae
in
Phocis,
where
he
bore
the
toponymic
epithet
"Abaeus"
(,
"Apollon
Abaios")
was
important
enough
to
be
consulted
by
Croesus
(Herodotus,
1.46).
Oracles
were
also
given
by
sons
of
Apollo.
Festivals.
The
chief
Apollonian
festivals
were
the
Boedromia,
Carneia,
Carpiae,
Daphnephoria,
Delia,
Hyacinthia,
Metageitnia,
Pyanepsia,
Pythia
and
Thargelia.
Attributes
and
symbols.
Apollo's
most
common
attributes
were
the
bow
and
arrow.
Other
attributes
of
his
included
the
kithara
(an
advanced
version
of
the
common
lyre),
the
plectrum
and
the
sword.
Another
common
emblem
was
the
sacrificial
tripod,
representing
his
prophetic
powers.
The
Pythian
Games
were
held
in
Apollo's
honor
every
four
years
at
Delphi.
The
bay
laurel
plant
was
used
in
expiatory
sacrifices
and
in
making
the
crown
of
victory
at
these
games.
The
palm
was
also
sacred
to
Apollo
because
he
had
been
born
under
one
in
Delos.
Animals
sacred
to
Apollo
included
wolves,
dolphins,
roe
deer,
swans,
cicadas
(symbolizing
music
and
song),
hawks,
ravens,
crows,
snakes
(referencing
Apollo's
function
as
the
god
of
prophecy),
mice
and
griffins,
mythical
eagle–lion
hybrids
of
Eastern
origin.
As
god
of
colonization,
Apollo
gave
oracular
guidance
on
colonies,
especially
during
the
height
of
colonization,
750–550
BCE.
According
to
Greek
tradition,
he
helped
Cretan
or
Arcadian
colonists
found
the
city
of
Troy.
However,
this
story
may
reflect
a
cultural
influence
which
had
the
reverse
direction:
Hittite
cuneiform
texts
mention
a
Minor
Asian
god
called
"Appaliunas"
or
"Apalunas"
in
connection
with
the
city
of
Wilusa
attested
in
Hittite
inscriptions,
which
is
now
generally
regarded
as
being
identical
with
the
Greek
Ilion
by
most
scholars.
In
this
interpretation,
Apollo's
title
of
"Lykegenes"
can
simply
be
read
as
"born
in
Lycia",
which
effectively
severs
the
god's
supposed
link
with
wolves
(possibly
a
folk
etymology).
In
literary
contexts,
Apollo
represents
harmony,
order,
and
reason—characteristics
contrasted
with
those
of
Dionysus,
god
of
wine,
who
represents
ecstasy
and
disorder.
The
contrast
between
the
roles
of
these
gods
is
reflected
in
the
adjectives
Apollonian
and
Dionysian.
However,
the
Greeks
thought
of
the
two
qualities
as
complementary:
the
two
gods
are
brothers,
and
when
Apollo
at
winter
left
for
Hyperborea,
he
would
leave
the
Delphic
oracle
to
Dionysus.
This
contrast
appears
to
be
shown
on
the
two
sides
of
the
Borghese
Vase.
Apollo
is
often
associated
with
the
Golden
Mean.
This
is
the
Greek
ideal
of
moderation
and
a
virtue
that
opposes
gluttony.
Roman
Apollo.
The
Roman
worship
of
Apollo
was
adopted
from
the
Greeks.
As
a
quintessentially
Greek
god,
Apollo
had
no
direct
Roman
equivalent,
although
later
Roman
poets
often
referred
to
him
as
Phoebus.
There
was
a
tradition
that
the
Delphic
oracle
was
consulted
as
early
as
the
period
of
the
kings
of
Rome
during
the
reign
of
Tarquinius
Superbus.
On
the
occasion
of
a
pestilence
in
the
430s
BC,
Apollo's
first
temple
at
Rome
was
established
in
the
Flaminian
fields,
replacing
an
older
cult
site
there
known
as
the
"Apollinare".
During
the
Second
Punic
War
in
212
BC,
the
"Ludi
Apollinares"
("Apollonian
Games")
were
instituted
in
his
honor,
on
the
instructions
of
a
prophecy
attributed
to
one
Marcius.
In
the
time
of
Augustus,
who
considered
himself
under
the
special
protection
of
Apollo
and
was
even
said
to
be
his
son,
his
worship
developed
and
he
became
one
of
the
chief
gods
of
Rome.
After
the
battle
of
Actium,
which
was
fought
near
a
sanctuary
of
Apollo,
Augustus
enlarged
Apollo's
temple,
dedicated
a
portion
of
the
spoils
to
him,
and
instituted
quinquennial
games
in
his
honour.
He
also
erected
a
new
temple
to
the
god
on
the
Palatine
hill.
Sacrifices
and
prayers
on
the
Palatine
to
Apollo
and
Diana
formed
the
culmination
of
the
Secular
Games,
held
in
17
BCE
to
celebrate
the
dawn
of
a
new
era.
In
art.
In
art,
Apollo
is
depicted
as
a
handsome
beardless
young
man,
often
with
a
kithara
(as
Apollo
Citharoedus)
or
bow
in
his
hand,
or
reclining
on
a
tree
(the
Apollo
Lykeios
and
Apollo
Sauroctonos
types).
The
Apollo
Belvedere
is
a
marble
sculpture
that
was
rediscovered
in
the
late
15th
century;
for
centuries
it
epitomized
the
ideals
of
Classical
Antiquity
for
Europeans,
from
the
Renaissance
through
the
nineteenth
century.
The
marble
is
a
Hellenistic
or
Roman
copy
of
a
bronze
original
by
the
Greek
sculptor
Leochares,
made
between
350
and
325
BC.
The
lifesize
so-called
"Adonis"
(shown
at
left)
found
in
1780
on
the
site
of
a
"villa
suburbana"
near
the
Via
Labicana
in
the
Roman
suburb
of
Centocelle
and
now
in
the
Ashmolean
Museum,
Oxford,
is
identified
as
an
Apollo
by
modern
scholars.
It
was
probably
never
intended
as
a
cult
object,
but
was
a
pastiche
of
several
fourth-century
and
later
Hellenistic
model
types,
intended
to
please
a
Roman
connoisseur
of
the
second
century
AD,
and
to
be
displayed
in
his
villa.
In
the
late
second
century
CE
floor
mosaic
from
El
Djem,
Roman
"Thysdrus"
(right),
he
is
identifiable
as
Apollo
Helios
by
his
effulgent
halo,
though
now
even
a
god's
divine
nakedness
is
concealed
by
his
cloak,
a
mark
of
increasing
conventions
of
modesty
in
the
later
Empire.
Another
haloed
Apollo
in
mosaic,
from
Hadrumentum,
is
in
the
museum
at
Sousse.
The
conventions
of
this
representation,
head
tilted,
lips
slightly
parted,
large-eyed,
curling
hair
cut
in
locks
grazing
the
neck,
were
developed
in
the
third
century
BCE
to
depict
Alexander
the
Great
(Bieber
1964,
Yalouris
1980).
Some
time
after
this
mosaic
was
executed,
the
earliest
depictions
of
Christ
will
be
beardless
and
haloed.
Birth.
When
Hera
discovered
that
Leto
was
pregnant
and
that
Zeus
was
the
father,
she
banned
Leto
from
giving
birth
on
"terra
firma",
or
the
mainland,
or
any
island.
In
her
wanderings,
Leto
found
the
newly
created
floating
island
of
Delos,
which
was
neither
mainland
nor
a
real
island,
so
she
gave
birth
there.
The
island
was
surrounded
by
swans.
Afterwards,
Zeus
secured
Delos
to
the
bottom
of
the
ocean.
This
island
later
became
sacred
to
Apollo.
It
is
also
stated
that
Hera
kidnapped
Ilithyia,
the
goddess
of
childbirth,
to
prevent
Leto
from
going
into
labor.
The
other
gods
tricked
Hera
into
letting
her
go
by
offering
her
a
necklace,
nine
yards
(8
m)
long,
of
amber.
Mythographers
agree
that
Artemis
was
born
first
and
then
assisted
with
the
birth
of
Apollo,
or
that
Artemis
was
born
one
day
before
Apollo,
on
the
island
of
Ortygia
and
that
she
helped
Leto
cross
the
sea
to
Delos
the
next
day
to
give
birth
to
Apollo.
Apollo
was
born
on
the
seventh
day
()
of
the
month
Thargelion
—according
to
Delian
tradition—
or
of
the
month
Bysios—
according
to
Delphian
tradition.
The
seventh
and
twentieth,
the
days
of
the
new
and
full
moon,
were
ever
afterwards
held
sacred
to
him.
Youth.
Four
days
after
his
birth,
Apollo
killed
the
chthonic
dragon
Python,
which
lived
in
Delphi
beside
the
Castalian
Spring.
This
was
the
spring
which
emitted
vapors
that
caused
the
oracle
at
Delphi
to
give
her
prophesies.
Hera
sent
the
serpent
to
hunt
Leto
to
her
death
across
the
world.
In
order
to
protect
his
mother,
Apollo
begged
Hephaestus
for
a
bow
and
arrows.
After
receiving
them,
Apollo
cornered
Python
in
the
sacred
cave
at
Delphi.
Apollo
killed
Python
but
had
to
be
punished
for
it,
since
Python
was
a
child
of
Gaia.
Hera
then
sent
the
giant
Tityos
to
kill
Leto.
This
time
Apollo
was
aided
by
his
sister
Artemis
in
protecting
their
mother.
During
the
battle
Zeus
finally
relented
his
aid
and
hurled
Tityos
down
to
Tartarus.
There
he
was
pegged
to
the
rock
floor,
covering
an
area
of,
where
a
pair
of
vultures
feasted
daily
on
his
liver.
Admetus.
When
Zeus
struck
down
Apollo's
son
Asclepius
with
a
lightning
bolt
for
resurrecting
Hippolytus
from
the
dead
(transgressing
Themis
by
stealing
Hades's
subjects),
Apollo
in
revenge
killed
the
Cyclopes,
who
had
fashioned
the
bolt
for
Zeus.
Apollo
would
have
been
banished
to
Tartarus
forever,
but
was
instead
sentenced
to
one
year
of
hard
labor
as
punishment,
thanks
to
the
intercession
of
his
mother,
Leto.
During
this
time
he
served
as
shepherd
for
King
Admetus
of
Pherae
in
Thessaly.
Admetus
treated
Apollo
well,
and,
in
return,
the
god
conferred
great
benefits
on
Admetus.
Apollo
helped
Admetus
win
Alcestis,
the
daughter
of
King
Pelias
and
later
convinced
the
Fates
to
let
Admetus
live
past
his
time,
if
another
took
his
place.
But
when
it
came
time
for
Admetus
to
die,
his
parents,
whom
he
had
assumed
would
gladly
die
for
him,
refused
to
cooperate.
Instead,
Alcestis
took
his
place,
but
Heracles
managed
to
"persuade"
Thanatos,
the
god
of
death,
to
return
her
to
the
world
of
the
living.
Trojan
War.
Apollo
shot
arrows
infected
with
the
plague
into
the
Greek
encampment
during
the
Trojan
War
in
retribution
for
Agamemnon's
insult
to
Chryses,
a
priest
of
Apollo
whose
daughter
Chryseis
had
been
captured.
He
demanded
her
return,
and
the
Achaeans
complied,
indirectly
causing
the
anger
of
Achilles,
which
is
the
theme
of
the
"Iliad".
When
Diomedes
injured
Aeneas
("Iliad"),
Apollo
rescued
him.
First,
Aphrodite
tried
to
rescue
Aeneas
but
Diomedes
injured
her
as
well.
Aeneas
was
then
enveloped
in
a
cloud
by
Apollo,
who
took
him
to
Pergamos,
a
sacred
spot
in
Troy.
Apollo
aided
Paris
in
the
killing
of
Achilles
by
guiding
the
arrow
of
his
bow
into
Achilles'
heel.
One
interpretation
of
his
motive
is
that
it
was
in
revenge
for
Achilles'
sacrilege
in
murdering
Troilus,
the
god's
own
son
by
Hecuba,
on
the
very
altar
of
the
god's
own
temple.
Niobe.
The
queen
of
Thebes
and
wife
of
Amphion,
Niobe
boasted
of
her
superiority
to
Leto
because
she
had
fourteen
children
(Niobids),
seven
male
and
seven
female,
while
Leto
had
only
two.
Apollo
killed
her
sons
as
they
practiced
athletics,
with
the
last
begging
for
his
life,
and
Artemis
her
daughters.
Apollo
and
Artemis
used
poisoned
arrows
to
kill
them,
though
according
to
some
versions
of
the
myth,
a
number
of
the
Niobids
were
spared
(Chloris,
usually).
Amphion,
at
the
sight
of
his
dead
sons,
either
killed
himself
or
was
killed
by
Apollo
after
swearing
revenge.
A
devastated
Niobe
fled
to
Mount
Sipylos
in
Asia
Minor
and
turned
into
stone
as
she
wept.
Her
tears
formed
the
river
Achelous.
Zeus
had
turned
all
the
people
of
Thebes
to
stone
and
so
no
one
buried
the
Niobids
until
the
ninth
day
after
their
death,
when
the
gods
themselves
entombed
them.
Consorts
and
children.
Love
affairs
ascribed
to
Apollo
are
a
late
development
in
Greek
mythology.
Their
vivid
anecdotal
qualities
have
made
favorites
some
of
them
of
painters
since
the
Renaissance,
so
that
they
stand
out
more
prominently
in
the
modern
imagination.
Female
lovers.
In
explanation
of
the
connection
of
Apollon
with
"daphne",
the
Laurel
whose
leaves
his
priestess
employed
at
Delphi,
it
was
told
by
Libanius,
a
fourth-century
CE
teacher
of
rhetoric,
that
Apollo
chased
a
nymph,
Daphne,
daughter
of
Peneus,
who
had
scorned
him.
In
Ovid's
telling
for
a
Roman
audience,
Phoebus
Apollo
chaffs
Cupid
for
toying
with
a
man's
weapon
suited
to
a
man,
whereupon
Cupid
wounds
him
with
an
arrow
with
a
golden
dart;
simultaneously,
however,
Eros
had
shot
a
leaden
arrow
into
Daphne,
causing
her
to
be
repulsed
by
Apollo.
Following
a
spirited
chase
by
Apollo,
Daphne
prayed
to
Mother
Earth,
or,
alternatively,
her
father
—
a
river
god
—
to
help
her
and
he
changed
her
into
the
Laurel
tree,
sacred
to
Apollo.
Apollo
had
an
affair
with
a
human
princess
named
Leucothea,
daughter
of
Orchamus
and
sister
of
Clytia.
Leucothea
loved
Apollo
who
disguised
himself
as
Leucothea's
mother
to
gain
entrance
to
her
chambers.
Clytia,
jealous
of
her
sister
because
she
wanted
Apollo
for
herself,
told
Orchamus
the
truth,
betraying
her
sister's
trust
and
confidence
in
her.
Enraged,
Orchamus
ordered
Leucothea
to
be
buried
alive.
Apollo
refused
to
forgive
Clytia
for
betraying
his
beloved,
and
a
grieving
Clytia
wilted
and
slowly
died.
Apollo
changed
her
into
an
incense
plant,
either
heliotrope
or
sunflower,
which
follows
the
sun
every
day.
Marpessa
was
kidnapped
by
Idas
but
was
loved
by
Apollo
as
well.
Zeus
made
her
choose
between
them,
and
she
chose
Idas
on
the
grounds
that
Apollo,
being
immortal,
would
tire
of
her
when
she
grew
old.
Castalia
was
a
nymph
whom
Apollo
loved.
She
fled
from
him
and
dived
into
the
spring
at
Delphi,
at
the
base
of
Mt.
Parnassos,
which
was
then
named
after
her.
Water
from
this
spring
was
sacred;
it
was
used
to
clean
the
Delphian
temples
and
inspire
poets.
By
Cyrene,
Apollo
had
a
son
named
Aristaeus,
who
became
the
patron
god
of
cattle,
fruit
trees,
hunting,
husbandry
and
bee-keeping.
He
was
also
a
culture-hero
and
taught
humanity
dairy
skills
and
the
use
of
nets
and
traps
in
hunting,
as
well
as
how
to
cultivate
olives.
With
Hecuba,
wife
of
King
Priam
of
Troy,
Apollo
had
a
son
named
Troilus.
An
oracle
prophesied
that
Troy
would
not
be
defeated
as
long
as
Troilus
reached
the
age
of
twenty
alive.
He
was
ambushed
and
killed
by
Achilles.
Apollo
also
fell
in
love
with
Cassandra,
daughter
of
Hecuba
and
Priam,
and
Troilus'
half-sister.
He
promised
Cassandra
the
gift
of
prophecy
to
seduce
her,
but
she
rejected
him
afterwards.
Enraged,
Apollo
indeed
gifted
her
with
the
ability
to
know
the
future,
with
a
curse
that
she
could
only
see
the
future
tragedies
and
that
no
one
would
ever
believe
her.
Coronis,
daughter
of
Phlegyas,
King
of
the
Lapiths,
was
another
of
Apollo's
liaisons.
Pregnant
with
Asclepius,
Coronis
fell
in
love
with
Ischys,
son
of
Elatus.
A
crow
informed
Apollo
of
the
affair.
When
first
informed
he
disbelieved
the
crow
and
turned
all
crows
black
(where
they
were
previously
white)
as
a
punishment
for
spreading
untruths.
When
he
found
out
the
truth
he
sent
his
sister,
Artemis,
to
kill
Coronis
(in
other
stories,
Apollo
himself
had
killed
Coronis).
As
a
result
he
also
made
the
crow
sacred
and
gave
them
the
task
of
announcing
important
deaths.
Apollo
rescued
the
baby
and
gave
it
to
the
centaur
Chiron
to
raise.
Phlegyas
was
irate
after
the
death
of
his
daughter
and
burned
the
Temple
of
Apollo
at
Delphi.
Apollo
then
killed
him
for
what
he
did.
In
Euripides'
play
"Ion",
Apollo
fathered
Ion
by
Creusa,
wife
of
Xuthus.
Creusa
left
Ion
to
die
in
the
wild,
but
Apollo
asked
Hermes
to
save
the
child
and
bring
him
to
the
oracle
at
Delphi,
where
he
was
raised
by
a
priestess.
One
of
his
other
liaisons
was
with
Acantha,
the
spirit
of
the
acanthus
tree.
Upon
her
death,
Apollo
transformed
her
into
a
sun-loving
herb.
According
to
the
"Biblioteca",
or
"library"
of
mythology
mis-attributed
to
Apollodorus,
he
fathered
the
Corybantes
on
the
Muse
Thalia.
Male
lovers.
Hyacinth
(or
Hyacinthus)
was
one
of
his
male
lovers.
Hyacinthus
was
a
Spartan
prince,
beautiful
and
athletic.
The
pair
were
practicing
throwing
the
discus
when
a
discus
thrown
by
Apollo
was
blown
off
course
by
the
jealous
Zephyrus
and
struck
Hyacinthus
in
the
head,
killing
him
instantly.
Apollo
is
said
to
be
filled
with
grief:
out
of
Hyacinthus'
blood,
Apollo
created
a
flower
named
after
him
as
a
memorial
to
his
death,
and
his
tears
stained
the
flower
petals
with
"άί"
"άί",
meaning
alas.
The
Festival
of
Hyacinthus
was
a
celebration
of
Sparta.
Another
male
lover
was
Cyparissus,
a
descendant
of
Heracles.
Apollo
gave
him
a
tame
deer
as
a
companion
but
Cyparissus
accidentally
killed
it
with
a
javelin
as
it
lay
asleep
in
the
undergrowth.
Cyparissus
asked
Apollo
to
let
his
tears
fall
forever.
Apollo
granted
the
request
by
turning
him
into
the
Cypress
named
after
him,
which
was
said
to
be
a
sad
tree
because
the
sap
forms
droplets
like
tears
on
the
trunk.
Birth
of
Hermes.
Hermes
was
born
on
Mount
Cyllene
in
Arcadia.
The
story
is
told
in
the
Homeric
Hymn
to
Hermes.
His
mother,
Maia,
had
been
secretly
impregnated
by
Zeus.
Maia
wrapped
the
infant
in
blankets
but
Hermes
escaped
while
she
was
asleep.
Hermes
ran
to
Thessaly,
where
Apollo
was
grazing
his
cattle.
The
infant
Hermes
stole
a
number
of
his
cows
and
took
them
to
a
cave
in
the
woods
near
Pylos,
covering
their
tracks.
In
the
cave,
he
found
a
tortoise
and
killed
it,
then
removed
the
insides.
He
used
one
of
the
cow's
intestines
and
the
tortoise
shell
and
made
the
first
lyre.
Apollo
complained
to
Maia
that
her
son
had
stolen
his
cattle,
but
Hermes
had
already
replaced
himself
in
the
blankets
she
had
wrapped
him
in,
so
Maia
refused
to
believe
Apollo's
claim.
Zeus
intervened
and,
claiming
to
have
seen
the
events,
sided
with
Apollo.
Hermes
then
began
to
play
music
on
the
lyre
he
had
invented.
Apollo,
a
god
of
music,
fell
in
love
with
the
instrument
and
offered
to
allow
exchange
of
the
cattle
for
the
lyre.
Hence,
Apollo
became
a
master
of
the
lyre.
Other
stories.
Apollo
gave
the
order
through
the
Oracle
at
Delphi,
for
Orestes
to
kill
his
mother,
Clytemnestra,
and
her
lover,
Aegisthus.
Orestes
was
punished
fiercely
by
the
Erinyes
(the
Furies,
female
personifications
of
vengeance)
for
this
crime.
Relentlessly
pursued
by
the
Furies,
Orestes
asked
for
the
intercession
of
Athena,
who
decreed
that
he
be
tried
by
a
jury
of
his
peers,
with
Apollo
acting
as
his
advocate.
In
the
Odyssey,
Odysseus
and
his
surviving
crew
landed
on
an
island
sacred
to
Helios
the
sun
god,
where
he
kept
sacred
cattle.
Though
Odysseus
warned
his
men
not
to
(as
Tiresias
and
Circe
had
told
him),
they
killed
and
ate
some
of
the
cattle
and
Helios
had
Zeus
destroy
the
ship
and
all
the
men,
except
Odysseus.
Apollo
also
had
a
lyre-playing
contest
with
Cinyras,
his
son,
who
committed
suicide
when
he
lost.
Apollo
killed
the
Aloadae
when
they
attempted
to
storm
Mt.
Olympus.
Callimachus
sang
that
Apollo
rode
on
the
back
of
a
swan
to
the
land
of
the
Hyperboreans
during
the
winter
months.
Apollo
turned
Cephissus
into
a
sea
monster.
Pan.
Once
Pan
had
the
audacity
to
compare
his
music
with
that
of
Apollo,
and
to
challenge
Apollo,
the
god
of
the
kithara,
to
a
trial
of
skill.
Tmolus,
the
mountain-god,
was
chosen
to
umpire.
Pan
blew
on
his
pipes,
and
with
his
rustic
melody
gave
great
satisfaction
to
himself
and
his
faithful
follower,
Midas,
who
happened
to
be
present.
Then
Apollo
struck
the
strings
of
his
lyre.
Tmolus
at
once
awarded
the
victory
to
Apollo,
and
all
but
Midas
agreed
with
the
judgment.
He
dissented,
and
questioned
the
justice
of
the
award.
Apollo
would
not
suffer
such
a
depraved
pair
of
ears
any
longer,
and
caused
them
to
become
the
ears
of
a
donkey.
Marsyas.
Apollo
has
ominous
aspects
aside
from
his
plague-bringing,
death-dealing
arrows:
Marsyas
was
a
satyr
who
challenged
Apollo
to
a
contest
of
music.
He
had
found
an
aulos
on
the
ground,
tossed
away
after
being
invented
by
Athena
because
it
made
her
cheeks
puffy.
The
contest
was
judged
by
the
Muses.
After
they
each
performed,
both
were
deemed
equal
until
Apollo
decreed
they
play
and
sing
at
the
same
time.
As
Apollo
played
the
lyre,
this
was
easy
to
do.
Marsyas
could
not
do
this
as
he
only
knew
how
to
use
the
flute
and
could
not
sing
at
the
same
time.
Apollo
was
declared
the
winner
because
of
this.
Apollo
flayed
Marsyas
alive
in
a
cave
near
Celaenae
in
Phrygia
for
his
hubris
to
challenge
a
god.
He
then
nailed
Marsyas'
shaggy
skin
to
a
nearby
pine-tree.
Marsyas'
blood
turned
into
the
river
Marsyas.
Another
variation
is
that
Apollo
played
his
instrument
(the
lyre)
upside
down.
Marsyas
could
not
do
this
with
his
instrument
(the
flute),
and
so
Apollo
hung
him
from
a
tree
and
flayed
him
alive.
Graeco–Roman
epithets
and
cult
titles.
Apollo,
like
other
Greek
deities,
had
a
number
of
epithets
applied
to
him,
reflecting
the
variety
of
roles,
duties,
and
aspects
ascribed
to
the
god.
However,
while
Apollo
has
a
great
number
of
appellations
in
Greek
myth,
only
a
few
occur
in
Latin
literature,
chief
among
them
Phoebus
("shining
one"),
which
was
very
commonly
used
by
both
the
Greeks
and
Romans
in
Apollo's
role
as
the
god
of
light.
In
Apollo's
role
as
healer,
his
appellations
included
Akesios,
Iatros,
and
Acestor
meaning
"healer".
He
was
also
called
Alexicacus
("restrainer
of
evil")
and
Apotropaeus
("he
who
averts
evil"),
and
was
referred
to
by
the
Romans
as
Averruncus
("averter
of
evils").
As
a
plague
god
and
defender
against
rats
and
locusts,
Apollo
was
known
as
Smintheus
("mouse-catcher")
and
Parnopius
("grasshopper").
The
Romans
also
called
Apollo
Culicarius
("driving
away
midges").
In
his
healing
aspect,
the
Romans
referred
to
Apollo
as
Medicus
("the
Physician"),
and
a
temple
was
dedicated
to
"Apollo
Medicus"
at
Rome,
probably
next
to
the
temple
of
Bellona.
As
a
sun-god
he
was
worshiped
as
Aegletes,
the
radiant
god.
As
a
god
of
archery,
Apollo
was
known
as
Aphetoros
("god
of
the
bow")
and
Argurotoxos
("with
the
silver
bow").
The
Romans
referred
to
Apollo
as
Articenens
("carrying
the
bow")
as
well.
As
a
pastoral
shepherd-god,
Apollo
was
known
as
Nomios
("wandering").
As
the
protector
of
roads
and
homes
he
was
Agyieus.
Apollo
was
also
known
as
Archegetes
("director
of
the
foundation"),
who
oversaw
colonies.
He
was
known
as
Klarios,
from
the
Doric
"klaros"
("allotment
of
land"),
for
his
supervision
over
cities
and
colonies.
He
was
known
as
Delphinios
("Delphinian"),
meaning
"of
the
womb",
in
his
association
with
"Delphoi"
(Delphi).
At
Delphi,
he
was
also
known
as
Pythios
("Pythian").
An
aitiology
in
the
Homeric
hymns
connects
the
epitheton
to
dolphins.
Kynthios,
another
common
epithet,
stemmed
from
his
birth
on
Mt.
Cynthus.
He
was
also
known
as
Lyceios
or
Lykegenes,
which
either
meant
"wolfish"
or
"of
Lycia",
Lycia
being
the
place
where
some
postulate
that
his
cult
originated.
Specifically
as
god
of
prophecy,
Apollo
was
known
as
Loxias
("the
obscure").
He
was
also
known
as
Coelispex
("he
who
watches
the
heavens")
to
the
Romans.
Apollo
was
attributed
the
epithet
Musagetes
as
the
leader
of
the
muses,
and
Nymphegetes
as
"nymph-leader".
Acesius
was
the
epithet
of
Apollo
worshipped
in
Elis,
where
he
had
a
temple
in
the
agora.
This
surname,
which
has
the
same
meaning
as
"akestor"
and
"alexikakos",
characterized
the
god
as
the
averter
of
evil.
Acraephius
or
Acraephiaeus
was
his
epithet
worshipped
in
the
Boeotian
town
of
Acraephia,
reputedly
founded
by
his
son,
Acraepheus.
Actiacus
was
his
epithet
in
Actium,
one
of
the
principal
places
of
his
worship.
Celtic
epithets
and
cult
titles.
Apollo
was
worshipped
throughout
the
Roman
Empire.
In
the
traditionally
Celtic
lands
he
was
most
often
seen
as
a
healing
and
sun
god.
He
was
often
equated
with
Celtic
gods
of
similar
character.
Reception.
Apollo
has
often
featured
in
postclassical
art
and
literature.
Percy
Bysshe
Shelley
composed
a
"Hymn
of
Apollo"
(1820),
and
the
god's
instruction
of
the
Muses
formed
the
subject
of
Igor
Stravinsky's
"Apollon
musagète"
(1927–1928).
The
name
"Apollo"
was
given
to
NASA's
Apollo
Lunar
program
in
the
1960s.
The
statue
of
Apollo
from
the
west
pediment
of
the
Temple
of
Zeus
at
Olympia
(currently
in
the
Archaeological
Museum
of
Olympia)
was
depicted
on
the
obverse
of
the
Greek
1000
drachmas
banknote
of
1987–2001.
---END.OF.DOCUMENT---
Andre
Agassi.
Andre
Kirk
Agassi
(;
born
April
29,
1970)
is
an
American
former
World
No.
1
professional
tennis
player
who
won
eight
Grand
Slam
singles
tournaments
and
an
Olympic
gold
medal
in
singles.
Generally
considered
by
critics
and
fellow
players
to
be
one
of
the
greatest
tennis
players
of
all
time,
he
has
been
called
the
best
service
returner
in
the
history
of
tennis.
Known
for
his
unorthodox
apparel
and
attitude,
Agassi
is
often
cited
as
one
of
the
most
charismatic
players
in
the
history
of
the
game,
and
is
credited
for
helping
revive
the
popularity
of
tennis
during
the
1990s.
He
is
married
to
fellow
retired
professional
tennis
player
and
multiple
Grand
Slam
champion
Steffi
Graf.
Agassi
is,
with
Rod
Laver,
Don
Budge,
Fred
Perry,
Roy
Emerson,
and
Roger
Federer,
one
of
only
six
men
to
have
achieved
a
Career
Grand
Slam,
one
of
only
three
(with
Laver
and
Federer)
since
the
beginning
of
the
Open
Era,
and
the
only
male
player
to
have
achieved
a
Career
Golden
Slam.
In
addition
to
his
Grand
Slam
and
Olympic
singles
titles,
he
won
the
Tennis
Masters
Cup
and
was
part
of
a
winning
Davis
Cup
team.
He
won
17
ATP
Masters
Series
tournaments,
more
than
any
other
player.
Agassi's
Grand
Slam
composition
is
(4
Australian
Open,
1
French
Open,
1
Wimbledon,
2
US
Open)
for
his
career.
After
suffering
from
sciatica
caused
by
two
bulging
discs
in
his
back,
a
spondylolisthesis
(vertebral
displacement)
and
a
bone
spur
that
interferes
with
the
nerve,
Agassi
retired
from
professional
tennis
on
September
3,
2006,
after
losing
in
the
third
round
of
the
US
Open.
He
is
the
founder
of
the
Andre
Agassi
Charitable
Foundation,
which
has
raised
over
$60
million
for
at-risk
children
in
Southern
Nevada.
In
2001,
the
Foundation
opened
the
Andre
Agassi
College
Preparatory
Academy
in
Las
Vegas,
a
K-12
public
charter
school
for
at-risk
children.
1970–1985:
Early
life.
Agassi
was
born
in
Las
Vegas,
Nevada,
to
Emmanuel
"Mike"
Aghassian
and
Elizabeth
"Betty"
Agassi
(née
Dudley).
His
father
is
an
Iranian
of
Armenian
and
Assyrian
ethnicity
who
represented
Iran
in
boxing
at
the
1948
and
1952
Olympic
Games
before
emigrating
to
the
United
States.
Andre
Agassi's
mother,
Betty,
is
a
breast
cancer
survivor.
Mike
Agassi
was
renowned
for
his
domineering
nature,
reportedly
taking
a
hammer
to
matches
and
banging
on
the
fences
in
disgust
when
Andre
lost
a
point.
He
sometimes
screamed
at
officials
and
was
ejected
more
than
once.
At
age
13,
Andre
was
sent
to
Nick
Bollettieri's
Tennis
Academy
in
Florida.
He
was
meant
to
stay
for
only
3
months
because
that
was
all
his
father
could
afford.
However,
after
ten
minutes
of
watching
Agassi
rally,
Bollettieri
called
Mike
and
said:
"Take
your
check
back.
He's
here
for
free,"
claiming
that
Agassi
had
more
natural
talent
than
anyone
else
he
had
seen.
1986–1993.
He
turned
professional
at
the
age
of
16
and
his
first
tournament
was
in
La
Quinta,
California.
He
won
his
first
match
against
John
Austin
6–4,
6–2
but
then
lost
his
second
match
to
Mats
Wilander
6–1,
6–1.
By
the
end
of
the
year,
Agassi
was
ranked
World
No.
91.
Agassi
won
his
first
top-level
singles
title
in
1987
at
the
Sul
American
Open
in
Itaparica.
He
ended
the
year
ranked
World
No.
25.
He
won
six
additional
tournaments
in
1988
(Memphis,
U.S.
Men's
Clay
Court
Championships,
Forest
Hills
WCT,
Stuttgart
Outdoor,
Volvo
International
and
Livingston
Open),
and,
by
December
of
that
year,
he
had
surpassed
US$2
million
in
career
prize
money
after
playing
in
just
43
tournaments
–
the
fastest
anyone
in
history
had
reached
that
level.
His
year-end
ranking
was
World
No.
3,
behind
second-ranked
Ivan
Lendl
and
top-ranked
Mats
Wilander.
Both
the
Association
of
Tennis
Professionals
and
"Tennis"
magazine
named
Agassi
the
Most
Improved
Player
of
the
Year
for
1988.
In
addition
to
not
playing
the
Australian
Open
(which
would
later
become
his
best
Grand
Slam
event)
for
the
first
eight
years
of
his
career,
Agassi
chose
not
to
play
at
Wimbledon
from
1988
through
1990
and
publicly
stated
that
he
did
not
wish
to
play
there
because
of
the
event's
traditionalism,
particularly
its
"predominantly
white"
dress
code
to
which
players
at
the
event
are
required
to
conform.
Strong
performances
on
the
tour
meant
that
Agassi
was
quickly
tipped
as
a
future
Grand
Slam
champion.
While
still
a
teenager,
he
reached
the
semi-finals
of
both
the
French
Open
and
the
US
Open
in
1988,
and
made
the
US
Open
semifinals
in
1989.
He
began
the
1990s,
however,
with
a
series
of
near-misses.
He
reached
his
first
Grand
Slam
final
in
1990
at
the
French
Open,
where
he
was
favored
before
losing
in
four
sets
to
Andrés
Gómez.
He
reached
his
second
Grand
Slam
final
of
the
year
at
the
US
Open,
defeating
defending
champion
Boris
Becker
in
the
semifinals.
His
opponent
in
the
final
was
Pete
Sampras;
a
year
earlier,
Agassi
had
beaten
Sampras
6-2,
6-1
after
which
he
told
his
coach
that
he
felt
bad
for
Sampras
because
he
was
never
going
to
make
it
as
a
pro.
Agassi
lost
the
US
Open
final
to
Sampras
6–4,
6–3,
6–2.
The
rivalry
between
these
two
American
players
became
the
dominant
rivalry
in
tennis
over
the
rest
of
the
decade.
Also
in
1990,
Agassi
helped
the
United
States
win
its
first
Davis
Cup
in
8
years
and
won
his
only
Tennis
Masters
Cup,
beating
reigning
Wimbledon
champion
Stefan
Edberg
in
the
final.
In
1991,
Agassi
reached
his
second
consecutive
French
Open
final,
where
he
faced
fellow
Bollettieri
Academy
alumnus
Jim
Courier.
Courier
emerged
the
victor
in
a
five
set
final.
Agassi
decided
to
play
at
Wimbledon
in
1991,
leading
to
weeks
of
speculation
in
the
media
about
the
clothes
he
would
wear.
He
eventually
emerged
for
the
first
round
in
a
completely
white
outfit.
He
went
on
to
reach
the
quarter-finals
on
that
occasion,
losing
in
five
sets
to
David
Wheaton.
Agassi's
Grand
Slam
tournament
breakthrough
came
at
Wimbledon,
not
at
the
French
Open
or
the
US
Open
where
he
had
previously
enjoyed
success.
In
1992,
he
defeated
Goran
Ivanišević
in
a
five
set
final.
Along
the
way,
Agassi
overcame
two
former
Wimbledon
champions
in
Boris
Becker
and
John
McEnroe.
No
other
baseliner
would
triumph
at
Wimbledon
until
Lleyton
Hewitt
ten
years
later.
Agassi
was
named
the
BBC
Overseas
Sports
Personality
of
the
Year
in
1992.
Agassi
once
again
played
on
the
United
States'
Davis
Cup
winning
team
in
1992.
It
was
their
second
Davis
cup
title
in
three
years.
1993
saw
Agassi
win
the
only
doubles
title
of
his
career,
at
the
Cincinnati
Masters,
partnered
with
Petr
Korda.
Agassi
missed
much
of
the
early
part
of
that
year
with
injuries.
Although
he
made
the
quarterfinals
in
his
Wimbledon
title
defense,
he
lost
to
eventual
champion
and
World
number
one
Pete
Sampras
in
five-sets.
Agassi
lost
in
the
first-round
at
the
US
Open
to
Thomas
Enqvist
and
required
wrist
surgery
late
in
the
year.
1994–1997.
With
new
coach
Brad
Gilbert
on
board,
Agassi
began
to
employ
more
of
a
tactical,
consistent
approach,
which
fueled
his
resurgence.
Agassi
started
slowly
in
1994,
losing
in
the
first
week
at
the
French
Open
and
Wimbledon.
Nevertheless,
Agassi
emerged
during
the
hard
court
season,
winning
the
Canadian
Open.
His
comeback
culminated
at
the
1994
US
Open
with
a
5-set
fourth-round
victory
against
compatriot
Michael
Chang
and
then
becoming
the
first
man
to
capture
the
US
Open
as
an
unseeded
player,
beating
Michael
Stich
in
the
final.
In
1995,
Agassi
shaved
his
balding
head,
breaking
with
his
old
"image
is
everything"
style.
He
competed
in
the
1995
Australian
Open
(his
first
appearance
at
the
event)
and
won,
beating
Sampras
in
a
four
set
final.
Agassi
and
Sampras
met
in
five
tournament
finals
in
1995,
all
on
hardcourt,
with
Agassi
winning
three.
Agassi
won
three
Masters
Series
events
in
1995
(Cincinnati,
Key
Biscayne,
and
the
Canadian
Open)
and
seven
titles
total.
He
compiled
a
career-best
26-match
winning
streak
during
the
summer
hardcourt
circuit,
which
ended
when
he
lost
the
US
Open
final
to
Sampras.
Agassi
reached
the
World
No.
1
ranking
for
the
first
time
in
April
1995.
He
held
that
ranking
until
November,
for
a
total
of
30
weeks.
In
terms
of
win/loss
record,
1995
was
Agassi's
best
year.
He
won
73
matches
and
lost
only
9.
Agassi
was
also
once
again
a
key
player
on
the
United
States'
Davis
Cup
winning
team
-
the
third
and
final
Davis
Cup
title
of
Agassi's
career.
1996
was
a
less
successful
year
for
Agassi,
as
he
failed
to
reach
any
Grand
Slam
final.
He
suffered
two
early
round
losses
at
the
hands
of
compatriots
Chris
Woodruff
and
Doug
Flach
at
the
French
Open
and
Wimbledon,
respectively,
and
lost
to
Chang
in
straight
sets
in
the
Australian
and
US
Open
semifinals.
At
the
time,
Agassi
blamed
the
loss
on
the
windy
conditions
but
later
admitted
in
his
biography
that
he
had
tanked
(lost
on
purpose)
this
match
as
he
bore
a
grudge
against
Boris
Becker
whom
he
would
have
faced
in
the
final.
The
high
point
for
Agassi
was
winning
the
men's
singles
gold
medal
at
the
Olympic
Games
in
Atlanta,
beating
Sergi
Bruguera
of
Spain
in
the
final
6–2,
6–3,
6–1.
Agassi
also
successfully
defended
his
singles
titles
in
Cincinnati
and
Key
Biscayne.
1997
was
the
low
point
of
Agassi's
career.
His
wrist
injury
resurfaced,
and
he
played
only
24
matches
during
the
year.
He
would
later
confess
that
he
started
using
crystal
methamphetamine
at
that
time,
allegedly
on
the
urging
of
a
friend.
He
failed
an
ATP
drug
test,
but
wrote
a
letter
claiming
the
same
friend
spiked
a
drink.
The
ATP
dropped
the
failed
drug
test
as
a
warning.
He
stated
upon
admitting
to
his
drug
use
that
the
letter
was
a
lie.
He
quit
the
drug
soon
after.
He
won
no
top-level
titles
and
his
ranking
sank
to
World
No.
141
on
November
10,
1997.
1998–2003.
In
1998,
Agassi
began
a
rigorous
conditioning
program
and
worked
his
way
back
up
the
rankings
by
playing
in
Challenger
Series
tournaments
(a
circuit
for
professional
players
ranked
outside
the
world's
top
50).
He
played
some
classic
matches
in
this
period,
most
notably
against
his
rival
Pete
Sampras
and
popular
Australian
Patrick
Rafter.
In
1998,
Agassi
won
five
titles
and
leapt
from
World
No.
122
at
the
start
of
the
year
to
World
No.
6
at
the
end
of
it,
making
it
the
highest
jump
into
the
top
10
made
by
any
player
during
a
single
calendar
year.
At
Wimbledon
that
year,
he
had
an
early
loss
in
the
second
round
to
ATP
player
Tommy
Haas.
He
won
five
titles
in
ten
finals
and
was
runner-up
at
the
Masters
Series
tournament
in
Key
Biscayne,
losing
to
Marcelo
Ríos,
who
became
World
No.
1
as
a
result
of
winning
that
tournament.
Agassi
entered
the
history
books
in
1999
when
he
came
back
from
two
sets
to
love
down
to
beat
Andrei
Medvedev
in
a
five-set
French
Open
final,
thereby
becoming,
at
the
time,
only
the
fifth
male
player
(joining
Rod
Laver,
Fred
Perry,
Roy
Emerson
and
Don
Budge-these
have
since
been
joined
by
a
sixth,
Roger
Federer)
to
have
won
all
four
Grand
Slam
singles
titles
during
his
career.
This
win
also
made
him
the
first
(of
only
two,
the
second
being
Roger
Federer)
male
players
in
history
to
have
won
all
four
Grand
Slam
titles
on
three
different
surfaces
(clay,
grass,
and
hard
courts),
a
tribute
to
his
adaptability,
as
the
other
four
men
had
won
their
Grand
Slam
titles
on
clay
and
grass
courts.
Agassi
also
became
the
first
male
player
to
win
the
Career
Golden
Slam,
consisting
of
all
four
Grand
Slam
tournaments
plus
an
Olympic
gold
medal.
Agassi
followed
his
1999
French
Open
victory
by
reaching
the
Wimbledon
final,
where
he
lost
to
Sampras
in
straight
sets.
He
rebounded
from
his
Wimbledon
defeat
by
winning
the
US
Open,
beating
Todd
Martin
in
five
sets
(rallying
from
a
2
sets
to
1
deficit)
in
the
final.
Agassi
ended
1999
as
the
World
No.
1,
ending
Sampras's
record
of
six
consecutive
year-ending
top
rankings
(1993–1998).
This
was
the
only
time
Agassi
ended
the
year
at
number
one.
Agassi
began
the
next
year
by
capturing
his
second
Australian
Open
title,
beating
Sampras
in
a
five-set
semifinal
and
Yevgeny
Kafelnikov
in
a
four-set
final.
He
was
the
first
male
player
to
have
reached
four
consecutive
Grand
Slam
finals
since
Rod
Laver
achieved
the
Grand
Slam
in
1969.
At
the
time,
Agassi
was
also
only
the
fourth
player
since
Laver
to
be
the
reigning
champion
of
three
of
four
Grand
Slam
events,
missing
only
the
Wimbledon
title.
2000
also
saw
Agassi
reach
the
semifinals
at
Wimbledon,
where
he
lost
in
five
sets
to
Rafter
in
a
match
considered
by
many
to
be
one
of
the
best
ever
played
at
Wimbledon.
At
the
inaugural
Tennis
Masters
Cup
in
Lisbon,
Agassi
reached
the
final
after
defeating
Marat
Safin
6–3,
6–3
in
the
semifinals
to
end
the
Russian's
hopes
to
become
the
youngest
World
No.
1
in
the
history
of
tennis.
Agassi
then
lost
to
Gustavo
Kuerten
in
the
final,
allowing
Kuerten
to
be
crowned
year-end
World
No.
1.
Agassi
opened
2001
by
successfully
defending
his
Australian
Open
title
with
a
straight-sets
final
win
over
Arnaud
Clément.
Enroute,
he
beat
a
cramping
Rafter
(7–5,
2–6,
6–7,
6–2,
6–3)
in
front
of
a
sell-out
crowd
in
what
turned
out
to
be
the
Aussie's
last
Australian
Open.
At
Wimbledon,
they
met
again
in
the
semifinals,
where
Agassi
lost
another
close
match
to
Rafter,
8–6
in
the
fifth
set.
In
the
quarterfinals
at
the
US
Open,
Agassi
lost
a
3
hour,
33
minute
epic
match
with
Sampras
6–7(7),
7–6(7),
7–6(2),
7–6(5),
with
no
breaks
of
serve
during
the
48-game
match.
Despite
the
setback,
Agassi
finished
2001
ranked
World
No.
3,
becoming
the
only
male
tennis
player
to
finish
a
year
ranked
in
the
top
10
in
three
different
decades
(1980s
-
finishing
World
No.
3
in
1988
and
No.
7
in
1989;
1990s
-
finishing
World
No.
4
in
1990,
No.
10
in
1991,
No.
9
in
1992,
No.
2
in
1994
and
1995,
No.
8
in
1996,
No.
6
in
1998
and
No.
1
in
1999;
2000s
-
finishing
World
No.
6
in
2000,
No.
3
in
2001,
No.
2
in
2002,
No.
4
in
2003,
No.
8
in
2004
and
No.
7
in
2005).
He
also
was
the
oldest
player
(age
31)
to
finish
in
the
top
three
since
32-year
old
Connors
finished
at
World
No.
2
in
1984.
2002
opened
with
disappointment
for
Agassi,
as
injury
forced
him
to
skip
the
Australian
Open,
where
he
was
a
two-time
defending
champion.
The
last
duel
between
Agassi
and
Sampras
came
in
the
final
of
the
US
Open,
which
Sampras
won
in
four
sets
and
left
Sampras
with
a
20–14
edge
in
their
34
career
meetings.
The
match
proved
to
be
the
last
of
Sampras's
career.
Agassi's
US
Open
finish,
along
with
his
Masters
Series
victories
in
Key
Biscayne,
Rome,
and
Madrid,
helped
him
finish
2002
as
the
oldest
year-end
World
No.
2
at
32
years
and
8
months.
In
2003,
Agassi
won
the
eighth
(and
final)
Grand
Slam
title
of
his
career
at
the
Australian
Open,
where
he
beat
Rainer
Schüttler
in
straight
sets
in
the
final.
In
March,
he
won
his
sixth
career
and
third
consecutive
Key
Biscayne
title,
in
the
process
surpassing
his
wife,
Steffi
Graf,
who
was
a
5-time
winner
of
the
event.
The
final
was
his
18th
straight
win
in
that
tournament,
which
broke
the
previous
record
of
17
set
by
Sampras
from
1993–1995.
(Agassi's
winning
streak
continued
to
20
after
winning
his
first
two
matches
at
the
2004
edition
of
that
tournament
before
bowing
to
Agustín
Calleri.)
With
the
victory,
Agassi
became
the
youngest
(19
years
old)
and
oldest
(32)
winner
of
the
Key
Biscayne
tournament.
On
April
28,
2003,
he
recaptured
the
World
No.
1
ranking
after
a
quarterfinal
victory
over
Xavier
Malisse
at
the
Queen's
Club
Championships
to
become
the
oldest
top
ranked
male
player
since
the
ATP
rankings
began
at
33
years
and
13
days.
He
held
the
World
No.
1
ranking
for
two
weeks
when
Lleyton
Hewitt
took
it
back
on
May
12,
2003.
Agassi
then
recaptured
the
World
No.
1
ranking
once
again
on
June
16,
2003,
which
he
held
for
12
weeks
until
September
7,
2003.
During
his
career,
Agassi
held
the
World
No.
1
ranking
for
a
total
of
101
weeks.
Agassi's
ranking
slipped
when
injuries
forced
him
to
withdraw
from
many
events.
He
did
manage
to
reach
the
US
Open
semifinals,
where
he
lost
to
Juan
Carlos
Ferrero
and
surrendered
his
World
No.
1
ranking
to
Ferrero.
At
the
year-ending
Tennis
Masters
Cup,
Agassi
lost
in
the
final
to
Federer
and
finished
the
year
ranked
World
No.
4.
At
age
33,
he
was
the
oldest
player
to
rank
in
the
top
five
since
Connors,
at
age
35,
was
World
No.
4
in
1987.
2004–2006.
In
2004,
Agassi
won
the
Masters
series
event
in
Cincinnati
to
bring
his
career
total
to
59
top-level
singles
titles
and
a
record
17
ATP
Masters
Series
titles,
having
already
won
seven
of
the
nine
ATP
Masters
tournament—all
except
the
tournaments
in
Monte
Carlo
and
Hamburg.
At
34,
he
became
the
second-oldest
singles
champion
in
Cincinnati
tournament
history
(the
tournament
began
in
1899),
surpassed
only
by
Ken
Rosewall
who
won
the
title
in
1970
at
age
35.
He
finished
the
year
ranked
World
No.
8,
the
oldest
player
to
finish
in
the
top
10
since
the
36-year-old
Connors
was
World
No.
7
in
1988.
Agassi
also
became
only
the
sixth
male
player
during
the
open
era
to
reach
800
career
wins
with
his
first
round
victory
over
Alex
Bogomolov
in
Countrywide
Classic
in
Los
Angeles.
Agassi's
2005
began
with
a
quarterfinal
loss
to
Federer
at
the
Australian
Open.
Agassi
had
several
other
deep
runs
at
tournaments
but
had
to
withdraw
from
several
events
due
to
injury.
He
lost
to
Jarkko
Nieminen
in
the
first
round
of
the
French
Open.
He
won
his
fourth
title
in
Los
Angeles
and
reached
the
final
of
the
Rogers
Cup
before
falling
to
World
No.
2
Rafael
Nadal.
Agassi's
2005
was
defined
by
an
improbable
run
to
the
US
Open
final.
After
beating
Răzvan
Sabău
and
Ivo
Karlović
in
straight
sets
and
Tomáš
Berdych
in
four
sets,
Agassi
won
three
consecutive
five-set
matches
to
advance
to
the
final.
The
most
notable
of
these
matches
was
his
quarterfinal
victory
over
James
Blake,
where
he
rallied
from
two
sets
down
to
win
3–6,
3–6,
6–3,
6–3,
7–6(6).
His
other
five-set
victims
were
Xavier
Malisse
in
the
fourth
round
and
Robby
Ginepri
in
the
semifinals.
In
the
final,
Agassi
faced
Federer,
who
was
seeking
his
second
consecutive
US
Open
title
and
his
sixth
Grand
Slam
title
in
two
years.
Federer
defeated
Agassi
in
four
sets,
although
Agassi
gave
him
a
scare
when
Agassi
was
up
a
break
in
the
third
set
after
splitting
the
first
two
sets.
Before
the
2005
Tennis
Masters
Cup
in
Shanghai,
Agassi
rolled
his
ankle
in
a
racquetball
accident
and
tore
several
ligaments.
He
was
unable
to
walk
for
weeks.
He
nevertheless
committed
to
the
tournament,
in
which
he
was
seeded
third,
and
played
Nikolay
Davydenko
in
his
first
round
robin
match.
Agassi's
movement
was
noticeably
hindered,
particularly
on
his
backhand
return
of
serve,
and
he
lost
in
straight
sets.
He
then
withdrew
from
the
tournament.
Agassi
finished
2005
ranked
World
No.
7,
his
16th
time
in
the
year-end
top
10
rankings,
which
tied
Connors
for
the
most
times
ranked
in
the
top
10
at
year's
end.
In
2005,
Agassi
left
Nike
after
17
years
and
signed
an
endorsement
deal
with
Adidas.
A
major
reason
for
Agassi
leaving
Nike
was
because
Nike
refused
to
donate
to
Agassi's
charities
and
Adidas
was
more
than
happy
to
do
so.
Agassi
had
a
poor
start
to
2006.
He
was
still
recovering
from
an
ankle
injury
and
also
suffering
from
back
and
leg
pain
and
lack
of
match
play.
Agassi
withdrew
from
the
Australian
Open
because
of
the
ankle
injury,
and
his
back
injury
and
other
pains
forced
him
to
withdraw
from
several
other
events,
eventually
skipping
the
entire
clay
court
season,
including
the
French
Open.
This
caused
his
ranking
to
drop
out
of
the
top
10
for
the
last
time.
Agassi
returned
for
the
grass
court
season,
playing
a
tune-up
and
then
Wimbledon.
He
was
defeated
in
the
third
round
by
World
No.
2
(and
eventual
runner-up)
Rafael
Nadal
7–6(5),
6–2,
6–4.
Against
conventions,
Agassi,
the
losing
player,
was
interviewed
on
court
after
the
match.
At
Wimbledon,
Agassi
announced
his
plans
to
retire
following
the
US
Open.
Agassi
played
only
two
events
during
the
summer
hardcourt
season,
with
his
best
result
being
a
quarterfinal
loss
at
the
Countrywide
Classic
in
Los
Angeles
to
Fernando
González
of
Chile
6–4,
3–6,
7–5.
As
a
result,
he
was
unseeded
at
the
US
Open.
Agassi
had
a
short
but
dramatic
run
in
his
final
US
Open.
Because
of
extreme
back
pain,
Agassi
was
forced
to
receive
anti-inflammatory
injections
after
every
match.
After
a
tough
four-set
win
against
Andrei
Pavel,
Agassi
faced
eighth-seeded
Marcos
Baghdatis
in
the
second
round,
who
had
earlier
advanced
to
the
2006
Australian
Open
final
and
Wimbledon
semifinals.
Agassi
won
6–4,
6–4,
3–6,
5–7,
7–5
as
the
younger
Baghdatis
succumbed
to
muscle
cramping
in
the
final
set.
In
his
last
match,
Agassi
fell
to
112th
ranked
big-serving
Benjamin
Becker
of
Germany
in
four
sets.
Agassi
received
an
eight
minute
standing
ovation
from
the
crowd
after
the
match
and
delivered
a
memorable
retirement
speech.
Earnings.
Agassi
earned
more
than
US$
30
million
in
prize-money
during
his
career,
third
only
to
Sampras
and
Federer
to
date.
He
also
earned
more
than
US
$25
million
a
year
through
endorsements,
during
his
career
and
fourth
in
all
sports
at
the
time.
Post
retirement.
Since
retiring
after
the
2006
US
Open,
Agassi
has
participated
in
a
series
of
charity
tournaments
and
continues
his
work
with
his
own
charity.
On
September
5,
2007,
Agassi
was
a
surprise
guest
commentator
for
the
Andy
Roddick/Roger
Federer
US
Open
quarterfinal.
He
played
an
exhibition
match
at
Wimbledon,
teamed
with
his
wife,
Steffi
Graf,
to
play
with
Tim
Henman
and
Kim
Clijsters.
He
will
play
World
Team
Tennis
for
the
Philadelphia
Freedoms
in
the
summer
of
2009
and
played
at
the
Outback
Champions
Series
event
for
the
first
time.
He
played
the
Cancer
Treatment
Centers
of
America
Tennis
Championships
at
Surprise,
Arizona
where
he
reached
the
final
before
bowing
to
eventual
champion
Todd
Martin
who
captured
his
fourth
career
Outback
Champions
Series
win.
On
the
way
to
the
finals,
Agassi
beat
Mikael
Pernfors
in
the
quarterfinals
and
Wayne
Ferreira
in
the
semifinals.
However,
he
clarified
that
he
will
not
be
playing
the
tour
on
a
full-time
basis
as
he
only
played
the
tournament
as
a
favor
to
long-time
friend
Jim
Courier.
Playing
style.
Early
on
in
his
career,
Agassi
would
look
to
end
points
quickly,
typically
by
inducing
a
weak
return
with
a
deep,
hard
shot,
and
then
playing
a
winner
at
an
extreme
angle.
His
return
of
serve,
baseline
game,
and
keen
sense
of
anticipation
were
among
the
best
in
the
game,
and
helped
him
win
the
Wimbledon
title
in
1992.
On
the
rare
occasion
that
he
charged
the
net,
Agassi
liked
to
take
the
ball
in
the
air
and
hit
a
swinging
volley
for
the
winner.
Agassi
continually
put
pressure
on
opponents
with
a
preference
to
taking
the
ball
early
and
was
famously
known
for
swinging
deep
angles
like
a
smoking
backhand
up
the
line.
His
strength
was
dictating
play
from
the
back
of
the
court.
While
growing
up
his
father
and
Nick
Bollettieri
trained
him
in
this
way.
He
was
never
known
for
a
strong
serve,
net
work
or
volleying.
When
in
control
of
a
point,
Agassi
would
often
pass
up
an
opportunity
to
attempt
a
winner
and
hit
a
slightly
more
conservative
shot,
both
to
minimize
his
errors
and
to
make
his
opponent
run
more.
His
penchant
for
running
players
around
point
after
point
has
earned
him
the
nickname
"The
Punisher".
Agassi's
serve
was
never
the
strength
of
his
game,
but
it
improved
steadily
over
the
course
of
his
career,
and
went
from
being
a
liability
to
being
above
average.
He
often
used
his
hard
slice
serve
in
the
deuce
service
box
to
seek
to
send
his
opponent
off
the
court,
followed
by
a
shot
to
the
opponent's
opposite
corner.
Agassi's
service
speed
when
hitting
a
flat
first
serve
would
often
range
between
to.
His
second
serve
however
was
usually
only
in
the
mid
80's.
He
relied
on
a
heavy
kick
serve
for
his
second
serve.
Personal
and
family
life.
Agassi
married
actress
Brooke
Shields
on
April
19,
1997.
In
February
1998,
they
filed
suit
against
"The
National
Enquirer"
claiming
it
printed
"false
and
fabricated"
statements
about
the
couple,
but
the
case
was
dismissed.
The
couple
later
filed
for
divorce,
which
was
granted
on
April
9,
1999.
At
the
1999
French
Open,
Agassi
and
Steffi
Graf
were
the
surprise
champions,
since
he
had
not
won
a
Grand
Slam
title
since
1995
and
she
since
1996.
At
the
winners'
ball,
they
met
each
other
for
the
second
time.
Shortly
after,
they
started
dating.
Graf
retired
after
they
both
reached
the
Wimbledon
final
in
July.
They
were
married
on
October
22,
2001.
Their
son,
Jaden
Gil,
was
born
four
days
later,
October
26.
Their
daughter,
Jaz
Elle,
was
born
on
October
3,
2003.
The
couple
live
in
the
Las
Vegas
area
and
own
several
vacation
homes.
Agassi's
older
sister,
Rita,
was
married
to
tennis
player
Pancho
Gonzales.
In
1995,
when
Gonzales
died
in
Las
Vegas,
Agassi
paid
for
the
funeral.
Long-time
trainer
Gil
Reyes
has
been
called
one
of
Agassi's
closest
friends;
some
have
described
him
as
being
a
"father
figure".
Andre
Agassi's
other
sister,
like
their
mother,
Betty,
is
a
breast
cancer
survivor.
In
December
2008,
Agassi's
childhood
friend
and
former
business
manager
Perry
Rogers
sued
Graf
for
$50,000
in
management
fees
he
claimed
that
she
owed
him.
Agassi's
autobiography,
"Open"
(written
with
assistance
from
J.
R.
Moehringer)
was
published
in
November
2009.
In
it,
Agassi
admitted
to
using
and
testing
positive
for
methamphetamine
in
1997,
and
that
his
then-distinctive
long
hair
was
actually
a
wig;
in
Agassi's
opinion,
his
defeat
in
the
1990
French
Open
final
was
partly
due
to
issues
with
the
wearing
of
the
wig.
He
also
revealed
that
he
had
always
hated
tennis
during
his
career,
because
of
the
constant
pressure
it
exerted
on
him.
He
also
revealed
he
thought
Pete
Sampras
was
"robotic".
The
book
reached
#1
on
the
New
York
Times
Best
Seller
list
and
received
favorable
reviews.
Politics.
Agassi
is
a
registered
Democrat
and
has
donated
more
than
$100,000
to
Democratic
candidates.
Philanthropy.
Agassi
has
participated
in
many
charity
organizations
and
founded
the
Andre
Agassi
Charitable
Association
in
1994,
which
assists
Las
Vegas'
young
people.
Agassi
was
awarded
the
ATP
Arthur
Ashe
Humanitarian
award
in
1995
for
his
efforts
to
help
disadvantaged
youth.
He
is
regularly
cited
as
the
most
charitable
and
socially
involved
player
in
professional
tennis.
It
has
also
been
surmised
that
he
may
be
the
most
charitable
athlete
of
his
generation,
which
includes
Lance
Armstrong.
Andre
Agassi's
charities
help
in
assisting
children
reach
their
athletic
potential.
His
Boys
&
Girls
Club
sees
2,000
children
throughout
the
year
and
boasts
a
world
class
junior
tennis
team.
It
also
has
a
basketball
program
(the
Agassi
Stars)
and
a
rigorous
system
that
encourages
a
mix
of
academics
and
athletics.
In
2001,
Agassi
opened
up
the
Andre
Agassi
College
Preparatory
Academy
in
Las
Vegas,
a
tuition-free
charter
school
for
at-risk
children
in
the
area.
In
2009,
the
graduating
class
had
100
percent
graduation
rate
and
a
100
percent
college
acceptance
rate.
Among
other
child-related
programs
that
Agassi
supports
through
his
Andre
Agassi
Charitable
Foundation
is
Clark
County's
only
residential
facility
for
abused
and
neglected
children
called
Child
Haven.
In
1997,
Agassi
donated
funding
to
Child
Haven
for
a
six-room
classroom
building
now
named
the
Agassi
Center
for
Education.
His
foundation
also
provided
$720,000
to
assist
in
the
building
of
the
Andre
Agassi
Cottage
for
Medically
Fragile
Children.
This
facility
opened
in
December
2001
and
accommodates
developmentally
delayed
or
handicapped
children
and
children
quarantined
for
infectious
diseases.
It
houses
approximately
20
beds
and
gives
children
with
special
needs
the
special
attention
needed
to
make
them
feel
comfortable
in
their
new
surroundings."
In
2007,
Agassi,
Muhammad
Ali,
Lance
Armstrong,
Warrick
Dunn,
Jeff
Gordon,
Mia
Hamm,
Tony
Hawk,
Andrea
Jaeger,
Jackie
Joyner-Kersee,
Mario
Lemieux,
Alonzo
Mourning
and
Cal
Ripken,
Jr.
founded
the
charity
Athletes
for
Hope,
which
helps
professional
athletes
get
involved
in
charitable
causes
and
inspires
millions
of
non-athletes
to
volunteer
and
support
the
community.
Recognition.
"Tennis"
magazine
named
him
the
7th
greatest
male
player—and
12th
greatest
player
overall—for
the
period
1965
through
2005.
Records.
Most
ATP
World
Tour
Masters
1000
(formerly
ATP
Masters
Series)
titles:
17
Oldest
top
ranked
male
player
in
the
ATP
Entry
Rankings:
33
years
4
months.
---END.OF.DOCUMENT---
Austro-Asiatic
languages.
The
Austro-Asiatic
languages
are
a
large
language
family
of
Southeast
Asia,
and
also
scattered
throughout
India
and
Bangladesh.
The
name
comes
from
the
Latin
word
for
"south"
and
the
Greek
name
of
Asia,
hence
"South
Asia."
Among
these
languages,
only
Khmer,
Vietnamese,
and
Mon
have
a
long
established
recorded
history,
and
only
Vietnamese
and
Khmer
have
official
status
(in
Vietnam
and
Cambodia,
respectively).
The
rest
of
the
languages
are
spoken
by
minority
groups.
Ethnologue
identifies
168
Austro-Asiatic
languages.
These
are
traditionally
divided
into
two
families,
Mon-Khmer
and
Munda,
but
two
recent
classifications
have
abandoned
Mon-Khmer
as
a
valid
node,
although
this
is
tentative
and
not
generally
accepted.
Austro-Asiatic
languages
have
a
disjunct
distribution
across
India,
Bangladesh
and
Southeast
Asia,
separated
by
regions
where
other
languages
are
spoken.
It
is
widely
believed
that
the
Austro-Asiatic
languages
are
the
autochthonous
languages
of
Southeast
Asia
and
the
eastern
Indian
subcontinent,
and
that
the
other
languages
of
the
region,
including
the
Indo-European,
Kradai,
Dravidian
and
Sino-Tibetan
languages,
are
the
result
of
later
migrations
of
people.
The
Austro-Asiatic
languages
are
well
known
for
having
a
"sesqui-syllabic"
pattern,
with
basic
nouns
and
verbs
consisting
of
a
reduced
minor
syllable
plus
a
full
syllable.
Many
of
them
also
have
infixes.
Classification.
Linguists
traditionally
recognize
two
primary
divisions
of
Austro-Asiatic:
the
Mon-Khmer
languages
of
Southeast
Asia,
Northeast
India
and
the
Nicobar
Islands,
and
the
Munda
languages
of
East
and
Central
India
and
parts
of
Bangladesh.
However,
no
evidence
for
this
classification
has
ever
been
published,
and
it
is
possible
that
the
linguistic
classification
has
been
influenced
by
researchers'
subjective
perception
of
a
racial
dichotomy
between
the
speakers
of
languages
that
have
traditionally
been
classified
as
Mon-Khmer
and
those
that
have
traditionally
been
classified
as
Munda.
Each
of
the
families
that
is
written
in
boldface
type
below
is
accepted
as
a
valid
clade.
However,
the
relationships
between
these
families
within
Austro-Asiatic
is
debated;
in
addition
to
the
traditional
classification,
two
recent
proposals
are
given,
neither
of
which
accept
traditional
Mon-Khmer
as
a
valid
unit.
It
should
be
noted
that
little
of
the
data
used
for
competing
classifications
has
ever
been
published,
and
therefore
cannot
be
evaluated
by
peer
review.
Ilia
Peiros
(2004).
Peiros
is
a
lexicostatistic
classification,
based
on
percentages
of
shared
vocabulary.
This
means
that
a
language
may
appear
to
be
more
distantly
related
than
it
actually
is
due
to
language
contact,
so
it
is
only
a
starting
point
for
a
proper
genealogical
classification.
Diffloth
(1974).
Diffloth's
widely
cited
original
classification,
now
abandoned
by
Diffloth
himself,
is
used
in
"Encyclopædia
Britannica"
and—except
for
the
breakup
of
Southern
Mon-Khmer—in
"Ethnologue."
Protolanguage.
This
is
identical
to
earlier
reconstructions
except
for,
which
is
better
preserved
in
the
Katuic
languages
which
Sidwell
specializes
in
than
in
other
branches
of
Austro-Asiatic.
---END.OF.DOCUMENT---
Afroasiatic
languages.
The
Afroasiatic
languages
constitute
a
language
family
with
about
375
living
languages
and
more
than
350
million
speakers
spread
throughout
North
Africa,
the
Horn
of
Africa,
and
Southwest
Asia,
as
well
as
parts
of
the
Sahel,
West
Africa
and
East
Africa.
The
most
widely
spoken
Afroasiatic
language
is
Arabic,
with
230
million
speakers
(all
the
colloquial
varieties).
In
addition
to
languages
now
spoken,
Afroasiatic
includes
several
ancient
languages,
such
as
Ancient
Egyptian,
Biblical
Hebrew,
and
Akkadian.
The
term
"Afroasiatic"
(often
now
spelled
as
Afro-Asiatic)
was
coined
by
Maurice
Delafosse
(1914).
It
did
not
come
into
general
use
until
it
was
adopted
by
Joseph
Greenberg
(1950)
to
replace
the
earlier
term
"Hamito-Semitic",
following
his
demonstration
that
Hamitic
is
not
a
valid
language
family.
The
term
"Hamito-Semitic"
remains
in
use
in
the
academic
traditions
of
some
European
countries.
Some
authors
now
replace
"Afro-Asiatic"
with
"Afrasian",
or,
reflecting
an
opinion
that
it
is
more
African
than
Asian,
"Afrasan".
Individual
scholars
have
called
the
family
"Erythraean"
(Tucker
1966)
and
"Lisramic"
(Hodge
1972).
Classification
history==.
In
the
9th
century,
the
Hebrew
grammarian
Judah
ibn
Quraysh
of
Tiaret
in
Algeria
was
the
first
to
link
two
branches
of
Afroasiatic
together;
he
perceived
a
relationship
between
Berber
and
Semitic.
He
knew
of
Semitic
through
Arabic,
Hebrew,
and
Aramaic.
In
the
course
of
the
19th
century,
Europeans
also
began
suggesting
such
relationships.
In
1844,
Theodor
Benfey
suggested
a
language
family
consisting
of
Semitic,
Berber,
and
Cushitic
(calling
the
latter
"Ethiopic").
In
the
same
year,
T.N.
Newman
suggested
a
relationship
between
Semitic
and
Hausa,
but
this
would
long
remain
a
topic
of
dispute
and
uncertainty.
Friedrich
Müller
named
the
traditional
"Hamito-Semitic"
family
in
1876
in
his
"Grundriss
der
Sprachwissenschaft".
He
defined
it
as
consisting
of
a
Semitic
group
plus
a
"Hamitic"
group
containing
Egyptian,
Berber,
and
Cushitic;
he
excluded
the
Chadic
group.
These
classifications
relied
in
part
on
non-linguistic
anthropological
and
racial
arguments
(see
Hamitic
hypothesis).
Leo
Reinisch
(1909)
proposed
linking
Cushitic
and
Chadic,
while
urging
a
more
distant
affinity
to
Egyptian
and
Semitic,
thus
foreshadowing
Greenberg,
but
his
suggestion
found
little
resonance.
Marcel
Cohen
(1924)
rejected
the
idea
of
a
distinct
Hamitic
subgroup
and
included
Hausa
(a
Chadic
language)
in
his
comparative
Hamito-Semitic
vocabulary.
Joseph
Greenberg
(1950)
strongly
confirmed
Cohen's
rejection
of
"Hamitic",
added
(and
sub-classified)
the
Chadic
branch,
and
proposed
the
new
name
"Afroasiatic"
for
the
family.
Nearly
all
scholars
have
accepted
Greenberg's
classification.
In
1969,
Harold
Fleming
proposed
that
what
had
previously
been
known
as
Western
Cushitic
is
an
independent
branch
of
Afroasiatic,
suggesting
for
it
the
new
name
Omotic.
This
proposal
and
name
have
met
with
widespread
acceptance.
Several
scholars,
including
Harold
Fleming
and
Robert
Hetzron,
have
since
questioned
the
traditional
inclusion
of
Beja
in
Cushitic.
Subgrouping.
Little
agreement
exists
on
the
subgrouping
of
the
five
or
six
branches
of
Afroasiatic:
Semitic,
Egyptian,
Berber,
Chadic,
Cushitic,
and
Omotic
(if
Omotic
is
not
included
in
Cushitic).
However,
Christopher
Ehret
(1979),
Harold
Fleming
(1981),
and
Joseph
Greenberg
(1981)
all
agree
that
the
Omotic
branch
split
from
the
rest
first.
Position
among
the
world's
languages.
Afroasiatic
is
one
of
the
four
language
families
of
Africa
identified
by
Joseph
Greenberg
in
his
book
"The
Languages
of
Africa"
(1963).
It
is
the
only
one
that
extends
outside
of
Africa,
via
the
Semitic
branch.
Origins
and
common
features.
All
Afroasiatic
subfamilies
show
evidence
of
a
causative
affix
"s",
but
a
similar
suffix
also
appears
in
other
groups,
such
as
the
Niger-Congo
languages.
Semitic,
Berber,
Cushitic
(including
Beja),
and
Chadic
support
possessive
suffixes.
Tonal
languages
appear
in
the
Omotic,
Chadic,
and
Cushitic
branches
of
Afroasiatic,
according
to
Ehret
(1996).
The
Semitic,
Berber,
and
Egyptian
branches
do
not
use
tones
phonemically.
---END.OF.DOCUMENT---
Andorra.
Andorra,
officially
the
Principality
of
Andorra
(),
also
called
the
Principality
of
the
Valleys
of
Andorra,
is
a
small
country
in
southwestern
Europe,
located
in
the
eastern
Pyrenees
mountains
and
bordered
by
Spain
and
France.
It
is
the
sixth
smallest
nation
in
Europe
having
an
area
of
and
an
estimated
population
of
83,888
in
2009.
Its
capital,
Andorra
la
Vella,
is
the
highest
capital
city
in
Europe,
being
at
an
elevation
of
1023 metres.
The
official
language
is
Catalan,
although
Spanish,
French,
and
Portuguese
are
also
commonly
spoken.
The
Principality
was
formed
in
1278.
The
role
of
monarch
is
shared
between
the
President
of
the
French
Republic
and
the
Bishop
of
Urgell,
Catalonia,
Spain.
It
is
a
prosperous
country
mainly
because
of
its
tourism
industry,
which
services
an
estimated
10.2 million
visitors
annually,
and
also
because
of
its
status
as
a
tax
haven.
It
is
not
a
member
of
the
European
Union,
but
the
euro
is
the
"de
facto"
currency.
The
people
of
Andorra
have
the
2nd
highest
human
life
expectancy
in
the
world
—
82 years
at
birth.
History.
Tradition
holds
that
Charles
the
Great
(Charlemagne)
granted
a
charter
to
the
Andorran
people
in
return
for
fighting
against
the
Moors.
Overlordship
of
the
territory
was
by
the
Count
of
Urgell
and
eventually
by
the
bishop
of
the
Diocese
of
Urgell.
In
988,
Borrell
II,
Count
of
Urgell,
gave
the
Andorran
valleys
to
the
Diocese
of
Urgell
in
exchange
for
land
in
Cerdanya.
Since
then
the
Bishop
of
Urgell,
based
in
Seu
d'Urgell,
has
owned
Andorra.
Before
1095,
Andorra
did
not
have
any
type
of
military
protection
and
the
Bishop
of
Urgell,
who
knew
that
the
Count
of
Urgell
wanted
to
reclaim
the
Andorran
valleys,
asked
for
help
and
protection
from
the
Lord
of
Caboet.
In
1095,
the
Lord
of
Caboet
and
the
Bishop
of
Urgell
signed
under
oath
a
declaration
of
their
co-sovereignty
over
Andorra.
Arnalda,
daughter
of
Arnau
of
Caboet,
married
the
Viscount
of
Castellbò
and
both
became
Viscounts
of
Castellbò
and
Cerdanya.
Years
later
their
daughter,
Ermessenda,
married
Roger
Bernat
II,
the
French
Count
of
Foix.
They
became
Roger
Bernat
II
and
Ermessenda
I,
Counts
of
Foix,
Viscounts
of
Castellbò
and
Cerdanya,
and
also
co-sovereigns
of
Andorra
(shared
with
the
Bishop
of
Urgell).
In
the
eleventh
century,
a
dispute
arose
between
the
Bishop
of
Urgell
and
the
Count
of
Foix.
The
conflict
was
resolved
in
1278
with
the
mediation
of
Aragon
by
the
signing
of
the
first
paréage
which
provided
that
Andorra's
sovereignty
be
shared
between
the
count
of
Foix
(whose
title
would
ultimately
transfer
to
the
French
head
of
state)
and
the
Bishop
of
Urgell,
in
Catalonia.
This
gave
the
principality
its
territory
and
political
form.
Over
the
years,
the
French
co-title
to
Andorra
passed
to
the
kings
of
Navarre.
After
Henry
of
Navarre
became
King
Henry
IV
of
France,
he
issued
an
edict
in
1607
that
established
the
head
of
the
French
state
and
the
Bishop
of
Urgell
as
co-princes
of
Andorra.
In
1812–13,
the
First
French
Empire
annexed
Catalonia
and
divided
it
in
four
départements,
with
Andorra
being
made
part
of
the
district
of
Puigcerdà
(département
of
Sègre).
20th
century.
Andorra
declared
war
on
Imperial
Germany
during
World
War
I,
but
did
not
actually
take
part
in
the
fighting.
It
remained
in
an
official
state
of
belligerency
until
1957
as
it
was
not
included
in
the
Treaty
of
Versailles.
In
1933,
France
occupied
Andorra
as
a
result
of
social
unrest
before
elections.
On
July
12,
1934,
adventurer
Boris
Skossyreff
issued
a
proclamation
in
Urgell,
declaring
himself
Boris
I,
sovereign
prince
of
Andorra,
simultaneously
declaring
war
on
the
Bishop
of
Urgell.
He
was
arrested
by
Spanish
authorities
on
July
20
and
ultimately
expelled
from
Spain.
From
1936
to
1940,
a
French
detachment
was
garrisoned
in
Andorra
to
prevent
influences
of
the
Spanish
Civil
War
and
Franco's
Spain.
Francoist
troops
reached
the
Andorran
border
in
the
later
stages
of
the
war.
During
World
War
II,
Andorra
remained
neutral
and
was
an
important
smuggling
route
between
Vichy
France
and
Spain.
Given
its
relative
isolation,
Andorra
has
existed
outside
the
mainstream
of
European
history,
with
few
ties
to
countries
other
than
France
and
Spain.
In
recent
times,
however,
its
thriving
tourist
industry
along
with
developments
in
transport
and
communications
have
removed
the
country
from
its
isolation.
Its
political
system
was
thoroughly
modernised
in
1993,
the
year
in
which
it
became
a
member
of
the
United
Nations
and
the
Council
of
Europe.
Politics.
Andorra
is
a
parliamentary
co-principality
with
the
President
of
France
and
the
Bishop
of
Urgell
(Catalonia,
Spain),
as
co-princes,
in
a
duumvirate.
The
politics
of
Andorra
take
place
in
a
framework
of
a
parliamentary
representative
democracy,
whereby
the
Prime
Minister
of
Andorra
is
the
head
of
government,
and
of
a
pluriform
multi-party
system.
The
current
Prime
Minister
is
Jaume
Bartumeu
of
the
Social
Democratic
Party
(PS).
Executive
power
is
exercised
by
the
government.
Legislative
power
is
vested
in
both
the
government
and
parliament.
The
Parliament
of
Andorra
is
known
as
the
General
Council.
The
General
Council
consists
of
between
28
and
42
Councilors,
as
the
members
of
the
legislative
branch
are
called.
The
Councilors
serve
for
four-year
terms
and
elections
are
held
between
the
thirtieth
and
fortieth
days
following
the
dissolution
of
the
previous
Council.
The
Councilors
can
be
elected
on
two
equal
constituencies.
Half
are
elected
in
equal
number
from
each
of
the
seven
administrative
parishes
and
the
other
half
of
the
Councilors
are
elected
from
a
single
national
constituency.
Fifteen
days
after
the
election,
the
Councilors
hold
their
inauguration.
During
this
session,
the
Syndic
General,
who
is
the
head
of
the
General
Council,
and
the
Subsyndic
General,
his
assistant,
are
elected.
Eight
days
later,
the
Council
convenes
once
more.
During
this
session
the
Head
of
Government,
the
Prime
Minister
of
Andorra,
is
chosen
from
among
the
Councilors.
Candidates
for
the
prime-ministerial
nomination
can
be
proposed
by
a
minimum
of
one-fifth
of
the
Councilors.
The
Council
then
elects
the
candidate
with
the
absolute
majority
of
votes
to
be
Head
of
Government.
The
Syndic
General
then
notifies
the
Co-princes
who
in
turn
appoint
the
elected
candidate
as
the
Prime
Minister
of
Andorra.
The
General
Council
is
also
responsible
for
proposing
and
passing
laws.
Bills
may
be
presented
to
the
Council
as
Private
Members'
Bills
by
three
of
the
Local
Parish
Councils
jointly
or
by
at
least
one
tenth
of
the
citizens
of
Andorra.
The
Council
also
approves
the
annual
budget
of
the
principality.
The
government
must
submit
the
proposed
budget
for
parliamentary
approval
at
least
two
months
before
the
previous
budget
expires.
If
the
budget
is
not
approved
by
the
first
day
of
the
next
year,
the
previous
budget
is
extended
until
a
new
one
is
approved.
Once
any
bill
is
approved,
the
Syndic
General
is
responsible
for
presenting
it
to
the
Co-princes
so
that
they
may
sign
and
enact
it.
If
the
Head
of
Government
is
not
satisfied
with
the
Council,
he
may
request
that
the
Co-princes
dissolve
the
Council
and
order
new
elections.
In
turn,
the
Councilors
have
the
power
to
remove
the
Head
of
Government
from
office.
After
a
motion
of
censure
is
approved
by
at
least
one-fifth
of
the
Councilors,
the
Council
will
vote
and
if
it
receives
the
absolute
majority
of
votes,
the
Prime
Minister
is
removed.
Law
and
criminal
justice.
The
judiciary
is
composed
of
the
Magistrates
Court,
the
Criminal
Law
Court,
the
High
Court
of
Andorra,
and
the
Constitutional
Court.
The
High
Court
of
Justice
is
composed
of
five
judges:
one
appointed
by
the
Head
of
Government,
one
each
by
the
Coprinces,
one
by
the
Syndic
General,
and
one
by
the
Judges
and
Magistrates.
It
is
presided
over
by
the
member
appointed
by
the
Syndic
General
and
the
judges
hold
office
for
six-year
terms.
The
Magistrates
and
Judges
are
appointed
by
the
High
Court,
and
so
is
the
President
of
the
Criminal
Law
Court.
The
High
Court
also
appoints
members
of
the
Office
of
the
Attorney
General.
The
Constitutional
Court
is
responsible
for
interpreting
the
Constitution
and
reviewing
all
appeals
of
unconstitutionality
against
laws
and
treaties.
It
is
composed
of
four
judges,
one
appointed
by
each
of
the
Coprinces
and
two
by
the
General
Council.
They
serve
eight-year
terms.
The
Court
is
presided
over
by
one
of
the
Judges
on
a
two-year
rotation
so
that
each
judge
at
one
point
will
be
the
leader
of
the
Court.
Foreign
relations
and
defence.
Responsibility
for
defending
Andorra
rests
with
Spain
and
France.
Geography.
File:Andorramap.png|thumb|250px|Map
of
Andorra
with
its
seven
parishes
labeled
()
rect
34
350
121
404
Andorra
la
Vella
rect
443
148
519
172
Canillo
rect
437
363
520
389
Encamp
rect
352
443
464
498
Escaldes-Engordany
rect
36
223
155
247
La
Massana
rect
284
78
354
103
Ordino
rect
208
567
304
618
Sant
Julià
de
Lòria
rect
651
54
745
83
France
rect
484
583
560
619
Spain
Physical
geography.
Due
to
its
location
in
the
eastern
Pyrenees
mountain
range,
Andorra
consists
predominantly
of
rugged
mountains,
the
highest
being
the
Coma
Pedrosa
at,
and
the
average
elevation
of
Andorra
is.
These
are
dissected
by
three
narrow
valleys
in
a
Y
shape
that
combine
into
one
as
the
main
stream,
the
Gran
Valira
river,
leaves
the
country
for
Spain
(at
Andorra's
lowest
point
of).
Andorra's
surface
area
is.
Phytogeographically,
Andorra
belongs
to
the
Atlantic
European
province
of
the
Circumboreal
Region
within
the
Boreal
Kingdom.
According
to
the
WWF,
the
territory
of
Andorra
belongs
to
the
ecoregion
of
Pyrenees
conifer
and
mixed
forests.
Climate.
Andorra
has
a
temperate
climate
similar
to
that
of
its
neighbours,
but
its
higher
elevation
means
there
is,
on
average,
more
snow
in
winter,
lower
humidity,
and
it
is
slightly
cooler
in
summer.
There
are,
on
average,
300 days
per
year
of
sunshine.
Economy.
Tourism,
the
mainstay
of
Andorra's
tiny,
well-to-do
economy,
accounts
for
roughly
80%
of
GDP.
An
estimated
10.2 million
tourists
visit
annually,
attracted
by
Andorra's
duty-free
status
and
by
its
summer
and
winter
resorts.
Andorra's
comparative
advantage
has
recently
eroded
as
the
economies
of
adjoining
France
and
Spain
have
been
opened
up,
providing
broader
availability
of
goods
and
lower
tariffs.
The
banking
sector,
with
its
tax
haven
status,
also
contributes
substantially
to
the
economy.
Agricultural
production
is
limited—only
2%
of
the
land
is
arable—and
most
food
has
to
be
imported.
Some
tobacco
is
grown
locally.
The
principal
livestock
activity
is
domestic
sheep
raising.
Manufacturing
output
consists
mainly
of
cigarettes,
cigars,
and
furniture.
Andorra's
natural
resources
include
hydroelectric
power,
mineral
water,
timber,
iron
ore,
and
lead.
Andorra
is
not
a
member
of
the
European
Union,
but
enjoys
a
special
relationship
with
it,
such
as
being
treated
as
an
EU
member
for
trade
in
manufactured
goods
(no
tariffs)
and
as
a
non-EU
member
for
agricultural
products.
Andorra
lacks
a
currency
of
its
own
and
uses
that
of
its
two
surrounding
nations.
Andorra
used
the
French
franc
and
the
Spanish
peseta
until
1999
when
both
currencies
were
replaced
by
the
EU's
single
currency,
the
euro.
Coins
and
notes
of
both
the
franc
and
the
peseta,
however,
remained
legal
tender
in
Andorra
until
2002.
Andorra
is
negotiating
to
issue
its
own
euro
coins.
Population.
The
population
of
Andorra
is
estimated
to
be
83,888
(July
2009).
The
population
has
grown
from
5,000
in
1900,
and
reached
a
peak
of
84,484
(estimated)
in
July
2008.
Andorrans
are
a
minority
in
their
own
country
(31,363);
other
nationalities
including
Spaniards
(27,300),
Portuguese
(13,794),
French
(5,213),
Britons
(1,085)
and
Italians
altogether
make
up
a
total
of
67.7%
of
Andorra's
population.
Languages.
The
historic
and
official
language
is
Catalan,
a
Romance
language.
Because
of
immigration,
historical
links,
and
close
geographic
proximity,
other
languages
such
as
Spanish,
French
and
Portuguese
are
also
commonly
spoken.
Most
Andorrans
also
speak
Spanish
(Castilian),
French
or
both.
Andorra
is
one
of
only
four
European
countries
(together
with
France,
Monaco,
and
Turkey)
that
have
never
signed
the
Council
of
Europe
Framework
Convention
on
National
Minorities.
Religion.
The
population
of
Andorra
is
predominantly
(90%)
Roman
Catholic.
Their
patron
saint
is
Our
Lady
of
Meritxell.
Though
it
is
not
an
official
state
religion,
the
constitution
acknowledges
a
special
relationship
with
the
Roman
Catholic
Church,
offering
some
special
privileges
to
that
group.
The
Muslim
community
is
primarily
made
up
of
North
African
immigrants.
Other
Christian
denominations
include
the
Anglican
Church,
Jehovah’s
Witnesses,
the
Reunification
Church,
the
New
Apostolic
Church,
and
The
Church
of
Jesus
Christ
of
Latter-day
Saints.
There
is
a
small
community
of
Hindus.
Schools.
Children
between
the
ages
of
6
and
16
are
required
by
law
to
have
full-time
education.
Education
up
to
secondary
level
is
provided
free
of
charge
by
the
government.
There
are
three
systems
of
schools
–
Andorran,
French
and
Spanish
–
which
use
Catalan,
French
and
Spanish,
respectively,
as
the
main
language
of
instruction.
Parents
may
choose
which
system
their
children
attend.
All
schools
are
built
and
maintained
by
Andorran
authorities,
but
teachers
in
the
French
and
Spanish
schools
are
paid
for
the
most
part
by
France
and
Spain.
About
50%
of
Andorran
children
attend
the
French
primary
schools,
and
the
rest
attend
Spanish
or
Andorran
schools.
University
of
Andorra.
The
University
of
Andorra
(UdA)
is
the
state
public
university
and
is
the
only
university
in
Andorra.
It
was
established
in
1997.
The
University
provides
first-level
degrees
in
nursing,
computer
science,
business
administration,
and
educational
sciences,
in
addition
to
higher
professional
education
courses.
The
only
two
graduate
schools
in
Andorra
are
the
Nursing
School
and
the
School
of
Computer
Science,
the
latter
having
a
PhD
programme.
Virtual
Studies
Centre.
The
geographical
complexity
of
the
country
as
well
as
the
small
number
of
students
prevents
the
University
of
Andorra
from
developing
a
full
academic
programme,
and
it
serves
principally
as
a
centre
for
virtual
studies,
connected
to
Spanish
and
French
universities.
The
Virtual
Studies
Centre
("Centre
d’Estudis
Virtuals")
at
the
University
runs
in
the
region
of
twenty
degrees
at
both
undergraduate
and
postgraduate
levels
in
fields
including
tourism,
law,
Catalan
philology,
humanities,
psychology,
political
sciences,
audiovisual
communication,
telecommunications
engineering,
and
East
Asia
studies.
The
Centre
also
runs
various
postgraduate
programmes
and
continuing-education
courses
for
professionals.
Healthcare.
Healthcare
in
Andorra
is
provided
to
all
employed
persons
and
their
families
by
the
government-run
social
security
system,
CASS
(Caixa
Andorrana
de
Seguretat
Social),
which
is
funded
by
employer
and
employee
contributions
in
respect
of
salaries.
The
cost
of
healthcare
is
covered
by
CASS
at
rates
of
75%
for
out-patient
expenses
such
as
medicines
and
hospital
visits,
90%
for
hospitalisation,
and
100%
for
work-related
accidents.
The
remainder
of
the
costs
may
be
covered
by
private
health
insurance.
Other
residents
and
tourists
require
full
private
health
insurance.
The
main
hospital,
Meritxell,
is
in
Escaldes-Engordany.
Its
services
include
24-hour
accident
and
emergency,
anatomy,
angiology,
pathology,
anesthesiology,
clinical
cardiology,
clinical
biochemistry,
clinical
neurology,
dermatology,
endocrinology,
gastroenterology,
genetics,
geriatrics,
hematology,
immunology,
intensive
care,
internal
medicine,
medical
oncology,
microbiology,
nephrology
and
dialysis,
neurophysiology,
obstetrics
and
gynecology,
oncology,
ophthalmology,
otorhinolaringology,
orthopedics,
pediatrics,
physiotherapy,
neonatology,
plastic
surgery,
general
surgery,
oral
and
maxillofacial
surgery,
neurosurgery,
pediatric
surgery,
thoracic
surgery,
trauma
surgery,
cardiovascular
surgery,
parasitology,
psychiatry,
radiodiagnostics,
radiotherapy,
urology,
and
venerealogy.
There
are
also
12
primary
health
care
centres
in
various
locations
around
the
Principality.
Transport.
Andorra
has
a
road
network
of,
of
which
is
unpaved.
The
two
main
roads
out
of
Andorra
la
Vella
are
the
CG-1
to
the
Spanish
border,
and
the
CG-2
to
the
French
border
via
the
Envalira
Tunnel
near
Pas
de
la
Casa.
In
winter,
the
main
roads
in
Andorra
are
usually
quickly
cleared
of
snow
and
remain
accessible,
but
the
main
road
out
of
Andorra
on
the
French
side
(RN-20/22)
is
less
frequently
cleared
and
is
sometimes
closed
by
avalanches.
Other
main
roads
out
of
Andorra
la
Vella
are
the
CG-3
and
CG-4
to
Arcalis
and
Pal,
respectively.
Bus
services
cover
all
metropolitan
areas
and
many
rural
communities,
with
services
on
most
major
routes
running
half-hourly
or
more
frequently
during
peak
travel
times.
There
are
frequent
long-distance
bus
services
from
Andorra
to
Barcelona
and
Barcelona
Airport,
and
also
to
Toulouse
and
Toulouse
Airport,
in
each
case
taking
approximately
3 hours.
Bus
routes
also
serve
Girona
Airport
and
Portugal
via
Lleida.
Bus
services
are
mostly
run
by
private
companies,
but
some
local
ones
are
operated
by
the
Government.
The
private
bus
companies
are
Autocars
Nadal,
Camino
Bus,
Cooperativa
Interurbana
Andorrana,
Eurolines,
Hispano
Andorrana,
and
Novatel.
There
are
no
railways,
ports,
or
airports
for
fixed-wing
aircraft
in
Andorra.
There
are,
however,
heliports
in
La
Massana,
Arinsal
and
Escaldes-Engordany
with
commercial
helicopter
services.
Nearby
airports
are
located
in
Barcelona,
Toulouse,
Perpignan,
Reus,
and
Girona.
The
closest
public
airport
is
Perpignan
-
Rivesaltes
Airport,
which
is
away
and
has
short-haul
services
to
several
destinations
in
the
United
Kingdom
and
France.
La
Seu
d'Urgell
Airport,
a
small
airfield
south
of
Andorra
currently
used
only
by
private
aeroplanes,
is
being
studied
by
the
Catalan
government
as
a
possible
future
airport
for
public
aviation
services.
The
nearest
railway
station
is
L'Hospitalet-près-l'Andorre
east
of
Andorra
which
is
on
the
-gauge
line
from
Latour-de-Carol,
()
southeast
of
Andorra,
to
Toulouse
and
on
to
Paris
by
the
French
high-speed
trains.
This
line
is
operated
by
the
SNCF.
Latour-de-Carol
has
a
scenic
metre-gauge
trainline
to
Villefranche-de-Conflent,
as
well
as
the
SNCF's
-gauge
line
connecting
to
Perpignan,
and
the
RENFE's
-gauge
line
to
Barcelona.
Media
and
telecommunications.
In
Andorra,
mobile
and
fixed
telephony
and
internet
services
are
operated
exclusively
by
the
Andorran
national
telecommunications
company,
SOM,
also
known
as
"Servei
de
Telecomunicacions
d'Andorra"
(STA).
The
same
company
also
manages
the
technical
infrastructure
for
national
broadcasting
of
digital
television
and
radio.
By
the
end
of
2010,
it
is
planned
that
every
home
in
the
country
will
have
Fibre-Optic
to
the
Home
for
internet
access
at
a
minimum
speed
of
100 Mbps.
There
is
only
one
Andorran
television
station,
"Ràdio
i
Televisió
d'Andorra"
(RTVA).
"Radio
Nacional
d’Andorra"
operates
two
radio
stations,
"Radio
Andorra"
and
"Andorra
Música".
There
are
three
national
newspapers,
"Diari
D'Andorra",
"El
Periòdic",
and
"Bon
Dia"
as
well
as
several
local
newspapers.
Culture.
The
official
and
historic
language
is
Catalan.
Thus,
its
culture
is
Catalan
with
some
own
specificity.
Andorra
is
home
to
folk
dances
like
the
contrapàs
and
marratxa,
which
survive
in
Sant
Julià
de
Lòria
especially.
Andorran
folk
music
has
similarities
to
the
music
of
its
neighbours,
but
is
especially
Catalan
in
character,
especially
in
the
presence
of
dances
such
as
the
sardana.
Other
Andorran
folk
dances
include
contrapàs
in
Andorra
la
Vella
and
Saint
Anne's
dance
in
Escaldes-Engordany.
Andorra's
national
holiday
is
Our
Lady
of
Meritxell
Day,
September
8.
---END.OF.DOCUMENT---
Arithmetic
mean.
In
mathematics
and
statistics,
the
arithmetic
mean
(or
simply
the
mean)
of
a
list
of
numbers
is
the
sum
of
all
of
the
list
divided
by
the
number
of
items
in
the
list.
If
the
list
is
a
statistical
population,
then
the
mean
of
that
population
is
called
a
population
mean.
If
the
list
is
a
statistical
sample,
we
call
the
resulting
statistic
a
sample
mean.
The
mean
is
the
most
commonly-used
type
of
average
and
is
often
referred
to
simply
as
the
"average".
The
term
"mean"
or
"arithmetic
mean"
is
preferred
in
mathematics
and
statistics
to
distinguish
it
from
other
averages
such
as
the
median
and
the
mode.
Introduction.
If
we
denote
a
set
of
data
by
"X"
=
("x"1,
"x"2...,
"x'n"),
then
the
sample
mean
is
typically
denoted
with
a
horizontal
bar
over
the
variable
(formula_1,
enunciated
"x"
bar").
The
Greek
letter
μ
is
used
to
denote
the
arithmetic
mean
of
an
entire
population.
Or,
for
a
random
variable
that
has
a
defined
mean,
μ
is
the
"probabilistic
mean"
or
expected
value
of
the
random
number.
If
the
set
"X"
is
a
collection
of
random
numbers
with
probabilistic
mean
of
μ,
then
for
any
individual
sample,
"x'i",
from
that
collection,
μ
=
E
is
the
expected
value
of
that
sample.
In
practice,
the
difference
between
μ
and
formula_1
is
that
μ
is
typically
unobservable
because
one
observes
only
a
sample
rather
than
the
whole
population,
and
if
the
sample
is
drawn
randomly,
then
one
may
treat
formula_1,
but
not
μ,
as
a
random
variable,
attributing
a
probability
distribution
to
it
(the
sampling
distribution
of
the
mean).
It
is
a
U-statistic
for
the
function
formula_5
meaning
that
it
is
obtained
by
averaging
a
1-sample
statistic
over
the
population.
If
"X"
is
a
random
variable,
then
the
expected
value
of
"X"
can
be
seen
as
the
long-term
arithmetic
mean
that
occurs
on
repeated
measurements
of
"X".
This
is
the
content
of
the
law
of
large
numbers.
As
a
result,
the
sample
mean
is
used
to
estimate
unknown
expected
values.
Simple
algebra
will
prove
that
a
mean
of
"n" + 1
numbers
is
larger
than
the
mean
of
"n"
numbers
if
and
only
if
the
new
number
is
larger
than
the
old
mean,
smaller
if
and
only
if
it
is
smaller,
and
remains
stable
if
and
only
if
it
is
equal
to
the
old
mean.
The
larger
"n"
is,
the
smaller
is
the
magnitude
of
the
change
in
the
mean
relative
to
the
distance
between
the
old
mean
and
the
new
number.
Note
that
several
other
"means"
have
been
defined,
including
the
generalized
mean,
the
generalized
f-mean,
the
harmonic
mean,
the
arithmetic-geometric
mean,
and
various
weighted
means.
Not
robust.
While
the
mean
is
often
used
to
report
central
tendency,
it
is
not
a
robust
statistic,
meaning
that
it
is
greatly
influenced
by
outliers.
Notably,
for
skewed
distributions,
the
arithmetic
mean
may
not
accord
with
one's
notion
of
"middle",
and
robust
statistics
such
as
the
median
may
be
a
better
description
of
central
tendency.
A
classic
example
is
average
income.
The
arithmetic
mean
may
be
misinterpreted
as
the
median
to
imply
that
most
people's
incomes
are
higher
than
is
in
fact
the
case.
When
presented
with
an
"average"
one
may
be
led
to
believe
that
"most"
people's
incomes
are
near
this
number.
This
"average"
(arithmetic
mean)
income
"is"
higher
than
most
people's
incomes,
because
high
income
outliers
skew
the
result
higher
(in
contrast,
the
median
income
"resists"
such
skew).
However,
this
"average"
says
nothing
about
the
number
of
people
near
the
median
income
(nor
does
it
say
anything
about
the
modal
income
that
most
people
are
near).
Nevertheless,
because
one
might
carelessly
relate
"average"
and
"most
people"
one
might
incorrectly
assume
that
most
people's
incomes
would
be
higher
(nearer
this
inflated
"average")
than
they
are.
For
instance,
reporting
the
"average"
net
worth
in
Medina,
Washington
as
the
arithmetic
mean
of
all
annual
net
worths
would
yield
a
surprisingly
high
number
because
of
Bill
Gates.
Consider
the
scores
(1,
2,
2,
2,
3,
9).
The
arithmetic
mean
is
3.17,
but
five
out
of
six
scores
are
below
this.
Compounding.
If
numbers
"multiply"
instead
of
"add,"
one
should
average
using
the
geometric
mean,
not
the
arithmetic
mean.
This
most
often
happens
when
computing
the
rate
of
return,
as
in
finance.
For
example,
if
a
stock
fell
10
%
in
the
first
year,
and
rose
30
%
in
the
second
year,
then
it
would
be
incorrect
to
report
its
"average"
increase
per
year
over
this
two
year
period
as
the
arithmetic
mean
(−10
%
+
30
%)/2
=
10
%;
the
correct
average
in
this
case
is
the
compound
annual
growth
rate,
which
yields
an
annualized
increase
per
year
of
only
8.2
%.
The
reason
for
this
is
that
each
of
those
percents
have
different
starting
points:
the
30%
is
30%
"of
a
smaller
number".
If
the
stock
starts
at
$30
and
falls
10
%,
it
is
now
at
$27.
If
the
stock
then
rises
30
%,
it
is
now
$35.1.
The
arithmetic
mean
of
those
rises
is
10
%,
but
since
the
stock
rose
by
$5.1
in
2
years,
an
average
of
8.2
%
would
result
in
the
final
$35.1
figure
[$30(1-10
%)(1+30
%)
=
$30(1+8.2
%)(1+8.2
%)
=
$35.1].
If
one
used
the
arithmetic
mean
10
%
in
the
same
way,
one
would
not
get
the
actual
increase
[$30(1+10
%)(1+10
%)
=
$36.3].
Stated
generally,
compounding
yields
90%
*
130%
=
117%
overall
growth,
and
annualizing
yields
formula_8,
so
8.2%
per
year.
Directions.
Particular
care
must
be
taken
when
using
cyclic
data
such
as
phases
or
angles.
Naïvely
taking
the
arithmetic
mean
of
1°
and
359°
yields
a
result
of
180°.
In
general
application
such
an
oversight
will
lead
to
the
average
value
artificially
moving
towards
the
middle
of
the
numerical
range.
A
solution
to
this
problem
is
to
use
the
optimization
formulation
(viz,
define
the
mean
as
the
central
point:
the
point
about
which
one
has
the
lowest
dispersion),
and
redefine
the
difference
as
a
modular
distance
(i.e.,
the
distance
on
the
circle:
so
the
modular
distance
between
1°
and
359°
is
2°,
not
358°).
---END.OF.DOCUMENT---
American
Football
Conference.
The
American
Football
Conference
(AFC)
is
one
of
the
two
conferences
of
the
National
Football
League
(NFL).
This
conference
and
its
counterpart,
the
National
Football
Conference
(NFC),
currently
contain
16
teams
each,
making
up
the
32
teams
of
the
NFL.
Current
teams.
Since
2002,
the
AFC
has
comprised
16
teams,
organized
into
four
divisions:
North,
South,
East,
West.
Season
structure.
Each
AFC
team
plays
the
other
teams
in
their
division
twice
(home
and
away)
during
the
regular
season,
in
addition
to
10
other
games
assigned
to
their
schedule
by
the
NFL
the
previous
May.
Two
of
these
games
are
assigned
on
the
basis
of
the
team's
final
division
standing
in
the
previous
season.
The
remaining
8
games
are
split
between
the
roster
of
two
other
NFL
divisions.
This
assignment
shifts
each
year.
For
instance,
in
the
2007
regular
season,
each
team
in
the
AFC
West
played
one
game
against
each
team
in
both
the
AFC
South
and
the
NFC
North.
In
this
way
division
competition
consists
of
common
opponents,
with
the
exception
of
the
2
games
assigned
on
the
strength
of
each
team's
prior
division
standing.
(i.e.
the
division
winner
will
face
the
other
two
division
winners
in
the
AFC
divisions
that
they
are
not
scheduled
to
play)
The
NFC
operates
according
to
the
same
system.
At
the
end
of
each
football
season,
there
are
playoff
games
involving
the
top
six
teams
in
the
AFC
(the
four
division
champions
by
place
standing
and
the
top
two
remaining
non-division-champion
teams
("wild
cards")
by
record).
The
last
two
teams
remaining
play
in
the
AFC
Championship
game
with
the
winner
receiving
the
Lamar
Hunt
Trophy.
The
AFC
champion
plays
the
NFC
champion
in
the
Super
Bowl.
After
Super
Bowl
XLIII
the
AFC
has
won
19
Super
Bowls
to
the
21
won
by
the
NFC.
Since
losing
13
consecutive
Super
Bowls
in
the
1980s
and
1990s
(XIX–XXXI),
the
AFC
has
won
nine
of
the
last
twelve.
The
losing
coach
of
the
AFC
Championship
game
is
the
coach
of
the
Pro
Bowl
the
week
after
the
Super
Bowl.
History.
The
AFC
was
created
after
the
NFL
merged
with
the
American
Football
League
(AFL)
in
1970.
All
of
the
10
former
AFL
teams
along
with
the
NFL's
Cleveland
Browns,
Pittsburgh
Steelers,
and
the
then-Baltimore
Colts
joined
the
AFC.
Since
the
merger,
five
expansion
teams
have
joined
the
AFC
and
two
have
left,
thus
making
the
current
total
16.
When
the
Seattle
Seahawks
and
the
Tampa
Bay
Buccaneers
joined
the
league
in
1976,
they
were
temporarily
placed
in
the
NFC
and
AFC
respectively.
This
arrangement
lasted
for
one
season
only
before
the
two
teams
switched
conferences.
The
Seahawks
eventually
returned
to
the
NFC
as
a
result
of
the
2002
realignment.
The
expansion
Jacksonville
Jaguars
joined
the
AFC
in
1995.
Due
to
the
relocation
controversy
of
the
Cleveland
Browns,
a
new
AFC
franchise
called
the
Baltimore
Ravens
was
officially
established
in
1996
while
the
Browns
were
reactivated
in
1999.
The
Houston
Texans
were
then
added
to
the
league
in
2002,
joining
the
AFC.
Logo.
The
merged
league
created
a
new
logo
for
the
AFC
that
took
elements
of
the
old
AFL
logo,
specifically
the
"A"
and
the
six
stars
surrounding
it.
The
AFC
logo
basically
remained
unchanged
from
1970
to
2009.
The
2010
NFL
season
introduced
an
updated
AFC
logo,
with
the
most
notable
revision
being
the
addition
of
a
fourth
star
(representing
the
four
divisions
of
the
AFC),
and
moving
the
stars
inside
the
letter,
similar
to
the
NFC
logo.
---END.OF.DOCUMENT---
Animal
Farm.
"Animal
Farm"
is
a
dystopian
allegorical
novella
by
George
Orwell.
Published
in
England
on
17
August
1945,
the
book
reflects
events
leading
up
to
and
during
the
Stalin
era
before
World
War
II.
Orwell,
a
democratic
socialist
and
a
member
of
the
Independent
Labour
Party
for
many
years,
was
a
critic
of
Joseph
Stalin
and
was
suspicious
of
Moscow-directed
Stalinism
after
his
experiences
with
the
NKVD
during
the
Spanish
Civil
War.
In
a
letter
to
Yvonne
Davet,
Orwell
described
"Animal
Farm"
as
his
novel
"contre
Stalin".
The
original
title
was
"Animal
Farm:
A
Fairy
Story",
but
"A
Fairy
Story"
was
dropped
by
the
US
publishers
for
its
1946
publication.
Of
all
the
translations
during
Orwell's
lifetime,
only
Telugu
kept
the
original
title.
Other
variations
in
the
title
include:
"A
Satire"
and
"A
Contemporary
Satire".
Orwell
suggested
for
the
French
translation
the
title
"Union
des
républiques
socialistes
animales",
recalling
the
French
name
of
the
Soviet
Union,
"Union
des
républiques
socialistes
soviétiques",
and
which
abbreviates
URSA,
which
means
"bear",
a
symbol
of
Russia,
in
Latin.
"Time"
Magazine
chose
the
book
as
one
of
the
100
best
English-language
novels
(1923
to
2005);
it
also
places
at
number
31
on
the
Modern
Library
List
of
Best
20th-Century
Novels.
It
won
a
Retrospective
Hugo
Award
in
1996
and
is
also
included
in
the
Great
Books
of
the
Western
World.
Overview.
The
novel
addresses
not
only
the
corruption
of
the
revolution
by
its
leaders
but
also
how
wickedness,
indifference,
ignorance,
greed
and
myopia
destroy
any
possibility
of
a
Utopia.
While
this
novel
portrays
corrupt
leadership
as
the
flaw
in
revolution
(and
not
the
act
of
revolution
itself),
it
also
shows
how
potential
ignorance
and
indifference
to
problems
within
a
revolution
could
allow
horrors
to
happen
if
smooth
transition
to
a
people's
government
isn't
satisfied.
Plot
summary.
Old
Major,
the
old
boar
on
the
Manor
Farm,
calls
the
animals
on
the
farm
for
a
meeting,
where
he
compares
the
humans
to
parasites
and
teaches
the
animals
a
revolutionary
song,
"Beasts
of
England."
When
Major
dies
three
days
later,
two
young
pigs,
Snowball
and
Napoleon,
assume
command
and
turn
his
dream
into
a
philosophy.
The
animals
revolt
and
drive
the
drunken
and
irresponsible
Mr.
Jones
from
the
farm,
renaming
it
"Animal
Farm."
The
Seven
Commandments
of
Animalism
are
written
on
the
wall
of
a
barn.
The
most
important
is
the
seventh,
"All
animals
are
equal."
All
the
animals
work,
but
the
workhorse,
Boxer,
does
more
than
others
and
adopts
the
maxim —
"I
will
work
harder."
Snowball
attempts
to
teach
the
animals
reading
and
writing;
food
is
plentiful;
and
the
farm
runs
smoothly.
The
pigs
elevate
themselves
to
positions
of
leadership
and
set
aside
special
food
items
ostensibly
for
their
personal
health.
Napoleon
takes
the
pups
from
the
farm
dogs
and
trains
them
privately.
When
Mr.
Jones
tries
retaking
the
farm,
the
animals
defeat
him
at
what
they
call
the
"Battle
of
the
Cowshed."
Napoleon
and
Snowball
struggle
for
leadership.
When
Snowball
announces
his
idea
for
a
windmill,
Napoleon
opposes
it.
Snowball
makes
a
speech
in
favour
of
the
windmill,
whereupon
Napoleon
has
his
dogs
chase
Snowball
away.
In
Snowball's
absence,
Napoleon
declares
himself
leader
and
makes
changes.
Meetings
will
no
longer
be
held
and
instead
a
committee
of
pigs
will
run
the
farm.
Using
a
young
pig
named
Squealer
as
a
mouthpiece,
Napoleon
announces
that
Snowball
stole
the
idea
for
the
windmill
from
him.
The
animals
work
harder
with
the
promise
of
easier
lives
with
the
windmill.
After
a
violent
storm,
the
animals
find
the
windmill
annihilated.
Napoleon
and
Squealer
convince
the
animals
that
Snowball
destroyed
the
windmill,
although
the
scorn
of
the
neighbouring
farmers
suggests
the
windmill's
walls
were
too
thin.
Once
Snowball
becomes
a
scapegoat,
Napoleon
begins
purging
the
farm,
killing
animals
he
accuses
of
consorting
with
Snowball.
Meanwhile,
Boxer
takes
up
a
second
maxim:
"Napoleon
is
always
right."
Napoleon
abuses
his
powers,
making
life
harder
for
the
animals;
the
pigs
impose
more
control
while
reserving
privileges
for
themselves.
The
pigs
rewrite
history,
villainizing
Snowball
and
glorifying
Napoleon.
Squealer
justifies
every
statement
Napoleon
makes,
even
the
pigs'
alteration
of
the
Seven
Commandments
of
Animalism.
"No
animal
shall
drink
alcohol"
is
changed
to
"No
animal
shall
drink
alcohol
"to
excess"
when
the
pigs
discover
the
farmer's
whisky.
"Beasts
of
England"
is
banned
as
inappropriate,
as
according
to
Napoleon
the
dream
of
Animal
Farm
has
been
realized.
It
is
replaced
by
an
anthem
glorifying
Napoleon,
who
appears
to
be
adopting
the
lifestyle
of
a
man.
The
animals,
though
cold,
starving,
and
overworked,
remain
convinced
through
psychological
conditioning
that
they
are
better
off
than
they
were
when
ruled
by
Mr.
Jones.
Squealer
abuses
the
animals'
poor
memories
and
invents
numbers
to
show
their
improvement.
Mr.
Frederick,
one
of
the
neighbouring
farmers,
swindles
Napoleon
by
buying
old
wood
with
forged
money,
and
then
attacks
the
farm,
using
blasting
powder
to
blow
up
the
restored
windmill.
Though
the
animals
win
the
battle,
they
do
so
at
great
cost,
as
many,
including
Boxer,
are
wounded.
Boxer
continues
working
harder
and
harder,
until
he
collapses
while
working
on
the
windmill.
Napoleon
sends
for
a
van
to
take
Boxer
to
the
veterinarian,
explaining
that
better
care
can
be
given
there.
Benjamin
the
donkey,
who
"could
read
as
well
as
any
pig",
notices
that
the
van
belongs
to
"Alfred
Simmonds,
Horse
Slaughterer
and
Glue
Boiler",
and
attempts
to
mount
a
rescue;
but
the
animals'
attempts
are
futile.
Squealer
reports
that
the
van
was
purchased
by
the
hospital
and
the
writing
from
the
previous
owner
had
not
been
repainted.
He
recounts
a
tale
of
Boxer's
death
in
the
hands
of
the
best
medical
care.
In
reality,
the
pigs
sent
Boxer
to
his
death
in
exchange
for
money
to
buy
more
whisky.
Years
pass,
and
the
pigs
learn
to
walk
upright,
carry
whips,
and
wear
clothes.
The
Seven
Commandments
are
reduced
to
a
single
phrase:
"All
animals
are
equal,
but
some
animals
are
more
equal
than
others."
Napoleon
holds
a
dinner
party
for
the
pigs
and
the
humans
of
the
area,
who
congratulate
Napoleon
on
having
the
hardest-working
animals
in
the
country
on
the
least
feed.
Napoleon
announces
an
alliance
with
the
humans,
against
the
labouring
classes
of
both
"worlds".
He
abolishes
practices
and
traditions
related
to
the
Revolution,
and
reverts
the
name
of
the
farm
to
"Manor
Farm".
The
animals,
overhearing
the
conversation,
notice
that
the
faces
of
the
pigs
have
begun
changing.
During
a
poker
match,
an
argument
breaks
out
between
Napoleon
and
Mr.
Pilkington
when
they
both
play
the
Ace
of
Spades,
and
the
animals
realize
that
the
faces
of
the
pigs
look
like
the
faces
of
humans
and
no
one
can
tell
the
difference
between
them.
Animalism.
Animalism
is
an
allegorical
mirror
of
the
Soviet
Union,
particularly
between
the
1910s
and
the
1940s,
as
well
as
the
evolution
of
the
view
of
the
Russian
revolutionaries
and
government
of
how
to
practice
it.
It
is
invented
by
the
highly
respected
pig
Old
Major.
The
pigs
Snowball,
Napoleon,
and
Squealer
adapt
Old
Major's
ideas
into
an
actual
philosophy,
which
they
formally
name
Animalism.
Soon
after,
Napoleon
and
Squealer
indulge
in
the
vices
of
humans
(drinking
alcohol,
sleeping
in
beds,
trading).
Squealer
is
employed
to
alter
the
Seven
Commandments
to
account
for
his
humanization,
which
represents
the
Soviet
government's
tweaking
of
communist
theory
to
make
it
more
a
reformation
of
capitalism
than
a
replacement.
Later,
Napoleon
and
his
pigs
are
corrupted
by
the
absolute
power
they
hold
over
the
farm.
To
maintain
their
popularity
with
the
other
animals,
Squealer
secretly
paints
additions
to
some
commandments
to
benefit
the
pigs
while
keeping
them
free
of
accusations
of
breaking
the
laws
(such
as
"No
animal
shall
drink
alcohol"
having
"to
excess"
appended
to
it
and
"No
animal
shall
sleep
in
a
bed"
with
"with
sheets"
added
to
it).
Eventually
the
laws
are
replaced
with
"All
animals
are
equal,
"but
some
animals
are
more
equal
than
others",
and
"Four
legs
good,
two
legs
"better!"
as
the
pigs
become
more
human.
Characters.
The
events
and
characters
in
Animal
Farm
satirise
Communism
("Animalism"),
authoritarian
government
and
human
gullibility
generally;
Snowball
is
seen
as
Leon
Trotsky
and
the
head
pig,
Napoleon,
is
Stalin.
Equines.
There
are
four
main
equine
characters:
Boxer,
Clover,
and
Mollie,
who
are
horses,
and
Benjamin,
who
is
a
donkey.
Boxer
is
a
loyal,
kind,
dedicated,
and
respectful
worker.
He
is
physically
the
strongest
animal
on
the
farm,
but
naive
and
slow,
which
leaves
him
constantly
stating
"I
will
work
harder"
and
"Napoleon
is
always
right"
despite
the
corruption.
Clover
is
Boxer's
companion,
who
constantly
cares
for
him,
and
she
also
acts
as
the
matriarch
for
the
other
horses,
and
other
animals
in
general
(such
as
the
ducklings
she
shelters
with
her
fore-legs
and
hooves
during
Old
Major's
speech).
Mollie
is
a
self-centred,
self-indulgent
and
vain
young
white
mare
who
likes
wearing
ribbons
in
her
mane,
eating
sugar
cubes,
and
being
pampered
and
groomed
by
humans.
She
quickly
leaves
for
another
farm
and
is
only
once
mentioned
again.
Benjamin
is
one
of
the
longest-lived
animals,
has
the
worst
temper
and
one
of
the
few
who
can
read.
Benjamin
is
a
very
dedicated
friend
to
Boxer,
and
does
nothing
to
warn
the
other
animals
of
the
pigs'
corruption,
which
he
secretly
realizes
is
steadily
unfolding.
When
asked
if
he
was
happier
post-Revolution
than
before
the
Revolution,
Benjamin
remarks,
"Donkeys
live
a
long
time.
None
of
you
has
ever
seen
a
dead
donkey."
He
is
cynical
and
pessimistic,
his
most
often
made
statement
being
"Life
will
go
on
as
it
has
always
gone
on
—
that
is,
badly".
But
he
is
also
one
of
the
wisest
animals
on
the
farm,
and
is
able
to
"read
as
well
as
any
pig".
Origin.
George
Orwell
wrote
the
manuscript
in
1943
and
1944
following
his
experiences
during
the
Spanish
Civil
War,
which
he
described
in
his
1938
"Homage
to
Catalonia".
In
the
preface
of
a
1947
Ukrainian
edition
of
Animal
Farm
he
explained
how
escaping
the
communist
purges
in
Spain
taught
him
"how
easily
totalitarian
propaganda
can
control
the
opinion
of
enlightened
people
in
democratic
countries."
This
motivated
Orwell
to
expose
and
strongly
condemn
what
he
saw
as
the
Stalinist
corruption
of
the
original
socialist
ideals.
Orwell
encountered
great
difficulty
getting
the
manuscript
published.
Four
publishers
refused;
one
had
initially
accepted
the
work
but
declined
after
consulting
with
the
Ministry
of
Information.
Eventually
Secker
and
Warburg
published
the
first
edition
in
1945.
Significance.
In
the
Eastern
Bloc
both
"Animal
Farm"
and
later,
also
"Nineteen
Eighty-Four"
were
on
the
list
of
forbidden
books
up
until
"die
Wende"
in
1989,
and
were
only
available
via
clandestine
Samizdat
networks.
The
novel's
"Battle
of
the
Windmill"
is
referred
to
by
Sant
Singh
Bal
as
one
"of
the
important
episodes
which
constitute
the
essence
of
the
plot
of
the
novel."
Harold
Bloom
writes
that
the
"Battle
of
the
Windmill
rings
a
special
bell:
the
repulse
of
the
Duke
of
Brunswick
in
1792,
following
the
Prussian
bombardment
that
made
the
windmill
of
Valmy
famous."
By
contrast,
Peter
Edgerly
Firchow
and
Peter
Hobley
Davison
consider
that
in
real
life,
with
events
in
"Animal
Farm"
mirroring
those
in
the
Soviet
Union,
this
fictional
battle
represents
the
Great
Patriotic
War
(World
War
II),
especially
the
Battle
of
Stalingrad
and
the
Battle
of
Moscow.
Prestwick
House's
"Activity
Pack"
for
"Animal
Farm"
also
identifies
the
Battle
of
the
Windmill
as
an
allegory
for
World
War
II,
while
noting
that
the
"catalyst
for
the
Battle
of
the
Windmill,
though,
is
less
clear."
During
the
battle,
Fredrick
drills
a
hole
and
places
explosives
inside,
and
it
is
followed
by
"All
the
animals,
except
Napoleon"
took
cover;
Orwell
had
the
publisher
alter
this
from
"All
the
animals,
including
Napoleon"
in
recognition
of
Joseph
Stalin's
decision
to
remain
in
Moscow
during
the
German
advance.
The
"Battle
of
the
Cowshed"
represents
the
allied
invasion
of
the
Soviet
Russia
in
1918,
and
the
defeat
of
the
White
Russians
in
the
Russian
Civil
War.
Efforts
to
find
a
publisher.
During
World
War
II
it
became
apparent
to
Orwell
that
anti-Soviet
literature
was
not
something
which
most
major
publishing
houses
would
touch —
including
his
regular
publisher
Gollancz.
He
also
submitted
the
manuscript
to
Faber
and
Faber,
where
the
poet
T.
S.
Eliot
(who
was
a
director
of
the
firm)
also
rejected
it;
Eliot
wrote
back
to
Orwell
praising
its
"good
writing"
and
"fundamental
integrity"
but
declaring
that
they
would
only
accept
it
for
publication
if
they
had
some
sympathy
for
the
viewpoint
"which
I
take
to
be
generally
Trotskyite".
Eliot
said
he
found
the
view
"not
convincing",
and
contended
that
the
pigs
were
made
out
to
be
the
best
to
run
the
farm;
he
posited
that
someone
might
argue
"what
was
needed..
was
not
more
communism
but
more
public-spirited
pigs".
Although
it
was
written
in
1943,
"Animal
Farm"
was
not
published
until
1945
due
to
paper
rationing
and
fear
of
damaging
the
Anglo-Soviet
alliance.
"The
Freedom
of
the
Press".
Orwell
originally
wrote
a
preface
which
complains
about
self-imposed
British
self-censorship
and
how
the
British
people
were
suppressing
criticism
of
the
USSR,
their
World
War
II
ally.
"The
sinister
fact
about
literary
censorship
in
England
is
that
it
is
largely
voluntary. ...
Things
are
kept
right
out
of
the
British
press,
not
because
the
Government
intervenes
but
because
of
a
general
tacit
agreement
that
'it
wouldn't
do'
to
mention
that
particular
fact."
The
preface
itself
was
censored
and
as
of
June
2009
has
not
been
published
with
most
editions
of
the
book.
His
wife
Eileen
Blair
had
worked
during
the
war
at
the
Ministry
of
Information
censoring
newspapers.
Secker
and
Warburg
published
the
first
edition
of
Animal
Farm
in
1945
without
any
introduction.
However,
the
publisher
had
provided
space
for
a
preface
in
the
author's
proof
composited
from
the
manuscript.
For
reasons
unknown,
no
preface
was
supplied
and
all
the
page
numbers
needed
to
be
redone
at
the
last
minute.
Years
later,
in
1972,
Ian
Angus
found
the
original
typescript
titled
"The
Freedom
of
the
Press",
and
Bernard
Crick
published
it,
together
with
his
own
introduction
in
The
Times
Literary
Supplement
on
15
September
1972
as
"How
the
essay
came
to
be
written".
Orwell's
essay
criticized
British
self-censorship
by
the
press,
specifically
the
suppression
of
unflattering
descriptions
of
Stalin
and
the
Soviet
government.
The
same
essay
also
appeared
in
the
Italian
1976
Animal
Farm
edition,
with
another
introduction
by
Crick,
claiming
to
be
the
first
edition
with
the
preface.
Other
publishers
were
still
declining
to
publish
it.
Cultural
references.
References
to
the
novella
are
frequent
in
other
works
of
popular
culture,
particularly
in
popular
music
and
television
series.
Adaptations.
"Animal
Farm"
has
been
adapted
to
film
twice.
The
1954
"Animal
Farm"
film
was
an
animated
feature
and
the
1999
"Animal
Farm"
film
was
a
TV
live
action
version,
both
differ
from
the
novel.
In
the
1954
film
Napoleon
is
overthrown
in
a
second
revolution
while
the
1999
film
shows
Napoleon's
regime
collapsing
in
on
itself,
as
happened
in
the
Soviet
Union.
Editions.
On
July
17,
2009,
Amazon.com
withdrew
certain
Amazon
Kindle
titles,
including
"Animal
Farm"
and
"Nineteen
Eighty-Four"
by
George
Orwell,
from
sale,
refunded
buyers,
and
remotely
deleted
items
from
purchasers'
devices
after
discovering
that
the
publisher
lacked
rights
to
publish
the
titles
in
question.
Notes
and
annotations
for
the
books
made
by
users
on
their
devices
were
also
deleted.
After
the
move
prompted
outcry
and
comparisons
to
"Nineteen
Eighty-Four"
itself,
Amazon
spokesman
Drew
Herdener
stated
that
the
company
is
"…
changing
our
systems
so
that
in
the
future
we
will
not
remove
books
from
customers'
devices
in
these
circumstances."
---END.OF.DOCUMENT---
Amphibian.
Amphibians
(class
Amphibia),
such
as
frogs,
toads,
salamanders,
newts,
and
caecilians,
are
ectothermic
(or
cold-blooded)
animals
that
either
metamorphose
from
a
juvenile
water-breathing
form,
to
an
adult
air-breathing
form,
or
paedomorph
and
retain
some
juvenile
characteristics.
Mudpuppies
and
waterdogs
are
good
examples
of
paedomorphic
species.
Though
amphibians
typically
have
four
limbs,
the
caecilians
are
notable
for
being
limbless.
Unlike
other
land
vertebrates
(amniotes),
amphibians
lay
eggs
in
water.
Amphibians
are
superficially
similar
to
reptiles.
Amphibians
are
ecological
indicators,
and
in
recent
decades
there
has
been
a
dramatic
decline
in
amphibian
populations
around
the
globe.
Many
species
are
now
threatened
or
extinct.
Amphibians
evolved
in
the
Devonian
Period
and
were
top
predators
in
the
Carboniferous
and
Permian
Periods,
but
many
lineages
were
wiped
out
during
the
Permian–Triassic
extinction.
One
group,
the
metoposaurs,
remained
important
predators
during
the
Triassic,
but
as
the
world
became
drier
during
the
Early
Jurassic
they
died
out,
leaving
a
handful
of
relict
temnospondyls
like
"Koolasuchus"
and
the
modern
orders
of
Lissamphibia.
Etymology.
Amphibian
is
derived
from
the
Ancient
Greek
term
ἀμφίβιος
"amphíbios"
which
means
both
kinds
of
life,
"amphi"
meaning
“both”
and
"bio"
meaning
life.
The
term
was
initially
used
for
all
kinds
of
combined
natures.
Eventually
it
was
used
to
refer
to
animals
that
live
both
in
the
water
and
on
land.
Evolutionary
history.
The
first
major
groups
of
amphibians
developed
in
the
Devonian
Period
from
fish
similar
to
the
modern
coelacanth
and
lungfish
which
had
evolved
multi-jointed
leg-like
fins
that
enabled
them
to
crawl
along
the
sea
bottom.
These
amphibians
were
as
much
as
one
to
five
meters
in
length.
However,
amphibians
never
developed
the
ability
to
live
their
entire
lives
on
land,
having
to
return
to
water
to
lay
their
shell-less
eggs.
In
the
Carboniferous
Period,
the
amphibians
moved
up
in
the
food
chain
and
began
to
occupy
the
ecological
position
currently
occupied
by
crocodiles.
These
amphibians
were
notable
for
eating
the
mega
insects
on
land
and
many
types
of
fishes
in
the
water.
During
the
Triassic
Period,
the
better
land-adapted
proto-crocodiles
began
to
compete
with
amphibians,
leading
to
their
reduction
in
size
and
importance
in
the
biosphere.
Taxonomy.
Of
these
only
the
last
subclass
includes
recent
species.
With
the
phylogenetic
revolution,
this
classification
has
been
modified,
or
changed,
and
the
Labyrinthodontia
discarded
as
being
a
paraphyletic
group
without
unique
defining
features
apart
from
shared
primitive
characteristics.
Classification
varies
according
to
the
preferred
phylogeny
of
the
author,
whether
they
use
a
stem-based
or
node-based
classification.
Generally
amphibians
are
defined
as
the
group
that
includes
the
common
ancestors
of
all
living
amphibians
(frogs,
salamanders,
etc.)
and
all
their
descendants.
This
may
also
include
extinct
groups
like
the
temnospondyls
(traditionally
placed
in
the
disbanded
subclass
“labyrinthodontia”),
and
the
Lepospondyls.
This
means
that
there
are
a
now
large
number
of
basal
Devonian
and
Carboniferous
tetrapod
groups,
described
as
“amphibians”
in
earlier
books,
that
are
no
longer
placed
in
the
formal
Amphibia.
All
recent
amphibians
are
included
in
the
subclass
Lissamphibia,
superorder
Salientia,
which
is
usually
considered
a
clade
(which
means
that
it
is
thought
that
they
evolved
from
a
common
ancestor
apart
from
other
extinct
groups),
although
it
has
also
been
suggested
that
salamanders
arose
separately
from
a
temnospondyl-like
ancestor.
Authorities
also
disagree
on
whether
Salientia
is
a
Superorder
that
includes
the
order
Anura,
or
whether
Anura
is
a
sub-order
of
the
order
Salientia.
Practical
considerations
seem
to
favor
using
the
former
arrangement
now.
The
Lissamphibia,
superorder
Salientia,
are
traditionally
divided
into
three
orders,
but
an
extinct
salamander-like
family,
the
Albanerpetontidae,
is
now
considered
part
of
the
Lissamphibia,
besides
the
superorder
Salientia.
Furthermore,
Salientia
includes
all
three
recent
orders
plus
a
single
Triassic
proto-frog,
"Triadobatrachus".
The
actual
number
of
species
partly
also
depends
on
the
taxonomic
classification
followed,
the
two
most
common
classifications
being
the
classification
of
the
website
AmphibiaWeb,
University
of
California
(Berkeley)
and
the
classification
by
herpetologist
Darrel
Frost
and
The
American
Museum
of
Natural
History,
available
as
the
online
reference
database
Amphibian
Species
of
the
World.
The
numbers
of
species
cited
above
follow
Frost.
Reproductive
system.
For
the
purpose
of
reproduction
most
amphibians
require
fresh
water.
A
few
(e.g.
"Fejervarya
raja")
can
inhabit
brackish
water
and
even
survive
(though
not
thrive)
in
seawater,
but
there
are
no
true
marine
amphibians.
Several
hundred
frog
species
in
adaptive
radiations
(e.g.,
"Eleutherodactylus",
the
Pacific
Platymantines,
the
Australo-Papuan
microhylids,
and
many
other
tropical
frogs),
however,
do
not
need
any
water
for
breeding
in
the
wild.
They
reproduce
via
direct
development,
an
ecological
and
evolutionary
adaptation
that
has
allowed
them
to
be
completely
independent
from
free-standing
water.
Almost
all
of
these
frogs
live
in
wet
tropical
rainforests
and
their
eggs
hatch
directly
into
miniature
versions
of
the
adult,
passing
through
the
tadpole
stage
within
the
egg.
Several
species
have
also
adapted
to
arid
and
semi-arid
environments,
but
most
of
them
still
need
water
to
lay
their
eggs.
Symbiosis
with
single
celled
algae
that
lives
in
the
jelly-like
layer
of
the
eggs
has
evolved
several
times.
The
larvae
(tadpoles
or
polliwogs)
breathe
with
exterior
gills.
After
hatching,
they
start
to
transform
gradually
into
the
adult's
appearance.
This
process
is
called
metamorphosis.
Typically,
the
animals
then
leave
the
water
and
become
terrestrial
adults,
but
there
are
many
interesting
exceptions
to
this
general
way
of
reproduction.
Conservation.
Dramatic
declines
in
amphibian
populations,
including
population
crashes
and
mass
localized
extinction,
have
been
noted
in
the
past
two
decades
from
locations
all
over
the
world,
and
amphibian
declines
are
thus
perceived
as
one
of
the
most
critical
threats
to
global
biodiversity.
A
number
of
causes
are
believed
to
be
involved,
including
habitat
destruction
and
modification,
over-exploitation,
pollution,
introduced
species,
climate
change,
endocrine-disrupting
pollutants,
destruction
of
the
ozone
layer
(ultraviolet
radiation
has
shown
to
be
especially
damaging
to
the
skin,
eyes,
and
eggs
of
amphibians),
and
diseases
like
chytridiomycosis.
However,
many
of
the
causes
of
amphibian
declines
are
still
poorly
understood,
and
are
a
topic
of
ongoing
discussion.
A
global
strategy
to
stem
the
crisis
has
been
released
in
the
form
of
the
Amphibian
Conservation
Action
Plan
(available
at
http://www.amphibians.org).
Developed
by
over
80
leading
experts
in
the
field,
this
call
to
action
details
what
would
be
required
to
curtail
amphibian
declines
and
extinctions
over
the
next
5
years
-
and
how
much
this
would
cost.
The
Amphibian
Specialist
Group
of
the
World
Conservation
Union
(IUCN)
is
spearheading
efforts
to
implement
a
comprehensive
global
strategy
for
amphibian
conservation.
On
January
21,
2008,
Evolutionarily
Distinct
and
Globally
Endangered
(EDGE),
as
given
by
chief
Helen
Meredith,
identified
nature's
most
endangered
species:
"The
EDGE
amphibians
are
amongst
the
most
remarkable
and
unusual
species
on
the
planet
and
yet
an
alarming
85%
of
the
top
100
are
receiving
little
or
no
conservation
attention."
The
top
10
endangered
species
(in
the
List
of
endangered
animal
species)
include:
the
Chinese
giant
salamander,
a
distant
relative
of
the
newt,
the
tiny
Gardiner's
Seychelles,
the
limbless
Sagalla
caecilian,
South
African
ghost
frogs,
lungless
Mexican
salamanders,
the
Malagasy
rainbow
frog,
Chile's
Darwin
frog
(Rhinoderma
rufum)
and
the
Betic
Midwife
Toad.
---END.OF.DOCUMENT---
Alaska.
Alaska
()
is
the
largest
state
of
the
United
States
by
area;
it
is
situated
in
the
northwest
extremity
of
the
North
American
continent,
with
Canada
to
the
east,
the
Arctic
Ocean
to
the
north,
and
the
Pacific
Ocean
to
the
west
and
south,
with
Russia
further
west
across
the
Bering
Strait.
Approximately
half
of
Alaska's
698,473
residents
live
within
the
Anchorage
metropolitan
area.
As
of
2009,
Alaska
remains
the
least
densely
populated
state
of
the
U.S.
The
U.S.
Senate
approved
the
purchase
of
Alaska
from
the
Russian
Empire
on
March
30,
1867,
for
$7.2 million
at
about
two
cents
per
acre
($4.74/km2).
The
land
went
through
several
administrative
changes
before
becoming
an
organized
territory
on
May
11,
1912,
and
the
49th
state
of
the
U.S.
on
January
3,
1959.
The
name
"Alaska"
(Аляска)
was
already
introduced
in
the
Russian
colonial
time,
when
it
was
used
only
for
the
peninsula
and
is
derived
from
the
Aleut
"alaxsxaq",
meaning
"the
mainland"
or
more
literally,
"the
object
towards
which
the
action
of
the
sea
is
directed".
It
is
also
known
as
Alyeska,
the
"great
land",
an
Aleut
word
derived
from
the
same
root.
Geography.
Alaska
has
a
longer
coastline
than
all
the
other
U.S.
states
combined.
It
is
the
only
non-contiguous
U.S.
state
on
continental
North
America;
about
of
British
Columbia
(Canada)
separate
Alaska
from
Washington
state.
Alaska
is
thus
an
exclave
of
the
United
States.
It
is
technically
part
of
the
continental
U.S.,
but
is
often
not
included
in
colloquial
use;
Alaska
is
not
part
of
the
contiguous
U.S.,
often
called
"the
Lower
48."
The
capital
city,
Juneau,
is
situated
on
the
mainland
of
the
North
American
continent,
but
is
not
connected
by
road
to
the
rest
of
the
North
American
highway
system.
The
state
is
bordered
by
the
Yukon
Territory
and
British
Columbia
in
Canada,
to
the
east,
the
Gulf
of
Alaska
and
the
Pacific
Ocean
to
the
south,
the
Bering
Sea,
Bering
Strait,
and
Chukchi
Sea
to
the
west
and
the
Arctic
Ocean
to
the
north.
Alaska's
territorial
waters
touch
Russia's
territorial
waters
in
the
Bering
Strait,
as
the
Russian
and
Alaskan
islands
are
only
apart.
As
it
extends
into
the
eastern
hemisphere,
it
is
technically
both
the
westernmost
and
easternmost
state
in
the
United
States,
as
well
as
also
being
the
northernmost.
Alaska
is
the
largest
state
in
the
United
States
in
land
area
at,
over
twice
the
size
of
Texas,
the
next
largest
state.
Alaska
is
larger
than
all
but
18
sovereign
countries.
Counting
territorial
waters,
Alaska
is
larger
than
the
combined
area
of
the
next
three
largest
states:
Texas,
California,
and
Montana.
It
is
also
larger
than
the
combined
area
of
the
22
smallest
U.S.
states.
The
International
Date
Line
was
drawn
west
of
180°
to
keep
the
whole
state,
and
thus
the
entire
North
American
continent,
within
the
same
legal
day.
Natural
features.
With
its
myriad
islands,
Alaska
has
nearly
of
tidal
shoreline.
The
Aleutian
Islands
chain
extends
west
from
the
southern
tip
of
the
Alaska
Peninsula.
Many
active
volcanoes
are
found
in
the
Aleutians.
Unimak
Island,
for
example,
is
home
to
Mount
Shishaldin,
which
is
an
occasionally
smoldering
volcano
that
rises
to
above
the
North
Pacific.
It
is
the
most
perfect
volcanic
cone
on
Earth,
even
more
symmetrical
than
Japan's
Mount
Fuji.
The
chain
of
volcanoes
extends
to
Mount
Spurr,
west
of
Anchorage
on
the
mainland.
Alaska
has
more
volcanoes
than
any
other
state.
Geologists
have
identified
Alaska
as
part
of
Wrangellia,
a
large
region
consisting
of
multiple
states
and
Canadian
provinces
in
the
Pacific
Northwest
which
is
actively
undergoing
continent
building.
One
of
the
world's
largest
tides
occurs
in
Turnagain
Arm,
just
south
of
Anchorage –
tidal
differences
can
be
more
than.
(Many
sources
say
Turnagain
has
the
second-greatest
tides
in
North
America,
but
several
areas
in
Canada
have
larger
tides.)
Alaska
has
more
than
three
million
lakes.
Marshlands
and
wetland
permafrost
cover
(mostly
in
northern,
western
and
southwest
flatlands).
Glacier
ice
covers
some
of
land
and
of
tidal
zone.
The
Bering
Glacier
complex
near
the
southeastern
border
with
Yukon
covers
alone.
With
over
100,000
of
them,
Alaska
has
half
of
the
world's
glaciers.
Land
ownership.
According
to
an
October
1998
report
by
the
United
States
Bureau
of
Land
Management,
approximately
65%
of
Alaska
is
owned
and
managed
by
the
U.S.
federal
government
as
public
lands,
including
a
multitude
of
national
forests,
national
parks,
and
national
wildlife
refuges.
Of
these,
the
Bureau
of
Land
Management
manages
87
million
acres
(350,000 km²),
or
23.8%
of
the
state.
The
Arctic
National
Wildlife
Refuge
is
managed
by
the
United
States
Fish
and
Wildlife
Service.
It
is
the
world's
largest
wildlife
refuge,
comprising.
Of
the
remaining
land
area,
the
State
of
Alaska
owns;
another
are
owned
by
12
regional
and
dozens
of
local
Native
corporations
created
under
the
Alaska
Native
Claims
Settlement
Act.
Thus,
indirectly,
the
84,000
Eskimo,
Aleut
and
American
Indian
inhabitants
of
Alaska
own
one-ninth
of
the
state.
Various
private
interests
own
the
remaining
land,
totaling
about
one
percent
of
the
state.
Climate.
The
climate
in
Juneau
and
the
southeast
panhandle
is
a
mid-latitude
oceanic
climate
(Köppen
climate
classification
"Cfb")
in
the
southern
sections
and
a
subarctic
oceanic
climate
(Köppen
"Cfc")
in
the
northern
parts.
On
an
annual
basis,
the
panhandle
is
both
the
wettest
and
warmest
part
of
Alaska
with
milder
temperatures
in
the
winter
and
high
precipitation
throughout
the
year.
Juneau
averages
over
of
precipitation
a
year,
while
other
areas
receive
over.
This
is
also
the
only
region
in
Alaska
in
which
the
average
daytime
high
temperature
is
above
freezing
during
the
winter
months.
The
climate
of
Anchorage
and
south
central
Alaska
is
mild
by
Alaskan
standards
due
to
the
region's
proximity
to
the
seacoast.
While
the
area
gets
less
rain
than
southeast
Alaska,
it
gets
more
snow,
and
days
tend
to
be
clearer.
On
average,
Anchorage
receives
of
precipitation
a
year,
with
around
of
snow,
although
there
are
areas
in
the
south
central
which
receive
far
more
snow.
It
is
a
subarctic
climate
(Köppen
"Dfc")
due
to
its
brief,
cool
summers.
The
climate
of
Western
Alaska
is
determined
in
large
part
by
the
Bering
Sea
and
the
Gulf
of
Alaska.
It
is
a
subarctic
oceanic
climate
in
the
southwest
and
a
continental
subarctic
climate
farther
north.
The
temperature
is
somewhat
moderate
considering
how
far
north
the
area
is.
This
area
has
a
tremendous
amount
of
variety
in
precipitation.
The
northern
side
of
the
Seward
Peninsula
is
technically
a
desert
with
less
than
of
precipitation
annually,
while
some
locations
between
Dillingham
and
Bethel
average
around
of
precipitation.
The
climate
of
the
interior
of
Alaska
is
subarctic.
Some
of
the
highest
and
lowest
temperatures
in
Alaska
occur
around
the
area
near
Fairbanks.
The
summers
may
have
temperatures
reaching
into
the
90s°F
(the
low
to
mid
30s
°C),
while
in
the
winter,
the
temperature
can
fall
below
−60
°F
(-52
°C).
Precipitation
is
sparse
in
the
Interior,
often
less
than
a
year,
but
what
precipitation
falls
in
the
winter
tends
to
stay
the
entire
winter.
The
highest
and
lowest
recorded
temperatures
in
Alaska
are
both
in
the
Interior.
The
highest
is
100
°F
(38
°C)
in
Fort
Yukon
(which
is
just
inside
the
arctic
circle)
on
June
27,
1915,
tied
with
Pahala,
Hawaii
as
the
lowest
high
temperature
in
the
United
States.
The
lowest
official
Alaska
temperature
is
−80
°F
(-62
°C)
in
Prospect
Creek
on
January
23,
1971,
one
degree
above
the
lowest
temperature
recorded
in
continental
North
America
(in
Snag,
Yukon,
Canada).
The
climate
in
the
extreme
north
of
Alaska
is
Arctic
(Köppen
"ET")
with
long,
very
cold
winters
and
short,
cool
summers.
Even
in
July,
the
average
low
temperature
in
Barrow
is
34
°F
(1
°C).
Precipitation
is
light
in
this
part
of
Alaska,
with
many
places
averaging
less
than
per
year,
mostly
as
snow
which
stays
on
the
ground
almost
the
entire
year.
History.
The
first
European
contact
with
Alaska
occurred
in
1741,
when
Vitus
Bering
led
an
expedition
for
the
Russian
Navy
aboard
the
"St.
Peter".
After
his
crew
returned
to
Russia
bearing
sea
otter
pelts
judged
to
be
the
finest
fur
in
the
world,
small
associations
of
fur
traders
began
to
sail
from
the
shores
of
Siberia
towards
the
Aleutian
islands.
The
first
permanent
European
settlement
was
founded
in
1784,
and
the
Russian-American
Company
carried
out
an
expanded
colonization
program
during
the
early
to
mid-1800s.
New
Archangel
on
Kodiak
Island
was
Alaska's
first
capital,
but
for
a
century
under
both
Russia
and
the
U.S.
Sitka
was
the
capital.
The
Russians
never
fully
colonized
Alaska,
and
the
colony
was
never
very
profitable.
William
H.
Seward,
the
U.S.
Secretary
of
State,
negotiated
the
Alaskan
purchase
with
the
Russians
in
1867
for
$7.2 million.
Alaska
was
loosely
governed
by
the
military
initially,
and
was
unofficially
a
territory
of
the
United
States
from
1884
on.
In
the
1890s,
gold
rushes
in
Alaska
and
the
nearby
Yukon
Territory
brought
thousands
of
miners
and
settlers
to
Alaska.
Alaska
was
granted
official
territorial
status
in
1912.
At
this
time
the
capital
was
moved
to
Juneau.
During
World
War
II,
the
Aleutian
Islands
Campaign
focused
on
the
three
outer
Aleutian
Islands –
Attu,
Agattu
and
Kiska
–
that
were
invaded
by
Japanese
troops
and
occupied
between
June
1942
and
August
1943.
Unalaska/Dutch
Harbor
became
a
significant
base
for
the
U.S.
Army
Air
Corps
and
Navy
submariners.
The
U.S.
Lend-Lease
program
involved
the
flying
of
American
warplanes
through
Canada
to
Fairbanks
and
thence
Nome;
Soviet
pilots
took
possession
of
these
aircraft,
ferrying
them
to
fight
the
German
invasion
of
the
Soviet
Union.
The
construction
of
military
bases
contributed
to
the
population
growth
of
some
Alaskan
cities.
Statehood
was
approved
on
July
7,
1958.
Alaska
was
officially
proclaimed
a
state
on
January
3,
1959.
In
1964,
the
massive
"Good
Friday
Earthquake"
killed
131
people
and
destroyed
several
villages,
mainly
by
the
resultant
tsunamis.
It
was
the
third
most
powerful
earthquake
in
the
recorded
history
of
the
world,
with
a
moment
magnitude
of
9.2.
It
was
over
one
thousand
times
more
powerful
than
the
1989
San
Francisco
earthquake.
Luckily,
the
epicenter
was
in
an
unpopulated
area
or
thousands
more
would
have
been
killed.
The
1968
discovery
of
oil
at
Prudhoe
Bay
and
the
1977
completion
of
the
Trans-Alaska
Pipeline
led
to
an
oil
boom.
In
1989,
the
"Exxon
Valdez"
hit
a
reef
in
the
Prince
William
Sound,
spilling
over
11
million
gallons
of
crude
oil
over
1,100 miles
(1,600 km)
of
coastline.
Today,
the
battle
between
philosophies
of
development
and
conservation
is
seen
in
the
contentious
debate
over
oil
drilling
in
the
Arctic
National
Wildlife
Refuge.
Demographics.
The
United
States
Census
Bureau,
as
of
July
1,
2008,
estimated
Alaska's
population
at
686,293,
which
represents
an
increase
of
59,361,
or
9.5%,
since
the
last
census
in
2000.
This
includes
a
natural
increase
since
the
last
census
of
60,994
people
(that
is
86,062
births
minus
25,068
deaths)
and
a
decrease
due
to
net
migration
of
5,469
people
out
of
the
state.
Immigration
from
outside
the
U.S.
resulted
in
a
net
increase
of
4,418
people,
and
migration
within
the
country
produced
a
net
loss
of
9,887
people.
In
2000
Alaska
ranked
the
48th
state
by
population,
ahead
of
Vermont
and
Wyoming
(and
Washington
D.C.).
Alaska
is
the
least
densely
populated
state,
and
one
of
the
most
sparsely
populated
areas
in
the
world,
at
1.0
person
per
square
mile
(0.42/km²),
with
the
next
state,
Wyoming,
at
5.1
per
square
mile
(1.97/km²).
Alaska
is
the
largest
U.S.
state
by
area,
and
the
sixth
wealthiest
(per
capita
income).
Race
and
ancestry.
According
to
the
2000
U.S.
Census,
White
Americans
made
up
69.3%
of
Alaska's
population.
African
Americans
made
up
3.5%
of
Alaska's
population.
In
addition,
American
Indians
and
Alaska
Natives
were
the
largest
minority
group;
they
made
up
15.6%
of
Alaska's
population.
Asian
Americans
made
up
4.0%
of
Alaska's
population.
Pacific
Islander
Americans
made
up
0.5%
of
Alaska's
population.
Individuals
from
some
other
race
made
up
1.6%
of
Alaska's
population
while
individuals
from
two
or
more
races
made
up
5.4%
of
the
state's
population.
In
addition,
Hispanics
and
Latinos
made
up
4.1%
of
Alaska's
population.
In
terms
of
ancestry,
German
Americans
were
the
largest
single
ethnic
group
in
Alaska;
they
made
up
16.6%
of
Alaska's
population
and
they
were
the
only
ethnic
group
in
the
state
to
number
over
100,000
members.
Irish
Americans
made
up
10.8%
of
Alaska's
population
while
English
Americans
made
up
9.6%
of
the
state's
population.
Norwegian
Americans
made
up
4.2%
of
Alaska's
population
and
French
Americans
made
up
3.2%
of
the
state's
population.
As
of
the
2005–2007
American
Community
Survey
conducted
by
the
U.S.
Census
Bureau,
White
Americans
made
up
68.5%
of
Alaska's
population.
Blacks
or
African
Americans
made
up
3.8%
of
Alaska's
population.
American
Indians
and
Alaska
Natives
made
up
13.4%
of
Alaska's
population;
still
remaining
the
largest
minority
group.
Asian
Americans
made
up
4.6%
of
Alaska's
population.
Pacific
Islander
Americans
remained
at
0.5%
of
the
state's
population.
Individuals
from
some
other
race
made
up
1.9%
of
Alaska's
population
while
individuals
from
two
or
more
races
made
up
7.2%
of
the
state's
population.
Hispanics
or
Latinos
made
up
5.5%
of
Alaska's
population.
In
terms
of
ancestry,
German
Americans
remained
the
largest
single
ethnic
group
in
Alaska;
they
made
up
19.3%
of
Alaska's
population
and
were
still
the
only
ethnic
group
in
the
state
with
over
100,000
members.
Irish
Americans
made
up
12.5%
of
Alaska's
population
while
English
Americans
made
up
10.8%
of
the
state's
population.
Norwegian
Americans
remained
at
4.2%
of
Alaska's
population
and
French
Americans
made
up
3.6%
of
the
state's
population.
Languages.
According
to
the
2005–2007
American
Community
Survey,
84.7%
of
people
over
the
age
of
five
speak
only
English
at
home.
About
3.5%
speak
Spanish
at
home.
About
2.2%
speak
another
Indo-European
language
at
home
and
about
4.3%
speak
an
Asian
language
at
home.
And
about
5.3%
speak
other
languages
at
home.
A
total
of
5.2%
of
Alaskans
speak
one
of
the
state's
22
indigenous
languages,
known
locally
as
"native
languages".
These
languages
belong
to
two
major
language
families:
Eskimo-Aleut
and
Na-Dene.
As
the
homeland
of
these
two
major
language
families
of
North
America,
Alaska
has
been
described
as
the
crossroads
of
the
continent,
providing
evidence
for
the
recent
settlement
of
North
America
by
way
of
the
Bering
land
bridge.
Religion.
Alaska
has
been
identified,
along
with
Pacific
Northwest
states
Washington
and
Oregon,
as
being
the
least
religious
in
the
U.S.
According
to
statistics
collected
by
the
Association
of
Religion
Data
Archives,
about
39%
of
Alaska
residents
were
members
of
religious
congregations.
Evangelical
Protestants
had
78,070
members,
Roman
Catholics
had
54,359,
and
mainline
Protestants
had
37,156.
After
Catholicism,
the
largest
single
denominations
are
The
Church
of
Jesus
Christ
of
Latter
Day
Saints
(Mormons/LDS)
with
29,460,
Southern
Baptists
with
22,959,
and
Orthodox
with
20,000.
The
large
Eastern
Orthodox
(with
49
parishes
and
up
to
50,000
followers)
population
is
a
result
of
early
Russian
colonization
and
missionary
work
among
Alaska
Natives.
In
1795,
the
First
Russian
Orthodox
Church
was
established
in
Kodiak.
Intermarriage
with
Alaskan
Natives
helped
the
Russian
immigrants
integrate
into
society.
As
a
result,
an
increasing
number
of
Russian
Orthodox
churches
gradually
became
established
within
Alaska.
Alaska
also
has
the
largest
Quaker
population
(by
percentage)
of
any
state.
In
2003
there
were
3,000
Jews
in
Alaska
(for
whom
observance
of
the
mitzvah
may
pose
special
problems).
Estimates
for
the
number
of
Alaskan
Muslims
range
from
2,000
to
5,000.
Alaskan
Hindus
often
share
venues
and
celebrations
with
members
of
other
religious
communities
including
Sikhs
and
Jains.
Economy.
The
2007
gross
state
product
was
$44.9
billion,
45th
in
the
nation.
Its
per
capita
personal
income
for
2007
was
$40,042,
ranking
15th
in
the
nation.
The
oil
and
gas
industry
dominates
the
Alaskan
economy,
with
more
than
80%
of
the
state's
revenues
derived
from
petroleum
extraction.
Alaska's
main
export
product
(excluding
oil
and
natural
gas)
is
seafood,
primarily
salmon,
cod,
Pollock
and
crab.
Agriculture
represents
only
a
fraction
of
the
Alaskan
economy.
Agricultural
production
is
primarily
for
consumption
within
the
state
and
includes
nursery
stock,
dairy
products,
vegetables,
and
livestock.
Manufacturing
is
limited,
with
most
foodstuffs
and
general
goods
imported
from
elsewhere.
Employment
is
primarily
in
government
and
industries
such
as
natural
resource
extraction,
shipping,
and
transportation.
Military
bases
are
a
significant
component
of
the
economy
in
both
Fairbanks
and
Anchorage.
Federal
subsidies
are
also
an
important
part
of
the
economy,
allowing
the
state
to
keep
taxes
low.
Its
industrial
outputs
are
crude
petroleum,
natural
gas,
coal,
gold,
precious
metals,
zinc
and
other
mining,
seafood
processing,
timber
and
wood
products.
There
is
also
a
growing
service
and
tourism
sector.
Tourists
have
contributed
to
the
economy
by
supporting
local
lodging.
Energy.
Alaska
has
vast
energy
resources.
Major
oil
and
gas
reserves
are
found
in
the
Alaska
North
Slope
(ANS)
and
Cook
Inlet
basins.
According
to
the
Energy
Information
Administration,
Alaska
ranks
second
in
the
nation
in
crude
oil
production.
Prudhoe
Bay
on
Alaska's
North
Slope
is
the
highest
yielding
oil
field
in
the
United
States
and
on
North
America,
typically
producing
about.
The
Trans-Alaska
Pipeline
can
pump
up
to
of
crude
oil
per
day,
more
than
any
other
crude
oil
pipeline
in
the
United
States.
Additionally,
substantial
coal
deposits
are
found
in
Alaska's
bituminous,
sub-bituminous,
and
lignite
coal
basins.
The
United
States
Geological
Survey
estimates
that
there
are
of
undiscovered,
technically
recoverable
gas
from
natural
gas
hydrates
on
the
Alaskan
North
Slope.
Alaska
also
offers
some
of
the
highest
hydroelectric
power
potential
in
the
country
from
its
numerous
rivers.
Large
swaths
of
the
Alaskan
coastline
offer
wind
and
geothermal
energy
potential
as
well.
Alaska's
economy
depends
heavily
on
increasingly
expensive
diesel
fuel
for
heating,
transportation,
electric
power
and
light.
Though
wind
and
hydroelectric
power
are
abundant
and
underdeveloped,
proposals
for
state-wide
energy
systems
(e.g.
with
special
low-cost
electric
interties)
were
judged
uneconomical
(at
the
time
of
the
report,
2001)
due
to
low
(<$0.50/Gal)
fuel
prices,
long
distances
and
low
population.
The
cost
of
a
gallon
of
gas
in
urban
Alaska
today
is
usually
$0.30-$0.60
higher
than
the
national
average;
prices
in
rural
areas
are
generally
significantly
higher
but
vary
widely
depending
on
transportation
costs,
seasonal
usage
peaks,
nearby
petroleum
development
infrastructure
and
many
other
factors.
Alaska
accounts
for
one-fifth
(20
percent)
of
domestically
produced
United
States
oil
production.
Prudhoe
Bay
(North
America's
largest
oil
field)
alone
accounts
for
8%
of
the
U.S.
domestic
oil
production.
Permanent
Fund.
The
Alaska
Permanent
Fund
is
a
legislatively
controlled
appropriation
established
in
1976
to
manage
a
surplus
in
state
petroleum
revenues
from
the
recently
constructed
Trans-Alaska
Pipeline
System.
From
its
initial
principal
of
$734,000,
the
fund
has
grown
to
$40
billion
as
a
result
of
oil
royalties
and
capital
investment
programs.
Starting
in
1982,
dividends
from
the
fund's
annual
growth
have
been
paid
out
each
year
to
eligible
Alaskans,
ranging
from
$331.29
in
1984
to
$3,269.00
in
2008
(which
included
a
one-time
$1200
"Resource
Rebate").
Every
year,
the
state
legislature
takes
out
8
percent
from
the
earnings,
puts
3
percent
back
into
the
principal
for
inflation
proofing,
and
the
remaining
5
percent
is
distributed
to
all
qualifying
Alaskans.
To
qualify
for
the
Alaska
State
Permanent
Fund
one
must
have
lived
in
the
state
for
a
minimum
of
12
months,
and
maintain
constant
residency.
Cost
of
living.
The
cost
of
goods
in
Alaska
has
long
been
higher
than
in
the
contiguous
48
states.
This
has
changed
for
the
most
part
in
Anchorage
and
to
a
lesser
extent
in
Fairbanks,
where
the
cost
of
living
has
dropped
somewhat
in
the
past
five
years.
Federal
government
employees,
particularly
United
States
Postal
Service
(USPS)
workers
and
active-duty
military
members,
receive
a
Cost
of
Living
Allowance
usually
set
at
25%
of
base
pay
because,
while
the
cost
of
living
has
gone
down,
it
is
still
one
of
the
highest
in
the
country.
The
introduction
of
big-box
stores
in
Anchorage,
Fairbanks
(Wal-Mart
in
March
2004),
and
Juneau
also
did
much
to
lower
prices.
However,
rural
Alaska
suffers
from
extremely
high
prices
for
food
and
consumer
goods,
compared
to
the
rest
of
the
country
due
to
the
relatively
limited
transportation
infrastructure.
Many
rural
residents
come
into
these
cities
and
purchase
food
and
goods
in
bulk
from
warehouse
clubs
like
Costco
and
Sam's
Club.
Some
have
embraced
the
free
shipping
offers
of
some
online
retailers
to
purchase
items
much
more
cheaply
than
they
could
in
their
own
communities,
if
they
are
available
at
all.
Agriculture.
Due
to
the
northern
climate
and
steep
terrain,
relatively
little
farming
occurs
in
Alaska.
Most
farms
are
in
either
the
Matanuska
Valley,
about
northeast
of
Anchorage,
or
on
the
Kenai
Peninsula,
about
southwest
of
Anchorage.
The
short
100-day
growing
season
limits
the
crops
that
can
be
grown,
but
the
long
sunny
summer
days
make
for
productive
growing
seasons.
The
primary
crops
are
potatoes,
carrots,
lettuce,
and
cabbage.
Farmers
exhibit
produce
at
the
Alaska
State
Fair.
"Alaska
Grown"
is
used
as
an
agricultural
slogan.
Alaska
has
an
abundance
of
seafood,
with
the
primary
fisheries
in
the
Bering
Sea
and
the
North
Pacific,
and
seafood
is
one
of
the
few
food
items
that
is
often
cheaper
within
the
state
than
outside
it.
Many
Alaskans
fish
the
rivers
during
salmon
season
to
gather
significant
quantities
of
their
household
diet
while
fishing
for
subsistence,
sport,
or
both.
Hunting
for
subsistence,
primarily
caribou,
moose,
and
Dall
sheep
is
still
common
in
the
state,
particularly
in
remote
Bush
communities.
An
example
of
a
traditional
native
food
is
Akutaq,
the
Eskimo
ice
cream,
which
can
consist
of
reindeer
fat,
seal
oil,
dried
fish
meat
and
local
berries.
Most
food
in
Alaska
is
transported
into
the
state
from
"outside",
and
shipping
costs
make
food
in
the
cities
relatively
expensive.
In
rural
areas,
subsistence
hunting
and
gathering
is
an
essential
activity
because
imported
food
is
prohibitively
expensive.
The
cost
of
importing
food
to
villages
begins
at
7¢
per
pound
(15¢/kg)
and
rises
rapidly
to
50¢
per
pound
($1.10/kg)
or
more.
The
cost
of
delivering
a
seven-pound
gallon
of
milk
is
about
$3.50
in
many
villages
where
per
capita
income
can
be
$20,000
or
less.
Fuel
for
snow
machines
and
boats
that
consume
a
couple
of
gallons
per
hour
can
exceed
$8.00
per
gallon.
Roads.
Alaska
has
few
road
connections
compared
to
the
rest
of
the
U.S.
The
state's
road
system
covers
a
relatively
small
area
of
the
state,
linking
the
central
population
centers
and
the
Alaska
Highway,
the
principal
route
out
of
the
state
through
Canada.
The
state
capital,
Juneau,
is
not
accessible
by
road,
only
a
car
ferry,
which
has
spurred
several
debates
over
the
decades
about
moving
the
capital
to
a
city
on
the
road
system,
or
building
a
road
connection
from
Haines.
The
western
part
of
Alaska
has
no
road
system
connecting
the
communities
with
the
rest
of
Alaska.
One
unique
feature
of
the
Alaska
Highway
system
is
the
Anton
Anderson
Memorial
Tunnel,
an
active
Alaska
Railroad
tunnel
recently
upgraded
to
provide
a
paved
roadway
link
with
the
isolated
community
of
Whittier
on
Prince
William
Sound
to
the
Seward
Highway
about
southeast
of
Anchorage.
At
the
tunnel
was
the
longest
road
tunnel
in
North
America
until
2007.
The
tunnel
is
the
longest
combination
road
and
rail
tunnel
in
North
America.
Rail.
Built
around
1915,
the
Alaska
Railroad
(ARR)
played
a
key
role
in
the
development
of
Alaska
through
the
20th
century.
It
links
north
Pacific
shipping
through
providing
critical
infrastructure
with
tracks
that
run
from
Seward
to
Interior
Alaska
by
way
of
South
Central
Alaska,
passing
through
Anchorage,
Eklutna,
Wasilla,
Talkeetna,
Denali,
and
Fairbanks,
with
spurs
to
Whittier,
Palmer
and
North
Pole.
The
cities,
towns,
villages,
and
region
served
by
ARR
tracks
are
known
statewide
as
"The
Railbelt".
In
recent
years,
the
ever-improving
paved
highway
system
began
to
eclipse
the
railroad's
importance
in
Alaska's
economy.
The
railroad,
though
famed
for
its
summertime
tour
passenger
service,
played
a
vital
role
in
Alaska's
development,
moving
freight
into
Alaska
while
transporting
natural
resources
southward
(i.e.,
coal
from
the
Usibelli
coal
mine
near
Healy
to
Seward
and
gravel
from
the
Matanuska
Valley
to
Anchorage).
The
Alaska
Railroad
was
one
of
the
last
railroads
in
North
America
to
use
cabooses
in
regular
service
and
still
uses
them
on
some
gravel
trains.
It
continues
to
offer
one
of
the
last
flag
stop
routes
in
the
country.
A
stretch
of
about
of
track
along
an
area
north
of
Talkeetna
remains
inaccessible
by
road;
the
railroad
provides
the
only
transportation
to
rural
homes
and
cabins
in
the
area;
until
construction
of
the
Parks
Highway
in
the
1970s,
the
railroad
provided
the
only
land
access
to
most
of
the
region
along
its
entire
route.
In
northern
Southeast
Alaska,
the
White
Pass
and
Yukon
Route
also
partly
runs
through
the
State
from
Skagway
northwards
into
Canada
(British
Columbia
and
Yukon
Territory),
crossing
the
border
at
White
Pass
Summit.
This
line
is
now
mainly
used
by
tourists,
often
arriving
by
cruise
liner
at
Skagway.
It
featured
in
the
1983
BBC
television
series
Great
Little
Railways.
Marine
transport.
Most
cities,
towns
and
villages
in
the
state
do
not
have
road
or
highway
access;
the
only
modes
of
access
involve
travel
by
air,
river,
or
the
sea.
Alaska's
well-developed
state-owned
ferry
system
(known
as
the
Alaska
Marine
Highway)
serves
the
cities
of
Alaska
Panhandle,
the
Gulf
Coast
and
the
Alaska
Peninsula.
The
system
also
operates
a
ferry
service
from
Bellingham,
Washington
and
Prince
Rupert,
British
Columbia
in
Canada
through
the
Inside
Passage
to
Skagway.
The
Inter-Island
Ferry
Authority
also
serves
as
an
important
marine
link
for
many
communities
in
the
Prince
of
Wales
Island
region
of
Southeast
and
works
in
concert
with
the
Alaska
Marine
Highway.
In
recent
years,
large
cruise
ships
began
creating
a
summertime
tourism
market,
mainly
connecting
the
Pacific
Northwest
to
Southeast
Alaska
and,
to
a
lesser
degree,
towns
along
the
north
gulf
coast.
Several
times
each
summer,
the
population
of
Ketchikan
sharply
rises
for
a
few
hours
when
two
ships
dock
to
debark
more
than
a
thousand
passengers
each
while
four
other
ships
lie
at
anchor
nearby,
waiting
their
turn
at
the
dock.
Air
transport.
Cities
not
served
by
road,
sea,
or
river
can
be
reached
only
by
air,
foot,
dogsled,
or
snowmachine
accounting
for
Alaska's
extremely
well-developed
bush
air
services—an
Alaskan
novelty.
Anchorage
itself,
and
to
a
lesser
extent
Fairbanks,
are
served
by
many
major
airlines.
Because
of
limited
highway
access,
air
travel
remains
the
most
efficient
form
of
transportation
in
and
out
of
the
state.
Anchorage
recently
completed
extensive
remodeling
and
construction
at
Ted
Stevens
Anchorage
International
Airport
to
help
accommodate
the
upsurge
in
tourism
(in
2000–2001,
the
latest
year
for
which
data
is
available,
2.4 million
total
arrivals
to
Alaska
were
counted,
1.7 million
by
air
travel;
1.4 million
were
visitors).
Regular
flights
to
most
villages
and
towns
within
the
state
that
are
commercially
viable
are
challenging
to
provide,
so
they
are
heavily
subsidized
by
the
federal
government
through
the
Essential
Air
Service
program.
Alaska
Airlines
is
the
only
major
airline
offering
in-state
travel
with
jet
service
(sometimes
in
combination
cargo
and
passenger
Boeing
737-400s)
from
Anchorage
and
Fairbanks
to
regional
hubs
like
Bethel,
Nome,
Kotzebue,
Dillingham,
Kodiak,
and
other
larger
communities
as
well
as
to
major
Southeast
and
Alaska
Peninsula
communities.
The
bulk
of
remaining
commercial
flight
offerings
come
from
small
regional
commuter
airlines
such
as
Era
Aviation,
PenAir,
and
Frontier
Flying
Service.
The
smallest
towns
and
villages
must
rely
on
scheduled
or
chartered
bush
flying
services
using
general
aviation
aircraft
such
as
the
Cessna
Caravan,
the
most
popular
aircraft
in
use
in
the
state.
Much
of
this
service
can
be
attributed
to
the
Alaska
bypass
mail
program
which
subsidizes
bulk
mail
delivery
to
Alaskan
rural
communities.
The
program
requires
70%
of
that
subsidy
to
go
to
carriers
who
offer
passenger
service
to
the
communities.
Many
communities
have
small
air
taxi
services,
such
as
Hudson's
Air
Service,
Kantishna
Air
Taxi,
and
Talkeetna
Air
Taxi.
These
operations,
though
now
catering
primarily
to
tourists,
originated
from
the
demand
for
customized
transport
to
remote
areas.
Perhaps
the
most
quintessentially
Alaskan
plane
is
the
bush
seaplane.
The
world's
busiest
seaplane
base
is
Lake
Hood,
located
next
to
Ted
Stevens
Anchorage
International
Airport,
where
flights
bound
for
remote
villages
without
an
airstrip
carry
passengers,
cargo,
and
many
items
from
stores
and
warehouse
clubs.
Alaska
has
the
highest
number
of
pilots
per
capita
of
any
U.S.
state:
out
of
the
estimated
663,661
residents,
8,550
are
pilots,
or
about
one
in
78.
Other
transport.
Another
Alaskan
transportation
method
is
the
dogsled.
In
modern
times
(that
is,
any
time
after
the
mid-late
1920s),
dog
mushing
is
more
of
a
sport
than
a
true
means
of
transportation.
Various
races
are
held
around
the
state,
but
the
best
known
is
the
Iditarod
Trail
Sled
Dog
Race,
a
1150-mile
(1850 km)
trail
from
Anchorage
to
Nome
(although
the
mileage
varies
from
year
to
year,
the
official
distance
is
set
at
1049
miles).
The
race
commemorates
the
famous
1925
serum
run
to
Nome
in
which
mushers
and
dogs
like
Togo
and
Balto
took
much-needed
medicine
to
the
diphtheria-stricken
community
of
Nome
when
all
other
means
of
transportation
had
failed.
Mushers
from
all
over
the
world
come
to
Anchorage
each
March
to
compete
for
cash,
prizes,
and
prestige.
The
"Serum
Run"
is
another
sled
dog
race
that
more
accurately
follows
the
route
of
the
famous
1925
relay,
leaving
from
the
community
of
Nenana
(southwest
of
Fairbanks)
to
Nome.
In
areas
not
served
by
road
or
rail,
primary
transportation
in
summer
is
by
all-terrain
vehicle
and
in
winter
by
snowmobile
or
"snow
machine,"
as
it
is
commonly
referred
to
in
Alaska.
State
government.
Like
all
other
U.S.
states,
Alaska
is
governed
as
a
republic,
with
three
branches
of
government:
an
executive
branch
consisting
of
the
Governor
of
Alaska
and
the
other
independently
elected
constitutional
officers;
a
legislative
branch
consisting
of
the
Alaska
House
of
Representatives
and
Alaska
Senate;
and
a
judicial
branch
consisting
of
the
Alaska
Supreme
Court
and
lower
courts.
The
State
of
Alaska
employs
approximately
15,000
employees
statewide.
The
Alaska
Legislature
consists
of
a
40-member
House
of
Representatives
and
a
20-member
Senate.
Senators
serve
four
year
terms
and
House
members
two.
The
Governor
of
Alaska
serves
four-year
terms.
The
lieutenant
governor
runs
separately
from
the
governor
in
the
primaries,
but
during
the
general
election,
the
nominee
for
governor
and
nominee
for
lieutenant
governor
run
together
on
the
same
ticket.
Alaska's
court
system
has
four
levels:
the
Alaska
Supreme
Court,
the
court
of
appeals,
the
superior
courts
and
the
district
courts.
The
superior
and
district
courts
are
trial
courts.
Superior
courts
are
courts
of
general
jurisdiction,
while
district
courts
only
hear
certain
types
of
cases,
including
misdemeanor
criminal
cases
and
civil
cases
valued
up
to
$100,000.
The
Supreme
Court
and
the
Court
Of
Appeals
are
appellate
courts.
The
Court
Of
Appeals
is
required
to
hear
appeals
from
certain
lower-court
decisions,
including
those
regarding
criminal
prosecutions,
juvenile
delinquency,
and
habeas
corpus.
The
Supreme
Court
hears
civil
appeals
and
may
in
its
discretion
hear
criminal
appeals.
State
politics.
Although
Alaska
entered
the
union
as
a
Democratic
state,
since
the
early
1970s
Alaska
has
been
characterized
as
a
Republican-leaning
state.
Local
political
communities
have
often
worked
on
issues
related
to
land
use
development,
fishing,
tourism,
and
individual
rights.
Alaska
Natives,
while
organized
in
and
around
their
communities,
have
been
active
within
the
Native
corporations.
These
have
been
given
ownership
over
large
tracts
of
land,
which
require
stewardship.
Alaska
is
the
only
state
in
which
possession
of
one
ounce
or
less
of
marijuana
in
one's
home
is
completely
legal
under
state
law,
though
the
federal
law
remains
in
force.
The
state
has
an
independence
movement
favoring
a
vote
on
secession
from
the
United
States,
with
the
Alaska
Independence
Party
labeled
as
one
of
"the
most
significant
state-level
third
parties
operating
in
the
20th
century".
Six
Republicans
and
four
Democrats
have
served
as
governor
of
Alaska.
In
addition,
Republican
Governor
Wally
Hickel
was
elected
to
the
office
for
a
second
term
in
1990
after
leaving
the
Republican
party
and
briefly
joining
the
Alaskan
Independence
Party
ticket
just
long
enough
to
be
reelected.
He
subsequently
officially
rejoined
the
Republican
party
in
1994.
Taxes.
To
finance
state
government
operations,
Alaska
depends
primarily
on
petroleum
revenues
and
federal
subsidies.
This
allows
it
to
have
the
lowest
individual
tax
burden
in
the
United
States,
and
be
one
of
only
five
states
with
no
state
sales
tax,
one
of
seven
states
that
do
not
levy
an
individual
income
tax,
and
one
of
two
states
that
has
neither.
The
Department
of
Revenue
Tax
Division
reports
regularly
on
the
state's
revenue
sources.
The
Department
also
issues
an
annual
summary
of
its
operations,
including
new
state
laws
that
directly
affect
the
tax
division.
While
Alaska
has
no
state
sales
tax,
89
municipalities
collect
a
local
sales
tax,
from
1–7.5%,
typically
3–5%.
Other
local
taxes
levied
include
raw
fish
taxes,
hotel,
motel,
and
bed-and-breakfast
'bed'
taxes,
severance
taxes,
liquor
and
tobacco
taxes,
gaming
(pull
tabs)
taxes,
tire
taxes
and
fuel
transfer
taxes.
A
part
of
the
revenue
collected
from
certain
state
taxes
and
license
fees
(such
as
petroleum,
aviation
motor
fuel,
telephone
cooperative)
is
shared
with
municipalities
in
Alaska.
Fairbanks
has
one
of
the
highest
property
taxes
in
the
state
as
no
sales
or
income
taxes
are
assessed
in
the
Fairbanks
North
Star
Borough
(FNSB).
A
sales
tax
for
the
FNSB
has
been
voted
on
many
times,
but
has
yet
to
be
approved,
leading
law
makers
to
increase
taxes
dramatically
on
other
goods
such
as
liquor
and
tobacco.
In
2008
the
Tax
Foundation
ranked
Alaska
as
having
the
4th
most
"business
friendly"
tax
policy.
More
"friendly"
states
were
Wyoming,
Nevada,
and
South
Dakota.
Federal
politics.
In
presidential
elections,
the
state's
electoral
college
votes
have
been
won
by
the
Republican
nominee
in
every
election
since
statehood,
except
for
1964.
No
state
has
voted
for
a
Democratic
presidential
candidate
fewer
times.
Alaska
supported
Democratic
nominee
Lyndon
B.
Johnson
in
the
landslide
year
of
1964,
although
the
1960
and
1968
elections
were
close.
Republican
John
McCain
defeated
Democrat
Barack
Obama
in
Alaska,
59.49%
to
37.83%.
McCain's
running
mate
was
Sarah
Palin,
the
state's
governor
and
the
first
Alaskan
on
a
major
party
ticket.
The
Alaska
Bush,
the
city
of
Juneau
and
midtown
and
downtown
Anchorage
have
been
strongholds
of
the
Democratic
party.
Matanuska-Susitna
Borough
and
South
Anchorage
typically
have
the
strongest
Republican
showing.
As
of
2004,
well
over
half
of
all
registered
voters
have
chosen
"Non-Partisan"
or
"Undeclared"
as
their
affiliation,
despite
recent
attempts
to
close
primaries.
Because
of
its
population
relative
to
other
U.S.
states,
Alaska
has
only
one
member
in
the
U.S.
House
of
Representatives.
This
seat
is
currently
being
held
by
Republican
Don
Young,
who
was
re-elected
to
his
19th
consecutive
term
in
2008.
On
November
19,
2008,
Democrat
Mark
Begich,
mayor
of
Anchorage,
defeated
long-time
Republican
senator
Ted
Stevens.
Stevens
had
been
convicted
on
seven
felony
counts
of
failing
to
report
gifts
on
Senate
financial
discloser
forms
one
week
before
the
election.
The
conviction
was
set
aside
in
April
2009
after
evidence
of
prosecutorial
misconduct
emerged.
Republican
Frank
Murkowski
held
the
state's
other
senatorial
position.
After
being
elected
governor
in
2002,
he
resigned
from
the
Senate
and
appointed
his
daughter,
State
Representative
Lisa
Murkowski
as
his
successor.
In
response
to
a
subsequent
ballot
initiative,
the
state
legislature
attempted
to
amend
the
law
to
limit
the
length
of
gubernatorial
appointments.
She
won
a
full
six-year
term
in
2004.
In
2006
Frank
Murkowski
was
defeated
in
the
Republican
primary
by
Sarah
Palin,
who
in
2008
became
the
Republican
nominee
for
Vice
President
of
the
United
States.
Cities,
towns
and
boroughs.
Alaska
is
not
divided
into
counties,
as
most
of
the
other
U.S.
states,
but
it
is
divided
into
"boroughs".
Many
of
the
more
densely
populated
parts
of
the
state
are
part
of
Alaska's
sixteen
boroughs,
which
function
somewhat
similarly
to
counties
in
other
states.
However,
unlike
county-equivalents
in
the
other
49
states,
the
boroughs
do
not
cover
the
entire
land
area
of
the
state.
The
area
not
part
of
any
borough
is
referred
to
as
the
Unorganized
Borough.
The
Unorganized
Borough
has
no
government
of
its
own,
but
the
U.S.
Census
Bureau
in
cooperation
with
the
state
divided
the
Unorganized
Borough
into
11
census
areas
solely
for
the
purposes
of
statistical
analysis
and
presentation.
A
recording
district
is
a
mechanism
for
administration
of
the
public
record
in
Alaska.
The
state
is
divided
into
34
recording
districts
which
are
centrally
administered
under
a
State
Recorder.
All
recording
districts
use
the
same
acceptance
criteria,
fee
schedule,
etc.,
for
accepting
documents
into
the
public
record.
Whereas
many
U.S.
states
use
a
three-tiered
system
of
decentralization—state/county/township—most
of
Alaska
uses
only
two
tiers—state/borough.
Owing
to
the
low
population
density,
most
of
the
land
is
located
in
the
Unorganized
Borough
which,
as
the
name
implies,
has
no
intermediate
borough
government
of
its
own,
but
is
administered
directly
by
the
state
government.
Currently
(2000
census)
57.71%
of
Alaska's
area
has
this
status,
with
13.05%
of
the
population.
For
statistical
purposes
the
United
States
Census
Bureau
divides
this
territory
into
census
areas.
Anchorage
merged
the
city
government
with
the
Greater
Anchorage
Area
Borough
in
1975
to
form
the
Municipality
of
Anchorage,
containing
the
city
proper
and
the
communities
of
Eagle
River,
Chugiak,
Peters
Creek,
Girdwood,
Bird,
and
Indian.
Fairbanks
has
a
separate
borough
(the
Fairbanks
North
Star
Borough)
and
municipality
(the
City
of
Fairbanks).
The
state's
most
populous
city
is
Anchorage,
home
to
278,700
people
in
2006,
225,744
of
whom
live
in
the
urbanized
area.
The
richest
location
in
Alaska
by
per
capita
income
is
Halibut
Cove
($89,895).
Yakutat
City,
Sitka,
Juneau,
and
Anchorage
are
the
four
largest
cities
in
the
U.S.
by
area.
Education.
The
Alaska
Department
of
Education
and
Early
Development
administers
many
school
districts
in
Alaska.
In
addition,
the
state
operates
a
boarding
school,
Mt.
Edgecumbe
High
School
in
Sitka;
and
provides
partial
funding
for
other
boarding
schools
including,
Nenana
Student
Living
Center
in
Nenana,
and
The
Galena
Interior
Learning
Academy
in
Galena.
There
are
more
than
a
dozen
colleges
and
universities
in
Alaska.
Accredited
universities
in
Alaska
include
the
University
of
Alaska
Anchorage,
University
of
Alaska
Fairbanks,
University
of
Alaska
Southeast,
and
Alaska
Pacific
University.
43%
of
the
population
attends
or
attended
college.
Alaska
has
had
a
problem
with
a
"brain
drain".
Many
of
its
young
people,
including
most
of
the
highest
academic
achievers,
leave
the
state
after
high
school
graduation
and
do
not
return.
The
University
of
Alaska
has
attempted
to
combat
this
by
offering
partial
four-year
scholarships
to
the
top
10%
of
Alaska
high
school
graduates,
via
the
Alaska
Scholars
Program.
Public
health
and
public
safety.
Alaska
residents
have
long
had
a
problem
with
alcohol
use
and
abuse.
Many
rural
communities
in
Alaska
have
outlawed
its
import.
This
problem
directly
relates
to
Alaska's
high
rate
of
Fetal
alcohol
syndrome
(FAS)
as
well
as
contributing
to
the
high
rate
of
suicides
and
teenage
pregnancies.
Suicide
rates
for
rural
residents
are
higher
than
urban.
Domestic
abuse
and
other
violent
crimes
are
also
at
high
levels
in
the
state;
this
is
in
part
linked
to
alcohol
abuse.
Culture.
Some
of
Alaska's
popular
annual
events
are
the
Iditarod
Trail
Sled
Dog
Race
that
starts
in
Anchorage
and
ends
in
Nome,
World
Ice
Art
Championships
in
Fairbanks,
the
Alaska
Hummingbird
Festival
in
Ketchikan,
the
Sitka
Whale
Fest,
and
the
Stikine
River
Garnet
Fest
in
Wrangell.
The
Stikine
River
features
the
largest
springtime
concentration
of
American
Bald
Eagles
in
the
world.
The
Alaska
Native
Heritage
Center
celebrates
the
rich
heritage
of
Alaska's
11
cultural
groups.
Their
purpose
is
to
enhance
self-esteem
among
Native
people
and
to
encourage
cross-cultural
exchanges
among
all
people.
The
Alaska
Native
Arts
Foundation
promotes
and
markets
Native
art
from
all
regions
and
cultures
in
the
State,
both
on
the
internet;
at
its
gallery
in
Anchorage,
500
West
Sixth
Avenue,
and
at
the
Alaska
House
New
York,
109
Mercer
Street
in
SoHo.
Alaska
Natives
–
Inuit,
Inupiaq
or
Yupik
drummers
and
dancers
–
give
informal
performances
in
the
lobby
of
the
Alaska
Native
Medical
Center
in
Anchorage
on
weekday
evenings.
Libraries.
The
four
main
libraries
in
the
state
are
the
Alaska
State
Library
in
Juneau,
the
Elmer
E.
Rasmuson
Library
in
Fairbanks,
the
Z.
J.
Loussac
Library
in
Anchorage,
and
the
APU
Consortium
Library,
also
in
Anchorage.
Alaska
is
one
of
three
states
(the
others
are
Delaware
and
Rhode
Island)
that
does
not
have
a
Carnegie
library.
Music.
Influences
on
music
in
Alaska
include
the
traditional
music
of
Alaska
Natives
as
well
as
folk
music
brought
by
later
immigrants
from
Russia
and
Europe.
Prominent
musicians
from
Alaska
include
singer
Jewel,
traditional
Aleut
flautist
Mary
Youngblood,
folk
singer-songwriter
Libby
Roderick,
Christian
music
singer/songwriter
Lincoln
Brewster,
metal/post
hardcore
band
36
Crazyfists
and
the
groups
Pamyua
and
Portugal.
The
Man.
There
are
many
established
music
festivals
in
Alaska,
including
the
Alaska
Folk
Festival,
the
Fairbanks
Summer
Arts
Festival
the
Anchorage
Folk
Festival,
the
Athabascan
Old-Time
Fiddling
Festival,
the
Sitka
Jazz
Festival,
and
the
Sitka
Summer
Music
Festival.
The
most
prominent
symphony
in
Alaska
is
the
Anchorage
Symphony
Orchestra,
though
the
Fairbanks
Symphony
Orchestra
and
Juneau
Symphony
are
also
notable.
The
Anchorage
Opera
is
currently
the
state's
only
professional
opera
company,
though
there
are
several
volunteer
and
semi-professional
organizations
in
the
state
as
well.
The
official
state
song
of
Alaska
is
"Alaska's
Flag",
which
was
adopted
in
1955;
it
celebrates
the
flag
of
Alaska.
Movies
filmed
in
Alaska.
Alaska's
first
independent
picture
all
made
on
place
was
in
the
silent
years.
The
Chechahcos,
was
released
in
1924
by
the
Alaska
Moving
Picture
Corp.
It
was
the
only
film
the
company
made.
One
of
the
most
prominent
movies
filmed
in
Alaska
is
MGM's
Academy
Award
winning
classic
"Mala
The
Magnificent"
starring
Alaska's
own
Ray
Mala.
In
1932
an
expedition
set
out
from
MGM's
studios
in
Hollywood
to
Alaska
to
film
what
was
then
billed
as
"The
Biggest
Picture
Ever
Made."
Upon
arriving
in
Alaska,
they
set
up
"Camp
Hollywood"
in
Northwest
Alaska,
where
they
lived
during
the
duration
of
the
filming.
Louis
B.
Mayer
spared
no
expense
in
making
sure
they
had
everything
they
needed
during
their
stay—he
even
sent
the
famous
chef
from
the
Hotel
Roosevelt
on
Hollywood
Blvd
(the
site
of
the
first
Oscars)
with
them
to
Alaska
to
cook
for
them.
When
"Eskimo"
premiered
at
the
famed
Astor
Theatre
in
Times
Square,
New
York,
the
studio
received
the
largest
amount
of
feedback
in
the
history
of
the
studio
up
to
that
time.
"Eskimo"
was
critically
acclaimed
and
released
worldwide;
as
a
result
Inupiat
Eskimo
actor
Ray
Mala
became
an
international
movie
star.
"Eskimo"
is
significant
for
the
following:
winning
the
very
first
Oscar
for
Best
Film
Editing
at
the
Academy
Awards,
for
forever
preserving
Inupiat
culture
on
film,
and
for
being
the
first
motion
picture
to
be
filmed
in
an
all
native
language
(Inupiat).
The
psychological
thriller
"Insomnia",
starring
Al
Pacino
and
Robin
Williams
was
shot
in
Canada,
but
was
set
in
Alaska.
The
2007
horror
feature
"30
Days
of
Night"
is
set
in
Barrow,
Alaska
but
was
filmed
in
New
Zealand.
Most
films
and
television
shows
set
in
Alaska
are
not
filmed
there;
for
example,
"Northern
Exposure",
set
in
the
fictional
town
of
Cicely,
Alaska,
was
actually
filmed
in
Roslyn,
Washington.
The
1983
Disney
movie
"Never
Cry
Wolf"
was
at
least
partially
shot
in
Alaska.
The
1991
film
"White
Fang",
starring
Ethan
Hawke,
was
filmed
in
and
around
Haines,
Alaska.
The
1999
John
Sayles
film
"Limbo",
starring
David
Strathairn,
Mary
Elizabeth
Mastrantonio
and
Kris
Kristofferson,
was
filmed
in
Juneau.
The
2007
film
directed
by
Sean
Penn,
"Into
The
Wild"
was
partially
filmed
and
set
in
Alaska.
The
film,
which
is
based
on
the
novel
of
the
same
name,
follows
the
adventures
of
Christopher
McCandless,
who
died
in
a
remote
abandoned
bus
in
Alaska
in
1992.
---END.OF.DOCUMENT---
Agriculture.
Agriculture
is
the
production
of
food
and
goods
through
farming.
Agriculture
was
the
key
development
that
led
to
the
rise
of
human
civilization,
with
the
husbandry
of
domesticated
animals
and
plants
(i.e.
crops)
creating
food
surpluses
that
enabled
the
development
of
more
densely
populated
and
stratified
societies.
The
study
of
agriculture
is
known
as
agricultural
science.
Central
to
human
society,
agriculture
is
also
observed
in
certain
species
of
ant
and
termite.
Agriculture
encompasses
a
wide
variety
of
specialties
and
techniques,
including
ways
to
expand
the
lands
suitable
for
plant
raising,
by
digging
water-channels
and
other
forms
of
irrigation.
Cultivation
of
crops
on
arable
land
and
the
pastoral
herding
of
livestock
on
rangeland
remain
at
the
foundation
of
agriculture.
In
the
past
century
there
has
been
increasing
concern
to
identify
and
quantify
various
forms
of
agriculture.
In
the
developed
world
the
range
usually
extends
between
sustainable
agriculture
(e.g.
permaculture
or
organic
agriculture)
and
intensive
farming
(e.g.
industrial
agriculture).
Modern
agronomy,
plant
breeding,
pesticides
and
fertilizers,
and
technological
improvements
have
sharply
increased
yields
from
cultivation,
and
at
the
same
time
have
caused
widespread
ecological
damage
and
negative
human
health
effects.
Selective
breeding
and
modern
practices
in
animal
husbandry
such
as
intensive
pig
farming
(and
similar
practices
applied
to
the
chicken)
have
similarly
increased
the
output
of
meat,
but
have
raised
concerns
about
animal
cruelty
and
the
health
effects
of
the
antibiotics,
growth
hormones,
and
other
chemicals
commonly
used
in
industrial
meat
production.
The
major
agricultural
products
can
be
broadly
grouped
into
foods,
fibers,
fuels,
and
raw
materials.
In
the
2000s,
plants
have
been
used
to
grow
biofuels,
biopharmaceuticals,
bioplastics,
and
pharmaceuticals.
Specific
foods
include
cereals,
vegetables,
fruits,
and
meat.
Fibers
include
cotton,
wool,
hemp,
silk
and
flax.
Raw
materials
include
lumber
and
bamboo.
Other
useful
materials
are
produced
by
plants,
such
as
resins.
Biofuels
include
methane
from
biomass,
ethanol,
and
biodiesel.
Cut
flowers,
nursery
plants,
tropical
fish
and
birds
for
the
pet
trade
are
some
of
the
ornamental
products.
In
2007,
about
one
third
of
the
world's
workers
were
employed
in
agriculture.
The
services
sector
has
overtaken
agriculture
as
the
economic
sector
employing
the
most
people
worldwide.
Despite
the
size
of
its
workforce,
agricultural
production
accounts
for
less
than
five
percent
of
the
gross
world
product
(an
aggregate
of
all
gross
domestic
products).
Etymology.
The
word
"agriculture"
is
the
English
adaptation
of
Latin
"agricultūra",
from
"ager",
"a
field",
and
"cultūra",
"cultivation"
in
the
strict
sense
of
"tillage
of
the
soil".
Thus,
a
literal
reading
of
the
word
yields
"tillage
of
a
field
/
of
fields"...
Overview.
Agriculture
has
played
a
key
role
in
the
development
of
human
civilization.
Until
the
Industrial
Revolution,
the
vast
majority
of
the
human
population
labored
in
agriculture.
Development
of
agricultural
techniques
has
steadily
increased
agricultural
productivity,
and
the
widespread
diffusion
of
these
techniques
during
a
time
period
is
often
called
an
agricultural
revolution.
A
remarkable
shift
in
agricultural
practices
has
occurred
over
the
past
century
in
response
to
new
technologies.
In
particular,
the
Haber-Bosch
method
for
synthesizing
ammonium
nitrate
made
the
traditional
practice
of
recycling
nutrients
with
crop
rotation
and
animal
manure
less
necessary.
Synthetic
nitrogen,
along
with
mined
rock
phosphate,
pesticides
and
mechanization,
have
greatly
increased
crop
yields
in
the
early
20th
century.
Increased
supply
of
grains
has
led
to
cheaper
livestock
as
well.
Further,
global
yield
increases
were
experienced
later
in
the
20th
century
when
high-yield
varieties
of
common
staple
grains
such
as
rice,
wheat,
and
corn
(maize)
were
introduced
as
a
part
of
the
Green
Revolution.
The
Green
Revolution
exported
the
technologies
(including
pesticides
and
synthetic
nitrogen)
of
the
developed
world
to
the
developing
world.
Thomas
Malthus
famously
predicted
that
the
Earth
would
not
be
able
to
support
its
growing
population,
but
technologies
such
as
the
Green
Revolution
have
allowed
the
world
to
produce
a
surplus
of
food.
Many
governments
have
subsidized
agriculture
to
ensure
an
adequate
food
supply.
These
agricultural
subsidies
are
often
linked
to
the
production
of
certain
commodities
such
as
wheat,
corn
(maize),
rice,
soybeans,
and
milk.
These
subsidies,
especially
when
instituted
by
developed
countries
have
been
noted
as
protectionist,
inefficient,
and
environmentally
damaging.
In
the
past
century
agriculture
has
been
characterized
by
enhanced
productivity,
the
use
of
synthetic
fertilizers
and
pesticides,
selective
breeding,
mechanization,
water
contamination,
and
farm
subsidies.
Proponents
of
organic
farming
such
as
Sir
Albert
Howard
argued
in
the
early
1900s
that
the
overuse
of
pesticides
and
synthetic
fertilizers
damages
the
long-term
fertility
of
the
soil.
While
this
feeling
lay
dormant
for
decades,
as
environmental
awareness
has
increased
in
the
2000s
there
has
been
a
movement
towards
sustainable
agriculture
by
some
farmers,
consumers,
and
policymakers.
In
recent
years
there
has
been
a
backlash
against
perceived
external
environmental
effects
of
mainstream
agriculture,
particularly
regarding
water
pollution,
resulting
in
the
organic
movement.
One
of
the
major
forces
behind
this
movement
has
been
the
European
Union,
which
first
certified
organic
food
in
1991
and
began
reform
of
its
Common
Agricultural
Policy
(CAP)
in
2005
to
phase
out
commodity-linked
farm
subsidies,
also
known
as
decoupling.
The
growth
of
organic
farming
has
renewed
research
in
alternative
technologies
such
as
integrated
pest
management
and
selective
breeding.
Recent
mainstream
technological
developments
include
genetically
modified
food.
In
late
2007,
several
factors
pushed
up
the
price
of
grains
consumed
by
humans
as
well
as
used
to
feed
poultry
and
dairy
cows
and
other
cattle,
causing
higher
prices
of
wheat
(up
58%),
soybean
(up
32%),
and
maize
(up
11%)
over
the
year.
Food
riots
took
place
in
several
countries
across
the
world.
Contributing
factors
included
drought
in
Australia
and
elsewhere,
increasing
demand
for
grain-fed
animal
products
from
the
growing
middle
classes
of
countries
such
as
China
and
India,
diversion
of
foodgrain
to
biofuel
production
and
trade
restrictions
imposed
by
several
countries.
An
epidemic
of
stem
rust
on
wheat
caused
by
race
Ug99
is
currently
spreading
across
Africa
and
into
Asia
and
is
causing
major
concern.
Approximately
40%
of
the
world's
agricultural
land
is
seriously
degraded.
In
Africa,
if
current
trends
of
soil
degradation
continue,
the
continent
might
be
able
to
feed
just
25%
of
its
population
by
2025,
according
to
UNU's
Ghana-based
Institute
for
Natural
Resources
in
Africa.
History.
Since
its
development
roughly
10,000
years
ago,
agriculture
has
expanded
vastly
in
geographical
coverage
and
yields.
Throughout
this
expansion,
new
technologies
and
new
crops
were
integrated.
Even
then
crops
were
modified
through
cross-breeding
for
better
yields.
Agricultural
practices
such
as
irrigation,
crop
rotation,
fertilizers,
and
pesticides
were
developed
long
ago,
but
have
made
great
strides
in
the
past
century.
The
history
of
agriculture
has
played
a
major
role
in
human
history,
as
agricultural
progress
has
been
a
crucial
factor
in
worldwide
socio-economic
change.
Wealth-concentration
and
militaristic
specializations
rarely
seen
in
hunter-gatherer
cultures
are
commonplace
in
societies
which
practice
agriculture.
So,
too,
are
arts
such
as
epic
literature
and
monumental
architecture,
as
well
as
codified
legal
systems.
When
farmers
became
capable
of
producing
food
beyond
the
needs
of
their
own
families,
others
in
their
society
were
freed
to
devote
themselves
to
projects
other
than
food
acquisition.
Historians
and
anthropologists
have
long
argued
that
the
development
of
agriculture
made
civilization
possible.
Ancient
origins.
The
Fertile
Crescent
of
Western
Asia,
Egypt,
and
India
were
sites
of
the
earliest
planned
sowing
and
harvesting
of
plants
that
had
previously
been
gathered
in
the
wild.
Independent
development
of
agriculture
occurred
in
northern
and
southern
China,
Africa's
Sahel,
New
Guinea
and
several
regions
of
the
Americas.
The
eight
so-called
Neolithic
founder
crops
of
agriculture
appear:
first
emmer
wheat
and
einkorn
wheat,
then
hulled
barley,
peas,
lentils,
bitter
vetch,
chick
peas
and
flax.
By
7000
BC,
small-scale
agriculture
reached
Egypt.
From
at
least
7000
BC
the
Indian
subcontinent
saw
farming
of
wheat
and
barley,
as
attested
by
archaeological
excavation
at
Mehrgarh
in
Balochistan.
By
6000
BC,
mid-scale
farming
was
entrenched
on
the
banks
of
the
Nile.
About
this
time,
agriculture
was
developed
independently
in
the
Far
East,
with
rice,
rather
than
wheat,
as
the
primary
crop.
Chinese
and
Indonesian
farmers
went
on
to
domesticate
taro
and
beans
including
mung,
soy
and
azuki.
To
complement
these
new
sources
of
carbohydrates,
highly
organized
net
fishing
of
rivers,
lakes
and
ocean
shores
in
these
areas
brought
in
great
volumes
of
essential
protein.
Collectively,
these
new
methods
of
farming
and
fishing
inaugurated
a
human
population
boom
that
dwarfed
all
previous
expansions
and
continues
today.
By
5000
BC,
the
Sumerians
had
developed
core
agricultural
techniques
including
large-scale
intensive
cultivation
of
land,
mono-cropping,
organized
irrigation,
and
the
use
of
a
specialized
labor
force,
particularly
along
the
waterway
now
known
as
the
Shatt
al-Arab,
from
its
Persian
Gulf
delta
to
the
confluence
of
the
Tigris
and
Euphrates.
Domestication
of
wild
aurochs
and
mouflon
into
cattle
and
sheep,
respectively,
ushered
in
the
large-scale
use
of
animals
for
food/fiber
and
as
beasts
of
burden.
The
shepherd
joined
the
farmer
as
an
essential
provider
for
sedentary
and
semi-nomadic
societies.
Maize,
manioc,
and
arrowroot
were
first
domesticated
in
the
Americas
as
far
back
as
5200
BC.
The
potato,
tomato,
pepper,
squash,
several
varieties
of
bean,
tobacco,
and
several
other
plants
were
also
developed
in
the
New
World,
as
was
extensive
terracing
of
steep
hillsides
in
much
of
Andean
South
America.
The
Greeks
and
Romans
built
on
techniques
pioneered
by
the
Sumerians
but
made
few
fundamentally
new
advances.
Southern
Greeks
struggled
with
very
poor
soils,
yet
managed
to
become
a
dominant
society
for
years.
The
Romans
were
noted
for
an
emphasis
on
the
cultivation
of
crops
for
trade.
Middle
Ages.
During
the
Middle
Ages,
farmers
in
North
Africa,
the
Near
East,
and
Europe
began
making
use
of
agricultural
technologies
including
irrigation
systems
based
on
hydraulic
and
hydrostatic
principles,
machines
such
as
norias,
water-raising
machines,
dams,
and
reservoirs.
This
combined
with
the
invention
of
a
three-field
system
of
crop
rotation
and
the
moldboard
plow
greatly
improved
agricultural
efficiency.
Modern
era.
After
1492,
a
global
exchange
of
previously
local
crops
and
livestock
breeds
occurred.
Key
crops
involved
in
this
exchange
included
the
tomato,
maize,
potato,
manioc,
cocoa
bean
and
tobacco
going
from
the
New
World
to
the
Old,
and
several
varieties
of
wheat,
spices,
coffee,
and
sugar
cane
going
from
the
Old
World
to
the
New.
The
most
important
animal
exportation
from
the
Old
World
to
the
New
were
those
of
the
horse
and
dog
(dogs
were
already
present
in
the
pre-Columbian
Americas
but
not
in
the
numbers
and
breeds
suited
to
farm
work).
Although
not
usually
food
animals,
the
horse
(including
donkeys
and
ponies)
and
dog
quickly
filled
essential
production
roles
on
western-hemisphere
farms.
The
potato
became
an
important
staple
crop
in
northern
Europe.
Since
being
introduced
by
Portuguese
in
the
16th
century,
maize
and
manioc
have
replaced
traditional
African
crops
as
the
continent's
most
important
staple
food
crops.
By
the
early
1800s,
agricultural
techniques,
implements,
seed
stocks
and
cultivated
plants
selected
and
given
a
unique
name
because
of
its
decorative
or
useful
characteristics
had
so
improved
that
yield
per
land
unit
was
many
times
that
seen
in
the
Middle
Ages.
With
the
rapid
rise
of
mechanization
in
the
late
19th
and
20th
centuries,
particularly
in
the
form
of
the
tractor,
farming
tasks
could
be
done
with
a
speed
and
on
a
scale
previously
impossible.
These
advances
have
led
to
efficiencies
enabling
certain
modern
farms
in
the
United
States,
Argentina,
Israel,
Germany,
and
a
few
other
nations
to
output
volumes
of
high-quality
produce
per
land
unit
at
what
may
be
the
practical
limit.
The
Haber-Bosch
method
for
synthesizing
ammonium
nitrate
represented
a
major
breakthrough
and
allowed
crop
yields
to
overcome
previous
constraints.
In
the
past
century
agriculture
has
been
characterized
by
enhanced
productivity,
the
substitution
of
labor
for
synthetic
fertilizers
and
pesticides,
water
pollution,
and
farm
subsidies.
In
recent
years
there
has
been
a
backlash
against
the
external
environmental
effects
of
conventional
agriculture,
resulting
in
the
organic
movement.
The
cereals
rice,
corn,
and
wheat
provide
60%
of
human
food
supply.
Between
1700
and
1980,
"the
total
area
of
cultivated
land
worldwide
increased
466%"
and
yields
increased
dramatically,
particularly
because
of
selectively-bred
high-yielding
varieties,
fertilizers,
pesticides,
irrigation,
and
machinery.
For
example,
irrigation
increased
corn
yields
in
eastern
Colorado
by
400
to
500%
from
1940
to
1997.
However,
concerns
have
been
raised
over
the
sustainability
of
intensive
agriculture.
Intensive
agriculture
has
become
associated
with
decreased
soil
quality
in
India
and
Asia,
and
there
has
been
increased
concern
over
the
effects
of
fertilizers
and
pesticides
on
the
environment,
particularly
as
population
increases
and
food
demand
expands.
The
monocultures
typically
used
in
intensive
agriculture
increase
the
number
of
pests,
which
are
controlled
through
pesticides.
Integrated
pest
management
(IPM),
which
"has
been
promoted
for
decades
and
has
had
some
notable
successes"
has
not
significantly
affected
the
use
of
pesticides
because
policies
encourage
the
use
of
pesticides
and
IPM
is
knowledge-intensive.
Although
the
"Green
Revolution"
significantly
increased
rice
yields
in
Asia,
yield
increases
have
not
occurred
in
the
past
15–20
years.
The
genetic
"yield
potential"
has
increased
for
wheat,
but
the
yield
potential
for
rice
has
not
increased
since
1966,
and
the
yield
potential
for
maize
has
"barely
increased
in
35
years".
It
takes
a
decade
or
two
for
herbicide-resistant
weeds
to
emerge,
and
insects
become
resistant
to
insecticides
within
about
a
decade.
Crop
rotation
helps
to
prevent
resistances.
Agricultural
exploration
expeditions,
since
the
late
nineteenth
century,
have
been
mounted
to
find
new
species
and
new
agricultural
practices
in
different
areas
of
the
world.
Two
early
examples
of
expeditions
include
Frank
N.
Meyer's
fruit-
and
nut-collecting
trip
to
China
and
Japan
from
1916-1918
and
the
Dorsett-Morse
Oriental
Agricultural
Exploration
Expedition
to
China,
Japan,
and
Korea
from
1929-1931
to
collect
soybean
germplasm
to
support
the
rise
in
soybean
agriculture
in
the
United
States.
In
2005,
the
agricultural
output
of
China
was
the
largest
in
the
world,
accounting
for
almost
one-sixth
of
world
share,
followed
by
the
EU,
India
and
the
USA,
according
to
the
International
Monetary
Fund.
More
than
40
million
Chinese
farmers
have
been
displaced
from
their
land
in
recent
years,
usually
for
economic
development,
contributing
to
the
87,000
demonstrations
and
riots
across
China
in
2005.
Economists
measure
the
total
factor
productivity
of
agriculture
and
by
this
measure
agriculture
in
the
United
States
is
roughly
2.6
times
more
productive
than
it
was
in
1948.
Six
countries
-
the
US,
Canada,
France,
Australia,
Argentina
and
Thailand
-
supply
90%
of
grain
exports.
The
United
States
controls
almost
half
of
world
grain
exports.
Water
deficits,
which
are
already
spurring
heavy
grain
imports
in
numerous
middle-sized
countries,
including
Algeria,
Iran,
Egypt,
and
Mexico,
may
soon
do
the
same
in
larger
countries,
such
as
China
or
India.
Crop
production
systems.
Cropping
systems
vary
among
farms
depending
on
the
available
resources
and
constraints;
geography
and
climate
of
the
farm;
government
policy;
economic,
social
and
political
pressures;
and
the
philosophy
and
culture
of
the
farmer.
Shifting
cultivation
(or
slash
and
burn)
is
a
system
in
which
forests
are
burnt,
releasing
nutrients
to
support
cultivation
of
annual
and
then
perennial
crops
for
a
period
of
several
years.
Then
the
plot
is
left
fallow
to
regrow
forest,
and
the
farmer
moves
to
a
new
plot,
returning
after
many
more
years
(10-20).
This
fallow
period
is
shortened
if
population
density
grows,
requiring
the
input
of
nutrients
(fertilizer
or
manure)
and
some
manual
pest
control.
Annual
cultivation
is
the
next
phase
of
intensity
in
which
there
is
no
fallow
period.
This
requires
even
greater
nutrient
and
pest
control
inputs.
Further
industrialization
lead
to
the
use
of
monocultures,
when
one
cultivar
is
planted
on
a
large
acreage.
Because
of
the
low
biodiversity,
nutrient
use
is
uniform
and
pests
tend
to
build
up,
necessitating
the
greater
use
of
pesticides
and
fertilizers.
Multiple
cropping,
in
which
several
crops
are
grown
sequentially
in
one
year,
and
intercropping,
when
several
crops
are
grown
at
the
same
time
are
other
kinds
of
annual
cropping
systems
known
as
polycultures.
In
tropical
environments,
all
of
these
cropping
systems
are
practiced.
In
subtropical
and
arid
environments,
the
timing
and
extent
of
agriculture
may
be
limited
by
rainfall,
either
not
allowing
multiple
annual
crops
in
a
year,
or
requiring
irrigation.
In
all
of
these
environments
perennial
crops
are
grown
(coffee,
chocolate)
and
systems
are
practiced
such
as
agroforestry.
In
temperate
environments,
where
ecosystems
were
predominantly
grassland
or
prairie,
highly
productive
annual
cropping
is
the
dominant
farming
system.
The
last
century
has
seen
the
intensification,
concentration
and
specialization
of
agriculture,
relying
upon
new
technologies
of
agricultural
chemicals
(fertilizers
and
pesticides),
mechanization,
and
plant
breeding
(hybrids
and
GMO's).
In
the
past
few
decades,
a
move
towards
sustainability
in
agriculture
has
also
developed,
integrating
ideas
of
socio-economic
justice
and
conservation
of
resources
and
the
environment
within
a
farming
system.
This
has
led
to
the
development
of
many
responses
to
the
conventional
agriculture
approach,
including
organic
agriculture,
urban
agriculture,
community
supported
agriculture,
ecological
or
biological
agriculture,
integrated
farming
and
holistic
management,
as
well
as
an
increased
trend
towards
agricultural
diversification.
Crop
statistics.
Important
categories
of
crops
include
grains
and
pseudograins,
pulses
(legumes),
forage,
and
fruits
and
vegetables.
Specific
crops
are
cultivated
in
distinct
growing
regions
throughout
the
world.
In
millions
of
metric
tons,
based
on
FAO
estimate.
Livestock
production
systems.
Animals,
including
horses,
mules,
oxen,
camels,
llamas,
alpacas,
and
dogs,
are
often
used
to
help
cultivate
fields,
harvest
crops,
wrangle
other
animals,
and
transport
farm
products
to
buyers.
Animal
husbandry
not
only
refers
to
the
breeding
and
raising
of
animals
for
meat
or
to
harvest
animal
products
(like
milk,
eggs,
or
wool)
on
a
continual
basis,
but
also
to
the
breeding
and
care
of
species
for
work
and
companionship.
Livestock
production
systems
can
be
defined
based
on
feed
source,
as
grassland
-
based,
mixed,
and
landless.
Grassland
based
livestock
production
relies
upon
plant
material
such
as
shrubland,
rangeland,
and
pastures
for
feeding
ruminant
animals.
Outside
nutrient
inputs
may
be
used,
however
manure
is
returned
directly
to
the
grassland
as
a
major
nutrient
source.
This
system
is
particularly
important
in
areas
where
crop
production
is
not
feasible
because
of
climate
or
soil,
representing
30-40
million
pastoralists.
Mixed
production
systems
use
grassland,
fodder
crops
and
grain
feed
crops
as
feed
for
ruminant
and
monogastic
(one
stomach;
mainly
chickens
and
pigs)
livestock.
Manure
is
typically
recycled
in
mixed
systems
as
a
fertilizer
for
crops.
Approximately
68%
of
all
agricultural
land
is
permanent
pastures
used
in
the
production
of
livestock.
Landless
systems
rely
upon
feed
from
outside
the
farm,
representing
the
de-linking
of
crop
and
livestock
production
found
more
prevalently
in
OECD
member
countries.
In
the
U.S.,
70%
of
the
grain
grown
is
fed
to
animals
on
feedlots.
Synthetic
fertilizers
are
more
heavily
relied
upon
for
crop
production
and
manure
utilization
becomes
a
challenge
as
well
as
a
source
for
pollution.
Production
practices.
Tillage
is
the
practice
of
plowing
soil
to
prepare
for
planting
or
for
nutrient
incorporation
or
for
pest
control.
Tillage
varies
in
intensity
from
conventional
to
no-till.
It
may
improve
productivity
by
warming
the
soil,
incorporating
fertilizer
and
controlling
weeds,
but
also
renders
soil
more
prone
to
erosion,
triggers
the
decomposition
of
organic
matter
releasing
CO2,
and
reduces
the
abundance
and
diversity
of
soil
organisms.
Pest
control
includes
the
management
of
weeds,
mites,
and
diseases.
Chemical
(pesticides),
biological
(biocontrol),
mechanical
(tillage),
and
cultural
practices
are
used.
Cultural
practices
include
crop
rotation,
culling,
cover
crops,
intercropping,
composting,
avoidance,
and
resistance.
Integrated
pest
management
attempts
to
use
all
of
these
methods
to
keep
pest
populations
below
the
number
which
would
cause
economic
loss,
and
recommends
pesticides
as
a
last
resort.
Nutrient
management
includes
both
the
source
of
nutrient
inputs
for
crop
and
livestock
production,
and
the
method
of
utilization
of
manure
produced
by
livestock.
Nutrient
inputs
can
be
chemical
inorganic
fertilizers,
manure,
green
manure,
compost
and
mined
minerals.
Crop
nutrient
use
may
also
be
managed
using
cultural
techniques
such
as
crop
rotation
or
a
fallow
period.
Manure
is
used
either
by
holding
livestock
where
the
feed
crop
is
growing,
such
as
in
managed
intensive
rotational
grazing,
or
by
spreading
either
dry
or
liquid
formulations
of
manure
on
cropland
or
pastures.
Water
management
is
where
rainfall
is
insufficient
or
variable,
which
occurs
to
some
degree
in
most
regions
of
the
world.
Some
farmers
use
irrigation
to
supplement
rainfall.
In
other
areas
such
as
the
Great
Plains
in
the
U.S.
and
Canada,
farmers
use
a
fallow
year
to
conserve
soil
moisture
to
use
for
growing
a
crop
in
the
following
year.
Agriculture
represents
70%
of
freshwater
use
worldwide.
Processing,
distribution,
and
marketing.
In
the
United
States,
food
costs
attributed
to
processing,
distribution,
and
marketing
have
risen
while
the
costs
attributed
to
farming
have
declined.
This
is
related
to
the
greater
efficiency
of
farming,
combined
with
the
increased
level
of
value
addition
(e.g.
more
highly
processed
products)
provided
by
the
supply
chain.
From
1960
to
1980
the
farm
share
was
around
40%,
but
by
1990
it
had
declined
to
30%
and
by
1998,
22.2%.
Market
concentration
has
increased
in
the
sector
as
well,
with
the
top
20
food
manufacturers
accounting
for
half
the
food-processing
value
in
1995,
over
double
that
produced
in
1954.
As
of
2000
the
top
six
US
supermarket
groups
had
50%
of
sales
compared
to
32%
in
1992.
Although
the
total
effect
of
the
increased
market
concentration
is
likely
increased
efficiency,
the
changes
redistribute
economic
surplus
from
producers
(farmers)
and
consumers,
and
may
have
negative
implications
for
rural
communities.
Crop
alteration
and
biotechnology.
Crop
alteration
has
been
practiced
by
humankind
for
thousands
of
years,
since
the
beginning
of
civilization.
Altering
crops
through
breeding
practices
changes
the
genetic
make-up
of
a
plant
to
develop
crops
with
more
beneficial
characteristics
for
humans,
for
example,
larger
fruits
or
seeds,
drought-tolerance,
or
resistance
to
pests.
Significant
advances
in
plant
breeding
ensued
after
the
work
of
geneticist
Gregor
Mendel.
His
work
on
dominant
and
recessive
alleles
gave
plant
breeders
a
better
understanding
of
genetics
and
brought
great
insights
to
the
techniques
utilized
by
plant
breeders.
Crop
breeding
includes
techniques
such
as
plant
selection
with
desirable
traits,
self-pollination
and
cross-pollination,
and
molecular
techniques
that
genetically
modify
the
organism.
Domestication
of
plants
has,
over
the
centuries
increased
yield,
improved
disease
resistance
and
drought
tolerance,
eased
harvest
and
improved
the
taste
and
nutritional
value
of
crop
plants.
Careful
selection
and
breeding
have
had
enormous
effects
on
the
characteristics
of
crop
plants.
Plant
selection
and
breeding
in
the
1920s
and
1930s
improved
pasture
(grasses
and
clover)
in
New
Zealand.
Extensive
X-ray
an
ultraviolet
induced
mutagenesis
efforts
(i.e.
primitive
genetic
engineering)
during
the
1950s
produced
the
modern
commercial
varieties
of
grains
such
as
wheat,
corn
(maize)
and
barley.
The
green
revolution
popularized
the
use
of
conventional
hybridization
to
increase
yield
many
folds
by
creating
"high-yielding
varieties".
For
example,
average
yields
of
corn
(maize)
in
the
USA
have
increased
from
around
2.5
tons
per
hectare
(t/ha)
(40
bushels
per
acre)
in
1900
to
about
9.4
t/ha
(150
bushels
per
acre)
in
2001.
Similarly,
worldwide
average
wheat
yields
have
increased
from
less
than
1
t/ha
in
1900
to
more
than
2.5
t/ha
in
1990.
South
American
average
wheat
yields
are
around
2
t/ha,
African
under
1
t/ha,
Egypt
and
Arabia
up
to
3.5
to
4
t/ha
with
irrigation.
In
contrast,
the
average
wheat
yield
in
countries
such
as
France
is
over
8
t/ha.
Variations
in
yields
are
due
mainly
to
variation
in
climate,
genetics,
and
the
level
of
intensive
farming
techniques
(use
of
fertilizers,
chemical
pest
control,
growth
control
to
avoid
lodging)..
Genetic
Engineering.
Genetically
Modified
Organisms
(GMO)
are
organisms
whose
genetic
material
has
been
altered
by
genetic
engineering
techniques
generally
known
as
recombinant
DNA
technology.
Genetic
engineering
has
expanded
the
genes
available
to
breeders
to
utilize
in
creating
desired
germlines
for
new
crops.
After
mechanical
tomato-harvesters
were
developed
in
the
early
1960s,
agricultural
scientists
genetically
modified
tomatoes
to
be
more
resistant
to
mechanical
handling.
More
recently,
genetic
engineering
is
being
employed
in
various
parts
of
the
world,
to
create
crops
with
other
beneficial
traits.
Herbicide-tolerant
GMO
Crops.
Roundup-Ready
seed
has
a
herbicide
resistant
gene
implanted
into
its
genome
that
allows
the
plants
to
tolerate
exposure
to
glyphosate.
Roundup
is
a
trade
name
for
a
glyphosate
based
product,
which
is
a
systemic,
non-selective
herbicide
used
to
kill
weeds.
Roundup-Ready
seeds
allow
the
farmer
to
grow
a
crop
that
can
be
sprayed
with
glyphosate
to
control
weeds
without
harming
the
resistant
crop.
Herbicide-tolerant
crops
are
used
by
farmers
worldwide.
Today,
92%
of
soybean
acreage
in
the
US
is
planted
with
genetically-modified
herbicide-tolerant
plants.
With
the
increasing
use
of
herbicide-tolerant
crops,
comes
an
increase
in
the
use
of
glyphosate
based
herbicide
sprays.
In
some
areas
glyphosate
resistant
weeds
have
developed,
causing
farmers
to
switch
to
other
herbicides.
Some
studies
also
link
widespread
glyphosate
usage
to
iron
deficiencies
in
some
crops,
which
is
both
a
crop
production
and
a
nutritional
quality
concern,
with
potential
economic
and
health
implications.
Insect-Resistant
GMO
Crops.
Other
GMO
crops
utilized
by
growers
include
insect-resistant
crops,
which
have
a
gene
from
the
soil
bacterium
"Bacillus
thuringiensis"
(Bt),
which
produces
a
toxin
specific
to
insects.
These
crops
protect
plants
from
damage
by
insects;
one
such
crop
is
Starlink.
Another
is
cotton,
which
accounts
for
63%
of
US
cotton
acreage.
Some
believe
that
similar
or
better
pest-resistance
traits
can
be
acquired
through
traditional
breeding
practices,
and
resistance
to
various
pests
can
be
gained
through
hybridization
or
cross-pollination
with
wild
species.
In
some
cases,
wild
species
are
the
primary
source
of
resistance
traits;
some
tomato
cultivars
that
have
gained
resistance
to
at
least
nineteen
diseases
did
so
through
crossing
with
wild
populations
of
tomatoes.
Costs
and
Benefits
of
GMOs.
Genetic
engineers
may
someday
develop
transgenic
plants
which
would
allow
for
irrigation,
drainage,
conservation,
sanitary
engineering,
and
maintaining
or
increasing
yields
while
requiring
fewer
fossil
fuel
derived
inputs
than
conventional
crops.
Such
developments
would
be
particularly
important
in
areas
which
are
normally
arid
and
rely
upon
constant
irrigation,
and
on
large
scale
farms.
However,
genetic
engineering
of
plants
has
proven
to
be
controversial.
Many
issues
surrounding
food
security
and
environmental
impacts
have
risen
regarding
GMO
practices.
For
example,
GMOs
are
questioned
by
some
ecologists
and
economists
concerned
with
GMO
practices
such
as
terminator
seeds,
which
is
a
genetic
modification
that
creates
sterile
seeds.
Terminator
seeds
are
currently
under
strong
international
opposition
and
face
continual
efforts
of
global
bans.
Another
controversial
issue
is
the
patent
protection
given
to
companies
that
develop
new
types
of
seed
using
genetic
engineering.
Since
companies
have
intellectual
ownership
of
their
seeds,
they
have
the
power
to
dictate
terms
and
conditions
of
their
patented
product.
Currently,
ten
seed
companies
control
over
two-thirds
of
the
global
seed
sales.
Vandana
Shiva
argues
that
these
companies
are
guilty
of
biopiracy
by
patenting
life
and
exploiting
organisms
for
profit
Farmers
using
patented
seed
are
restricted
from
saving
seed
for
subsequent
plantings,
which
forces
farmers
to
buy
new
seed
every
year.
Since
seed
saving
is
a
traditional
practice
for
many
farmers
in
both
developing
and
developed
countries,
GMO
seeds
legally
bind
farmers
to
change
their
seed
saving
practices
to
buying
new
seed
every
year.
Locally
adapted
seeds
are
an
essential
hertitage
that
has
the
potential
to
be
lost
with
current
hybridized
crops
and
GMOs.
Locally
adapted
seeds,
also
called
land
races
or
crop
eco-types,
are
important
because
they
have
adapted
over
time
to
the
specific
microclimates,
soils,
other
environmental
conditions,
field
designs,
and
ethnic
preference
indigenous
to
the
exact
area
of
cultivation.
Introducing
GMOs
and
hybridized
commercial
seed
to
an
area
brings
the
risk
of
cross-pollination
with
local
land
races
Therefore,
GMOs
pose
a
threat
to
the
sustainability
of
land
races
and
the
ethnic
heritage
of
cultures.
Once
seed
contains
transgenic
material,
it
becomes
subject
to
the
conditions
of
the
seed
company
that
owns
the
patent
of
the
transgenic
material.
There
is
also
concern
that
GMOs
will
cross-pollinate
with
wild
species
and
permanently
alter
native
populations’
genetic
integrity;
there
are
already
identified
populations
of
wild
plants
with
transgenic
genes.
GMO
gene
flow
to
related
weed
species
is
a
concern,
as
well
as
cross-pollination
with
non-transgenic
crops.
Since
many
GMO
crops
are
harvested
for
their
seed,
such
as
rapeseed,
seed
spillage
in
is
problematic
for
volunteer
plants
in
rotated
fields,
as
well
as
seed-spillage
during
transportation.
Food
safety
and
labeling.
Food
security
issues
also
coincide
with
food
safety
and
food
labeling
concerns.
Currently
a
global
treaty,
the
BioSafety
Protocol,
regulates
the
trade
of
GMOs.
The
EU
currently
requires
all
GMO
foods
to
be
labeled,
whereas
the
US
does
not
require
transparent
labeling
of
GMO
foods.
Since
there
are
still
questions
regarding
the
safety
and
risks
associated
with
GMO
foods,
some
believe
the
public
should
have
the
freedom
to
choose
and
know
what
they
are
eating
and
require
all
GMO
products
to
be
labeled.
Environmental
impact.
Agriculture
imposes
external
costs
upon
society
through
pesticides,
nutrient
runoff,
excessive
water
usage,
and
assorted
other
problems.
A
2000
assessment
of
agriculture
in
the
UK
determined
total
external
costs
for
1996
of
£2,343
million,
or
£208
per
hectare.
A
2005
analysis
of
these
costs
in
the
USA
concluded
that
cropland
imposes
approximately
$5
to
16
billion
($30
to
$96
per
hectare),
while
livestock
production
imposes
$714
million.
Both
studies
concluded
that
more
should
be
done
to
internalize
external
costs,
and
neither
included
subsidies
in
their
analysis,
but
noted
that
subsidies
also
influence
the
cost
of
agriculture
to
society.
Both
focused
on
purely
fiscal
impacts.
The
2000
review
included
reported
pesticide
poisonings
but
did
not
include
speculative
chronic
effects
of
pesticides,
and
the
2004
review
relied
on
a
1992
estimate
of
the
total
impact
of
pesticides.
Livestock
issues.
A
senior
UN
official
and
co-author
of
a
UN
report
detailing
this
problem,
Henning
Steinfeld,
said
"Livestock
are
one
of
the
most
significant
contributors
to
today's
most
serious
environmental
problems".
Livestock
production
occupies
70%
of
all
land
used
for
agriculture,
or
30%
of
the
land
surface
of
the
planet.
It
is
one
of
the
largest
sources
of
greenhouse
gases,
responsible
for
18%
of
the
world's
greenhouse
gas
emissions
as
measured
in
CO2
equivalents.
By
comparison,
all
transportation
emits
13.5%
of
the
CO2.
It
produces
65%
of
human-related
nitrous
oxide
(which
has
296
times
the
global
warming
potential
of
CO2,)
and
37%
of
all
human-induced
methane
(which
is
23
times
as
warming
as
CO2.
It
also
generates
64%
of
the
ammonia,
which
contributes
to
acid
rain
and
acidification
of
ecosystems.
Livestock
expansion
is
cited
as
a
key
factor
driving
deforestation,
in
the
Amazon
basin
70%
of
previously
forested
area
is
now
occupied
by
pastures
and
the
remainder
used
for
feedcrops.
Through
deforestation
and
land
degradation,
livestock
is
also
driving
reductions
in
biodiversity.
Land
transformation
and
degradation.
Land
transformation,
the
use
of
land
to
yield
goods
and
services,
is
the
most
substantial
way
humans
alter
the
Earth's
ecosystems,
and
is
considered
the
driving
force
in
the
loss
of
biodiversity.
Estimates
of
the
amount
of
land
transformed
by
humans
vary
from
39–50%.
Land
degradation,
the
long-term
decline
in
ecosystem
function
and
productivity,
is
estimated
to
be
occurring
on
24%
of
land
worldwide,
with
cropland
overrepresented.
The
UN-FAO
report
cites
land
management
as
the
driving
factor
behind
degradation
and
reports
that
1.5
billion
people
rely
upon
the
degrading
land.
Degradation
can
be
deforestation,
desertification,
soil
erosion,
mineral
depletion,
or
chemical
degradation
(acidification
and
salinization).
Eutrophication.
Eutrophication,
excessive
nutrients
in
aquatic
ecosystems
resulting
in
algal
blooms
and
anoxia,
leads
to
fish
kills,
loss
of
biodiversity,
and
renders
water
unfit
for
drinking
and
other
industrial
uses.
Excessive
fertilization
and
manure
application
to
cropland,
as
well
as
high
livestock
stocking
densities
cause
nutrient
(mainly
nitrogen
and
phosphorus)
runoff
and
leaching
from
agricultural
land.
These
nutrients
are
major
nonpoint
pollutants
contributing
to
eutrophication
of
aquatic
ecosystems.
Pesticides.
Pesticide
use
has
increased
since
1950
to
2.5
million
tons
annually
worldwide,
yet
crop
loss
from
pests
has
remained
relatively
constant.
The
World
Health
Organization
estimated
in
1992
that
3
million
pesticide
poisonings
occur
annually,
causing
220,000
deaths.
Pesticides
select
for
pesticide
resistance
in
the
pest
population,
leading
to
a
condition
termed
the
'pesticide
treadmill'
in
which
pest
resistance
warrants
the
development
of
a
new
pesticide.
An
alternative
argument
is
that
the
way
to
'save
the
environment'
and
prevent
famine
is
by
using
pesticides
and
intensive
high
yield
farming,
a
view
exemplified
by
a
quote
heading
the
Center
for
Global
Food
Issues
website:
'Growing
more
per
acre
leaves
more
land
for
nature'.
However,
critics
argue
that
a
trade-off
between
the
environment
and
a
need
for
food
is
not
inevitable,
and
that
pesticides
simply
replace
good
agronomic
practices
such
as
crop
rotation.
Climate
Change.
Climate
change
has
the
potential
to
affect
agriculture
through
changes
in
temperature,
rainfall
(timing
and
quantity),
CO2,
solar
radiation
and
the
interaction
of
these
elements.
Agriculture
can
both
mitigate
or
worsen
global
warming.
Some
of
the
increase
in
CO2
in
the
atmosphere
comes
from
the
decomposition
of
organic
matter
in
the
soil,
and
much
of
the
methane
emitted
into
the
atmosphere
is
caused
by
the
decomposition
of
organic
matter
in
wet
soils
such
as
rice
paddies.
Further,
wet
or
anaerobic
soils
also
lose
nitrogen
through
denitrification,
releasing
the
greenhouse
gas
nitric
oxide.
Changes
in
management
can
reduce
the
release
of
these
greenhouse
gases,
and
soil
can
further
be
used
to
sequester
some
of
the
CO2
in
the
atmosphere.
Distortions
in
modern
global
agriculture.
Differences
in
economic
development,
population
density
and
culture
mean
that
the
farmers
of
the
world
operate
under
very
different
conditions.
A
US
cotton
farmer
may
receive
US$230
in
government
subsidies
per
acre
planted
(in
2003),
while
farmers
in
Mali
and
other
third-world
countries
do
without.
When
prices
decline,
the
heavily
subsidised
US
farmer
is
not
forced
to
reduce
his
output,
making
it
difficult
for
cotton
prices
to
rebound,
but
his
Mali
counterpart
may
go
broke
in
the
meantime.
A
livestock
farmer
in
South
Korea
can
calculate
with
a
(highly
subsidized)
sales
price
of
US$1300
for
a
calf
produced.
A
South
American
Mercosur
country
rancher
calculates
with
a
calf's
sales
price
of
US$120–200
(both
2008
figures).
With
the
former,
scarcity
and
high
cost
of
land
is
compensated
with
public
subsidies,
the
latter
compensates
absence
of
subsidies
with
economics
of
scale
and
low
cost
of
land.
In
the
Peoples
Republic
of
China,
a
rural
household's
productive
asset
may
be
one
hectare
of
farmland.
In
Brazil,
Paraguay
and
other
countries
where
local
legislature
allows
such
purchases,
international
investors
buy
thousands
of
hectares
of
farmland
or
raw
land
at
prices
of
a
few
hundred
US$
per
hectare.
Energy
and
Agriculture.
Since
the
1940s,
agricultural
productivity
has
increased
dramatically,
due
largely
to
the
increased
use
of
energy-intensive
mechanization,
fertilizers
and
pesticides.
The
vast
majority
of
this
energy
input
comes
from
fossil
fuel
sources.
Between
1950
and
1984,
the
Green
Revolution
transformed
agriculture
around
the
globe,
with
world
grain
production
increasing
by
250%
as
world
population
doubled.
Modern
agriculture's
heavy
reliance
on
petrochemicals
and
mechanization
has
raised
concerns
that
oil
shortages
could
increase
costs
and
reduce
agricultural
output,
causing
food
shortages.
Modern
or
industrialized
agriculture
is
dependent
on
fossil
fuels
in
two
fundamental
ways:
1)
direct
consumption
on
the
farm
and
2)
indirect
consumption
to
manufacture
inputs
used
on
the
farm.
Direct
consumption
includes
the
use
of
lubricants
and
fuels
to
operate
farm
vehicles
and
machinery;
and
use
of
gas,
liquid
propane,
and
electricity
to
power
dryers,
pumps,
lights,
heaters,
and
coolers.
American
farms
directly
consumed
about
1.2
exajoules
(1.1
quadrillion
BTU)
in
2002,
or
just
over
1
percent
of
the
nation's
total
energy.
Indirect
consumption
is
mainly
oil
and
natural
gas
used
to
manufacture
fertilizers
and
pesticides,
which
accounted
for
0.6
exajoules
(0.6
quadrillion
BTU)
in
2002.
The
energy
used
to
manufacture
farm
machinery
is
also
a
form
of
indirect
agricultural
energy
consumption,
but
it
is
not
included
in
USDA
estimates
of
U.S.
agricultural
energy
use.
Together,
direct
and
indirect
consumption
by
U.S.
farms
accounts
for
about
2
percent
of
the
nation's
energy
use.
Direct
and
indirect
energy
consumption
by
U.S.
farms
peaked
in
1979,
and
has
gradually
declined
over
the
past
30
years.
Food
systems
encompass
not
just
agricultural
production,
but
also
off-farm
processing,
packaging,
transporting,
marketing,
consumption,
and
disposal
of
food
and
food-related
items.
Agriculture
accounts
for
approximately
one-fifth
of
food
system
energy
use
in
the
United
States.
Oil
shortages
could
impact
this
food
supply.
Some
farmers
using
modern
organic-farming
methods
have
reported
yields
as
high
as
those
available
from
conventional
farming
without
the
use
of
synthetic
fertilizers
and
pesticides.
However,
the
reconditioning
of
soil
to
restore
nutrients
lost
during
the
use
of
monoculture
agriculture
techniques
made
possible
by
petroleum-based
technology
takes
time.
In
2007,
higher
incentives
for
farmers
to
grow
non-food
biofuel
crops
combined
with
other
factors
(such
as
over-development
of
former
farm
lands,
rising
transportation
costs,
climate
change,
growing
consumer
demand
in
China
and
India,
and
population
growth)
to
cause
food
shortages
in
Asia,
the
Middle
East,
Africa,
and
Mexico,
as
well
as
rising
food
prices
around
the
globe.
As
of
December
2007,
37
countries
faced
food
crises,
and
20
had
imposed
some
sort
of
food-price
controls.
Some
of
these
shortages
resulted
in
food
riots
and
even
deadly
stampedes.
The
biggest
fossil
fuel
input
to
agriculture
is
the
use
of
natural
gas
as
a
hydrogen
source
for
the
Haber-Bosch
fertilizer-creation
process.
Natural
gas
is
used
because
it
is
the
cheapest
currently
available
source
of
hydrogen.
When
oil
production
becomes
so
scarce
that
natural
gas
is
used
as
a
partial
stopgap
replacement,
and
hydrogen
use
in
transportation
increases,
natural
gas
will
become
much
more
expensive.
If
the
Haber
Process
is
unable
to
be
commercialized
using
renewable
energy
(such
as
by
electrolysis)
or
if
other
sources
of
hydrogen
are
not
available
to
replace
the
Haber
Process,
in
amounts
sufficient
to
supply
transportation
and
agricultural
needs,
this
major
source
of
fertilizer
would
either
become
extremely
expensive
or
unavailable.
This
would
either
cause
food
shortages
or
dramatic
rises
in
food
prices.
Mitigation
of
effects
of
petroleum
shortages.
One
effect
oil
shortages
could
have
on
agriculture
is
a
full
return
to
organic
agriculture.
In
light
of
peak-oil
concerns,
organic
methods
are
more
sustainable
than
contemporary
practices
because
they
use
no
petroleum-based
pesticides,
herbicides,
or
fertilizers.
Some
farmers
using
modern
organic-farming
methods
have
reported
yields
as
high
as
those
available
from
conventional
farming.
Organic
farming
may
however
be
more
labor-intensive
and
would
require
a
shift
of
the
workforce
from
urban
to
rural
areas.
It
has
been
suggested
that
rural
communities
might
obtain
fuel
from
the
biochar
and
synfuel
process,
which
uses
agricultural
"waste"
to
provide
charcoal
fertilizer,
some
fuel
"and"
food,
instead
of
the
normal
food
vs
fuel
debate.
As
the
synfuel
would
be
used
on-site,
the
process
would
be
more
efficient
and
might
just
provide
enough
fuel
for
a
new
organic-agriculture
fusion.
It
has
been
suggested
that
some
transgenic
plants
may
some
day
be
developed
which
would
allow
for
maintaining
or
increasing
yields
while
requiring
fewer
fossil-fuel-derived
inputs
than
conventional
crops.
The
possibility
of
success
of
these
programs
is
questioned
by
ecologists
and
economists
concerned
with
unsustainable
GMO
practices
such
as
terminator
seeds,
and
a
January
2008
report
shows
that
GMO
practices
"fail
to
deliver
environmental,
social
and
economic
benefits."
While
there
has
been
some
research
on
sustainability
using
GMO
crops,
at
least
one
hyped
and
prominent
multi-year
attempt
by
Monsanto
Company
has
been
unsuccessful,
though
during
the
same
period
traditional
breeding
techniques
yielded
a
more
sustainable
variety
of
the
same
crop.
Additionally,
a
survey
by
the
bio-tech
industry
of
subsistence
farmers
in
Africa
to
discover
what
GMO
research
would
most
benefit
sustainable
agriculture
only
identified
non-transgenic
issues
as
areas
needing
to
be
addressed.
Nevertheless,
some
governments
in
Africa
continue
to
view
investments
in
new
transgenic
technologies
as
an
essential
component
of
efforts
to
improve
sustainability.
---END.OF.DOCUMENT---
Aldous
Huxley.
Aldous
Leonard
Huxley
(26
July
1894 –
22
November
1963)
was
an
English
writer
and
one
of
the
most
prominent
members
of
the
famous
Huxley
family.
He
spent
the
later
part
of
his
life
in
the
United
States,
living
in
Los
Angeles
from
1937
until
his
death
in
1963.
Best
known
for
his
novels
including
"Brave
New
World"
and
wide-ranging
output
of
essays,
Huxley
also
edited
the
magazine
"Oxford
Poetry",
and
published
short
stories,
poetry,
travel
writing,
and
film
stories
and
scripts.
Aldous
Huxley
was
a
humanist
and
pacifist,
and
he
was
latterly
interested
in
spiritual
subjects
such
as
parapsychology
and
philosophical
mysticism.
He
is
also
well
known
for
advocating
and
taking
psychedelics.
By
the
end
of
his
life
Huxley
was
considered,
in
some
academic
circles,
a
leader
of
modern
thought
and
an
intellectual
of
the
highest
rank,
and
highly
regarded
as
one
of
the
most
prominent
explorers
of
visual
communication
and
sight-related
theories
as
well.
Early
years.
Aldous
Huxley
was
born
in
Godalming,
Surrey,
UK
in
1894.
He
was
the
third
son
of
the
writer
and
school-master
Leonard
Huxley
and
first
wife,
Julia
Arnold
who
founded
Prior's
Field
School.
Julia
was
the
niece
of
Matthew
Arnold
and
the
sister
of
Mrs.
Humphrey
Ward.
Aldous
was
the
grandson
of
Thomas
Henry
Huxley,
the
zoologist,
agnostic
and
controversialist
("Darwin's
Bulldog").
His
brother
Julian
Huxley
and
half-brother
Andrew
Huxley
also
became
outstanding
biologists.
Huxley
had
another
brother
Noel
Trevenen
(1891–1914)
who
committed
suicide
after
a
period
of
clinical
depression.
Huxley
began
his
learning
in
his
father's
well-equipped
botanical
laboratory,
then
continued
in
a
school
named
Hillside.
His
teacher
was
his
mother
who
supervised
him
for
several
years
until
she
became
terminally
ill.
After
Hillside,
he
was
educated
at
Eton
College.
Huxley's
mother
died
in
1908,
when
he
was
fourteen.
In
1911,
he
suffered
an
illness
(keratitis
punctata)
which
"left
[him]
practically
blind
for
two
to
three
years".
Aldous's
near-blindness
disqualified
him
from
service
in
the
First
World
War.
Once
his
eyesight
recovered
sufficiently,
he
was
able
to
study
English
literature
at
Balliol
College,
Oxford.
In
1916
he
edited
"Oxford
Poetry"
and
later
graduated
with
first
class
honours.
Following
his
education
at
Balliol,
Huxley
was
financially
indebted
to
his
father
and
had
to
earn
a
living.
He
taught
French
for
a
year
at
Eton,
where
Eric
Blair
(later
known
by
the
pen
name
George
Orwell)
and
Stephen
Runciman
were
among
his
pupils,
but
was
remembered
as
an
incompetent
and
hopeless
teacher
who
couldn’t
keep
discipline.
Nevertheless,
Blair
and
others
were
impressed
by
his
use
of
words.
For
a
short
while
in
1918,
he
was
employed
acquiring
provisions
at
the
Air
Ministry.
Significantly,
Huxley
also
worked
for
a
time
in
the
1920s
at
the
technologically-advanced
Brunner
and
Mond
chemical
plant
in
Billingham,
Teesside,
and
the
most
recent
introduction
to
his
famous
science
fiction
novel
"Brave
New
World"
(1932)
states
that
this
experience
of
"an
ordered
universe
in
a
world
of
planless
incoherence"
was
one
source
for
the
novel.
Huxley
completed
his
first
(unpublished)
novel
at
the
age
of
seventeen
and
began
writing
seriously
in
his
early
twenties.
His
earlier
work
includes
important
novels
on
the
dehumanizing
aspects
of
scientific
progress,
most
famously
"Brave
New
World",
and
on
pacifist
themes
(for
example,
"Eyeless
in
Gaza").
In
"Brave
New
World"
Huxley
portrays
a
society
operating
on
the
principles
of
mass
production
and
Pavlovian
conditioning.
Huxley
was
strongly
influenced
by
F.
Matthias
Alexander
and
included
him
as
a
character
in
"Eyeless
in
Gaza".
Middle
years.
During
the
First
World
War,
Huxley
spent
much
of
his
time
at
Garsington
Manor,
home
of
Lady
Ottoline
Morrell,
working
as
a
farm
labourer.
Here
he
met
several
Bloomsbury
figures
including
Bertrand
Russell
and
Clive
Bell.
Later,
in
"Crome
Yellow"
(1921)
he
caricatured
the
Garsington
lifestyle.
In
1919
he
married
Maria
Nys
(10
September
1899
-
12
February
1955),
a
Belgian
woman
he
met
at
Garsington.
They
had
one
child,
Matthew
Huxley
(19
April
1920
-
10
February
2005),
who
had
a
career
as
an
epidemiologist.
The
family
lived
in
Italy
part
of
the
time
in
the
1920s,
where
Huxley
would
visit
his
friend
D.
H.
Lawrence.
Following
Lawrence's
death
in
1930,
Huxley
edited
Lawrence's
letters
(1933).
In
1937,
Huxley
moved
to
Hollywood,
California
with
his
wife
Maria,
son
Matthew,
and
friend
Gerald
Heard.
He
lived
in
the
U.S.,
mainly
in
southern
California,
until
his
death,
but
also
for
a
time
in
Taos,
New
Mexico,
where
he
wrote
"Ends
and
Means"
(published
in
1937).
In
this
work
he
examines
the
fact
that
although
most
people
in
modern
civilization
agree
that
they
want
a
world
of
"liberty,
peace,
justice,
and
brotherly
love",
they
have
not
been
able
to
agree
on
how
to
achieve
it.
Heard
introduced
Huxley
to
Vedanta
(Veda-Centric
Hinduism),
meditation,
and
vegetarianism
through
the
principle
of
ahimsa.
In
1938
Huxley
befriended
J.
Krishnamurti,
whose
teachings
he
greatly
admired.
He
also
became
a
Vedantist
in
the
circle
of
Hindu
Swami
Prabhavananda,
and
introduced
Christopher
Isherwood
to
this
circle.
Not
long
after,
Huxley
wrote
his
book
on
widely
held
spiritual
values
and
ideas,
"The
Perennial
Philosophy",
which
discussed
the
teachings
of
renowned
mystics
of
the
world.
Huxley
became
a
close
friend
of
Remsen
Bird,
president
of
Occidental
College.
He
spent
much
time
at
the
college,
which
is
in
the
Eagle
Rock
neighborhood
of
Los
Angeles.
The
college
appears
as
"Tarzana
College"
in
his
satirical
novel
"After
Many
a
Summer"
(1939).
The
novel
won
Huxley
that
year's
James
Tait
Black
Memorial
Prize
for
fiction.
Huxley
also
incorporated
Bird
into
the
novel.
During
this
period
Huxley
earned
some
Hollywood
income
as
a
writer.
In
March
1938,
his
friend
Anita
Loos,
a
novelist
and
screenwriter,
put
him
in
touch
with
Metro-Goldwyn-Mayer
who
hired
Huxley
for
"Madame
Curie"
which
was
originally
to
star
Greta
Garbo
and
be
directed
by
George
Cukor.
(The
film
was
eventually
filmed
by
MGM
in
1943
with
a
different
director
and
stars.)
Huxley
received
screen
credit
for
"Pride
and
Prejudice"
(1940)
and
was
paid
for
his
work
on
a
number
of
other
films,
including
"Jane
Eyre"
(1944).
However,
his
experience
in
Hollywood
was
not
a
success.
When
he
wrote
a
synopsis
of
"Alice
in
Wonderland",
Walt
Disney
rejected
it
on
the
grounds
that
"he
could
only
understand
every
third
word".
Huxley's
leisurely
development
of
ideas,
it
seemed,
was
not
suitable
for
the
movie
moguls,
who
demanded
fast,
dynamic
dialogue
above
all
else.
On
21
October
1949,
Huxley
wrote
to
George
Orwell,
author
of
"Nineteen
Eighty-Four",
congratulating
Orwell
on
"how
fine
and
how
profoundly
important
the
book
is".
In
his
letter
to
Orwell,
he
predicted:
"Within
the
next
generation
I
believe
that
the
world's
leaders
will
discover
that
infant
conditioning
and
narco-hypnosis
are
more
efficient,
as
instruments
of
government,
than
clubs
and
prisons,
and
that
the
lust
for
power
can
be
just
as
completely
satisfied
by
suggesting
people
into
loving
their
servitude
as
by
flogging
them
and
kicking
them
into
obedience."
Post-war.
After
the
Second
World
War
Huxley
applied
for
United
States
citizenship,
but
his
application
was
continuously
deferred
on
the
grounds
that
he
would
not
say
he
would
take
up
arms
to
defend
the
U.S.,
so
he
withdrew
it.
Nevertheless,
he
remained
in
the
country,
and
in
1959
he
turned
down
an
offer
of
a
Knight
Bachelor
by
the
Macmillan
government.
During
the
1950s
Huxley's
interest
in
the
field
of
psychical
research
grew
keener,
and
his
later
works
are
strongly
influenced
by
both
mysticism
and
his
experiences
with
psychedelic
drugs.
In
October
1930,
the
occultist
Aleister
Crowley
dined
with
Huxley
in
Berlin,
and
to
this
day
rumours
persist
that
Crowley
introduced
Huxley
to
peyote
on
that
occasion.
He
was
introduced
to
mescaline
(considered
to
be
the
key
active
ingredient
of
peyote)
by
the
psychiatrist
Humphry
Osmond
in
1953.
Through
Dr.
Osmond,
Huxley
met
millionaire
Alfred
Matthew
Hubbard
who
would
deal
with
LSD
on
a
wholesale
basis.
On
24
December
1955,
Huxley
took
his
first
dose
of
LSD.
Indeed,
Huxley
was
a
pioneer
of
self-directed
psychedelic
drug
use
"in
a
search
for
enlightenment",
famously
taking
100
micrograms
of
LSD
as
he
lay
dying.
His
psychedelic
drug
experiences
are
described
in
the
essays
"The
Doors
of
Perception"
(the
title
deriving
from
some
lines
in
the
book
"The
Marriage
of
Heaven
and
Hell"
by
William
Blake),
and
"Heaven
and
Hell".
Some
of
his
writings
on
psychedelics
became
frequent
reading
among
early
hippies.
While
living
in
Los
Angeles,
Huxley
was
a
friend
of
Ray
Bradbury.
According
to
Sam
Weller's
biography
of
Bradbury,
the
latter
was
dissatisfied
with
Huxley,
especially
after
Huxley
encouraged
Bradbury
to
take
psychedelic
drugs.
In
1955,
Huxley's
wife,
Maria,
died
of
breast
cancer.
In
1956
he
married
Laura
Archera
(1911–2007),
also
an
author.
She
wrote
"This
Timeless
Moment",
a
biography
of
Huxley.
In
1960
Huxley
himself
was
diagnosed
with
cancer,
and
in
the
years
that
followed,
with
his
health
deteriorating,
he
wrote
the
Utopian
novel
"Island",
and
gave
lectures
on
"Human
Potentialities"
at
the
Esalen
institute,
which
were
fundamental
to
the
forming
of
the
Human
Potential
Movement.
On
his
deathbed,
unable
to
speak,
Huxley
made
a
written
request
to
his
wife
for
"LSD,
100
µg,
intramuscular".
According
to
her
account
of
his
death,
in
"This
Timeless
Moment",
she
obliged
with
an
injection
at
11:45
am
and
another
a
couple
of
hours
later.
He
died
at
5:21
pm
on
22
November
1963,
aged
69.
Huxley's
ashes
were
interred
in
the
family
grave
at
the
Watts
Cemetery,
home
of
the
Watts
Mortuary
Chapel
in
Compton,
a
village
near
Guildford,
Surrey,
England.
Media
coverage
of
his
death
was
overshadowed
by
the
assassination
of
President
John
F.
Kennedy,
on
the
same
day,
as
was
the
death
of
the
Irish
author
C.
S.
Lewis.
This
coincidence
was
the
inspiration
for
Peter
Kreeft's
book
'.
Huxley's
only
child,
Matthew
Huxley,
was
also
an
author,
as
well
as
an
educator,
anthropologist,
and
prominent
epidemiologist.
Aldous
Huxley
is
also
survived
by
two
grandchildren.
Association
with
Vedanta.
Beginning
in
1939
and
continuing
until
his
death
in
1963,
Huxley
had
an
extensive
association
with
the
Vedanta
Society
of
Southern
California,
founded
and
headed
by
Swami
Prabhavananda.
Together
with
Gerald
Heard,
Christopher
Isherwood,
and
other
followers
he
was
initiated
by
the
Swami
and
was
taught
meditation
and
spiritual
practices.
In
1944
Huxley
wrote
the
introduction
to
the
"Bhagavad
Gita:
The
Song
of
God",
translated
by
Swami
Prabhavanada
and
Christopher
Isherwood,
which
was
published
by
The
Vedanta
Society
of
Southern
California.
From
1941
through
1960
Huxley
contributed
48
articles
to
"Vedanta
and
the
West",
published
by
the
Society.
He
also
served
on
the
editorial
board
with
Isherwood,
Heard,
and
playwright
John
van
Druten
from
1951
through
1962.
Huxley
also
occasionally
lectured
at
the
Hollywood
and
Santa
Barbara
Vedanta
temples.
Two
of
those
lectures
have
been
released
on
CD:
"Knowledge
and
Understanding"
and
"Who
Are
We"
from
1955.
After
the
publication
of
"The
Doors
of
Perception",
Huxley
and
the
Swami
disagreed
about
the
meaning
and
importance
of
the
LSD
drug
experience,
which
may
have
caused
the
relationship
to
cool,
but
Huxley
continued
to
write
articles
for
the
Society's
journal,
lecture
at
the
temple,
and
attend
social
functions.
Literary
themes.
"Crome
Yellow"
(1921)
attacks
Victorian
and
Edwardian
social
principles
which
led
to
World
War
I
and
its
terrible
aftermath.
Together
with
Huxley's
second
novel,
"Antic
Hay"
(1923),
the
book
expresses
much
of
the
mood
of
disenchantment
of
the
early
1920s.
It
was
intended
to
reflect,
as
Huxley
stated
in
a
letter
to
his
father,
"the
life
and
opinions
of
an
age
which
has
seen
the
violent
disruption
of
almost
all
the
standards,
conventions
and
values
current
in
the
present
epoch."
Huxley's
reputation
for
iconoclasm
and
emancipation
grew.
He
was
condemned
for
his
explicit
discussion
of
sex
and
free
thought
in
his
fiction.
"Antic
Hay",
for
example,
was
burned
in
Cairo
and
in
the
years
that
followed
many
of
Huxley's
books
were
received
with
disapproval
or
banned
at
one
time
or
another.
The
exclusion
of
"Brave
New
World",
"Point
Counter
Point"
and
"Island"
from
"Time"
magazine's
Best
100
novels
list
in
2006
created
an
uproar.
Huxley,
however,
said
that
a
novel
should
be
full
of
interesting
opinions
and
arresting
ideas,
describing
his
aim
as
a
novelist
as
being
'to
arrive,
technically,
at
a
perfect
fusion
of
the
novel
and
the
essay';
and
with
"Point
Counter
Point"
(1928),
Huxley
wrote
his
first
true
'novel
of
ideas',
the
type
of
thought-provoking
fiction
with
which
he
is
now
associated.
One
of
his
main
ideas
was
pessimism
about
the
cultural
future
of
society,
a
pessimism
which
sprang
largely
from
his
visit
to
the
United
States
between
September
1925
and
June
1926.
He
recounted
his
experiences
in
"Jesting
Pilate"
(1926):
"The
thing
which
is
happening
in
America
is
a
reevaluation
of
values,
a
radical
alteration
(for
the
worse)
of
established
standards",
and
it
was
soon
after
this
visit
that
he
conceived
the
idea
of
writing
a
satire
of
what
he
had
encountered.
"Brave
New
World"
(1932)
as
well
as
"Island"
(1962)
form
the
cornerstone
of
Huxley's
damning
indictment
of
commercialism
based
upon
goods
generally
manufactured
from
other
countries.
Indeed
also,
"Brave
New
World"
(along
with
Orwell's
"Nineteen
Eighty-Four"
and
Yevgeni
Zamyatin's
"We")
helped
form
the
anti-utopian
or
dystopian
tradition
in
literature
and
has
become
synonymous
with
a
future
world
in
which
the
human
spirit
is
subject
to
conditioning
and
control.
"Island"
acts
as
an
antonym
to
"Brave
New
World";
it
is
described
as
"one
of
the
truly
great
philosophical
novels".
He
devoted
his
time
at
his
small
house
at
Llano
in
the
Mojave
Desert
to
a
life
of
contemplation,
mysticism,
and
experimentation
with
hallucinogenic
drugs.
His
suggestions
in
The
Doors
of
Perception
(1954)
that
mescaline
and
lysergic
acid
were
'drugs
of
unique
distinction'
which
should
be
exploited
for
the
'supernaturally
brilliant'
visionary
experience
they
offered
provoked
even
more
outrage
than
his
passionate
defense
of
the
Bates
method
in
"The
Art
of
Seeing"
(1942).
However,
the
book
went
on
to
become
a
cult
text
in
the
psychedelic
1960s,
and
inspire
the
name
of
the
rock
band
The
Doors
(although
it
was
originally
derived
from
William
Blake's
"Marriage
of
Heaven
and
Hell").
Huxley
also
appears
on
the
sleeve
of
The
Beatles'
landmark
1967
album
"Sgt.
Pepper's
Lonely
Hearts
Club
Band".
Eyesight.
With
respect
to
details
about
the
true
quality
of
Huxley’s
eyesight
at
specific
points
in
his
life,
there
are
differing
accounts.
Around
1939,
Huxley
encountered
the
Bates
Method
for
better
eyesight,
and
a
teacher,
Margaret
Corbett,
who
was
able
to
teach
him
in
the
method.
In
1940,
Huxley
relocated
from
Hollywood
to
a
"ranchito"
in
the
high
desert
hamlet
of
Llano,
California
in
northernmost
Los
Angeles
County.
Huxley
then
said
that
his
sight
improved
dramatically
with
the
Bates
Method
and
the
extreme
and
pure
natural
lighting
of
the
southwestern
American
desert.
He
reported
that
for
the
first
time
in
over
25
years,
he
was
able
to
read
without
glasses
and
without
strain.
He
even
tried
driving
a
car
along
the
dirt
road
beside
the
ranch.
He
wrote
a
book
about
his
successes
with
the
Bates
Method,
"The
Art
of
Seeing"
which
was
published
in
1942
(US),
1943
(UK).
It
was
from
this
period,
with
the
publication
of
the
generally
disputed
theories
contained
in
the
latter
book,
that
a
growing
degree
of
popular
controversy
arose
over
the
subject
of
Huxley’s
eyesight.
"Then
suddenly
he
faltered—and
the
disturbing
truth
became
obvious.
He
wasn't
reading
his
address
at
all.
He
had
learned
it
by
heart.
To
refresh
his
memory
he
brought
the
paper
closer
and
closer
to
his
eyes.
When
it
was
only
an
inch
or
so
away
he
still
couldn't
read
it,
and
had
to
fish
for
a
magnifying
glass
in
his
pocket
to
make
the
typing
visible
to
him.
It
was
an
agonizing
moment."
"...Although
I
feel
it
was
an
injustice
to
treat
Aldous
as
though
he
were
blind,
it
is
true
there
were
many
indications
of
his
impaired
vision.
For
instance,
although
Aldous
did
not
wear
glasses,
he
would
quite
often
use
a
magnifying
lens..."
"The
most
characteristic
fact
about
the
functioning
of
the
total
organism,
or
any
part
of
the
organism,
is
that
it
is
not
constant,
but
highly
variable."
Nevertheless,
the
topic
of
Huxley’s
eyesight
continues
to
endure
similar,
significant
controversy,
regardless
of
how
trivial
a
subject
matter
it
might
initially
appear.
Awards.
In
1959
Aldous
Huxley
received
the
American
Academy
of
Arts
and
Letters
Award
of
Merit
for
the
novel
"Brave
New
World".
He
received
the
James
Tait
Black
Memorial
Prize
in
1939
for
"After
Many
a
Summer
Dies
the
Swan".
In
1962,
Huxley
was
awarded
the
Companion
of
Literature
by
the
Royal
Society
of
Literature.
Films.
Notable
works
include
the
original
screenplay
for
Disney's
animated
"Alice
in
Wonderland"
(which
was
rejected
because
it
was
too
literary),
two
productions
of
"Brave
New
World",
one
of
"Point
Counter
Point",
one
of
"Eyeless
in
Gaza",
and
one
of
"Ape
and
Essence".
He
was
a
credited
screenwriter
for
"Pride
and
Prejudice"
(1940),
co-authored
the
screenplay
for
"Jane
Eyre"
(1944)
with
John
Houseman,
"A
Woman's
Vengeance"
(1947),
and
contributed
to
the
screenplays
of
"Madame
Curie"
(1943)
and
"Alice
in
Wonderland"
(1951)
without
credit.
Director
Ken
Russell's
1971
film
"The
Devils",
starring
Vanessa
Redgrave
and
Oliver
Reed,
was
adapted
from
Huxley's
"The
Devils
of
Loudun".
A
made-for-television
adaptation
of
"Brave
New
World"
was
made
in
1990.
---END.OF.DOCUMENT---
Aberdeen
(disambiguation).
Aberdeen
is
a
city
in
Scotland,
United
Kingdom.
---END.OF.DOCUMENT---
Algae.
Algae
(or;
singular
"alga",
Latin
for
"seaweed")
are
a
large
and
diverse
group
of
simple,
typically
autotrophic
organisms,
ranging
from
unicellular
to
multicellular
forms.
The
largest
and
most
complex
marine
forms
are
called
seaweeds.
They
are
photosynthetic,
like
plants,
and
"simple"
because
they
lack
the
many
distinct
organs
found
in
land
plants.
Though
the
prokaryotic
"Cyanobacteria"
(commonly
referred
to
as
blue-green
algae)
were
traditionally
included
as
"algae"
in
older
textbooks,
many
modern
sources
regard
this
as
outdated
as
they
are
now
considered
to
be
closely
related
to
bacteria.
The
term
"algae"
is
now
restricted
to
eukaryotic
organisms.
All
true
algae
therefore
have
a
nucleus
enclosed
within
a
membrane
and
chloroplasts
bound
in
one
or
more
membranes.
Algae
constitute
a
paraphyletic
and
polyphyletic
group,
as
they
do
not
include
all
the
descendants
of
the
last
universal
ancestor
nor
do
they
all
descend
from
a
common
algal
ancestor,
although
their
chloroplasts
seem
to
have
a
single
origin.
Diatoms
are
also
examples
of
algae.
Algae
lack
the
various
structures
that
characterize
land
plants,
such
as
phyllids
(leaves)
and
rhizoids
in
nonvascular
plants,
or
leaves,
roots,
and
other
organs
that
are
found
in
tracheophytes
(vascular
plants).
Many
are
photoautotrophic,
although
some
groups
contain
members
that
are
mixotrophic,
deriving
energy
both
from
photosynthesis
and
uptake
of
organic
carbon
either
by
osmotrophy,
myzotrophy,
or
phagotrophy.
Some
unicellular
species
rely
entirely
on
external
energy
sources
and
have
limited
or
no
photosynthetic
apparatus.
Nearly
all
algae
have
photosynthetic
machinery
ultimately
derived
from
the
Cyanobacteria,
and
so
produce
oxygen
as
a
by-product
of
photosynthesis,
unlike
other
photosynthetic
bacteria
such
as
purple
and
green
sulfur
bacteria.
Fossilized
filamentous
algae
from
the
Vindhya
basin
have
been
dated
back
to
1.6
to
1.7
billion
years
ago.
The
first
alga
to
have
its
genome
sequenced
was
"Cyanidioschyzon
merolae".
Etymology
and
study.
The
singular
"alga"
is
the
Latin
word
for
a
particular
seaweed
and
retains
that
meaning
in
English.
The
etymology
is
obscure.
Although
some
speculate
that
it
is
related
to
Latin
"algēre",
"be
cold",
there
is
no
known
reason
to
associate
seaweed
with
temperature.
A
more
likely
source
is
"alliga",
"binding,
entwining."
Since
Algae
has
become
a
biological
classification,
alga
can
also
mean
one
classification
under
Algae,
parallel
to
a
fungus
being
a
species
of
fungi,
a
plant
being
a
species
of
plant,
and
so
on.
The
ancient
Greek
word
for
seaweed
was
"φῦκος"
(fūkos
or
phykos),
which
could
mean
either
the
seaweed,
probably
Red
Algae,
or
a
red
dye
derived
from
it.
The
Latinization,
"fūcus",
meant
primarily
the
cosmetic
rouge.
The
etymology
is
uncertain,
but
a
strong
candidate
has
long
been
some
word
related
to
the
Biblical
"פוך"
(pūk),
"paint"
(if
not
that
word
itself),
a
cosmetic
eye-shadow
used
by
the
ancient
Egyptians
and
other
inhabitants
of
the
eastern
Mediterranean.
It
could
be
any
color:
black,
red,
green,
blue.
Accordingly
the
modern
study
of
marine
and
freshwater
algae
is
called
either
phycology
or
algology.
The
name
Fucus
appears
in
a
number
of
taxa.
Classification.
While
"Cyanobacteria"
have
been
traditionally
included
among
the
Algae,
recent
works
usually
exclude
them
due
to
large
differences
such
as
the
lack
of
membrane-bound
organelles,
the
presence
of
a
single
circular
chromosome,
the
presence
of
peptidoglycan
in
the
cell
walls,
and
ribosomes
different
in
size
and
content
from
those
of
the
Eukaryotes.
Rather
than
in
chloroplasts,
they
conduct
photosynthesis
on
specialized
infolded
cytoplasmic
membranes
called
thylakoid
membranes.
Therefore,
they
differ
significantly
from
the
Algae
despite
occupying
similar
ecological
niches.
By
modern
definitions
Algae
are
Eukaryotes
and
conduct
photosynthesis
within
membrane-bound
organelles
called
chloroplasts.
Chloroplasts
contain
circular
DNA
and
are
similar
in
structure
to
Cyanobacteria,
presumably
representing
reduced
cyanobacterial
endosymbionts.
The
exact
nature
of
the
chloroplasts
is
different
among
the
different
lines
of
Algae,
reflecting
different
endosymbiotic
events.
The
table
below
describes
the
composition
of
the
three
major
groups
of
Algae.
Their
lineage
relationships
are
shown
in
the
figure
in
the
upper
right.
Many
of
these
groups
contain
some
members
that
are
no
longer
photosynthetic.
Some
retain
plastids,
but
not
chloroplasts,
while
others
have
lost
plastids
entirely.
W.H.Harvey
(1811—1866)
was
the
first
to
divide
the
Algae
into
four
divisions
based
on
their
pigmentation.
This
is
the
first
use
of
a
biochemical
criterion
in
plant
systematics.
Harvey's
four
divisions
are:
Red
Algae
(Rhodophyta),
Brown
Algae
(Heteromontophyta),
Green
Algae
(Chlorophyta)
and
Diatomaceae.
Relationship
to
higher
plants.
The
first
plants
on
earth
evolved
from
shallow
freshwater
algae
much
like
"Chara"
some
400
million
years
ago.
These
probably
had
an
isomorphic
alternation
of
generations
and
were
probably
filamentous.
Fossils
of
isolated
land
plant
spores
suggest
land
plants
may
have
been
around
as
long
as
475
million
years
ago.
Morphology.
A
range
of
algal
morphologies
are
exhibited,
and
convergence
of
features
in
unrelated
groups
is
common.
The
only
groups
to
exhibit
three
dimensional
multicellular
thalli
are
the
reds
and
browns,
and
some
chlorophytes.
Apical
growth
is
constrained
to
subsets
of
these
groups:
the
florideophyte
reds,
various
browns,
and
the
charophytes.
The
form
of
charophytes
is
quite
different
to
those
of
reds
and
browns,
because
have
distinct
nodes,
separated
by
internode
'stems';
whorls
of
branches
reminiscent
of
the
horsetails
occur
at
the
nodes.
Conceptacles
are
another
polyphyletic
trait;
they
appear
in
the
coralline
algae
and
the
Hildenbrandiales,
as
well
as
the
browns.
Most
of
the
simpler
algae
are
unicellular
flagellates
or
amoeboids,
but
colonial
and
non-motile
forms
have
developed
independently
among
several
of
the
groups.
Some
of
the
more
common
organizational
levels,
more
than
one
of
which
may
occur
in
the
life
cycle
of
a
species,
are
In
three
lines
even
higher
levels
of
organization
have
been
reached,
with
full
tissue
differentiation.
These
are
the
brown
algae,—some
of
which
may
reach
50
m
in
length
(kelps)—the
red
algae,
and
the
green
algae.
The
most
complex
forms
are
found
among
the
green
algae
(see
Charales
and
Charophyta),
in
a
lineage
that
eventually
led
to
the
higher
land
plants.
The
point
where
these
non-algal
plants
begin
and
algae
stop
is
usually
taken
to
be
the
presence
of
reproductive
organs
with
protective
cell
layers,
a
characteristic
not
found
in
the
other
alga
groups.
Symbiotic
algae.
Some
species
of
algae
form
symbiotic
relationships
with
other
organisms.
In
these
symbioses,
the
algae
supply
photosynthates
(organic
substances)
to
the
host
organism
providing
protection
to
the
algal
cells.
The
host
organism
derives
some
or
all
of
its
energy
requirements
from
the
algae.
Examples
are
as
follows.
Lichens.
"Lichens"
are
defined
by
the
International
Association
for
Lichenology
to
be
"an
association
of
a
fungus
and
a
photosynthetic
symbiont
resulting
in
a
stable
vegetative
body
having
a
specific
structure."
The
fungi,
or
mycobionts,
are
from
the
Ascomycota
with
a
few
from
the
Basidiomycota.
They
are
not
found
alone
in
nature
but
when
they
began
to
associate
is
not
known.
One
mycobiont
associates
with
the
same
phycobiont
species,
rarely
two,
from
the
Green
Algae,
except
that
alternatively
the
mycobiont
may
associate
with
the
same
species
of
Cyanobacteria
(hence
"photobiont"
is
the
more
accurate
term).
A
photobiont
may
be
associated
with
many
specific
mycobionts
or
live
independently;
accordingly,
lichens
are
named
and
classified
as
fungal
species.
The
association
is
termed
a
morphogenesis
because
the
lichen
has
a
form
and
capabilities
not
possessed
by
the
symbiont
species
alone
(they
can
be
experimentally
isolated).
It
is
possible
that
the
photobiont
triggers
otherwise
latent
genes
in
the
mycobiont.
Coral
reefs.
Coral
reefs
are
accumulated
from
the
calcareous
exoskeletons
of
marine
invertebrates
of
the
Scleractinia
order;
i.e.,
the
Stony
Corals.
As
animals
they
metabolize
sugar
and
oxygen
to
obtain
energy
for
their
cell-building
processes,
including
secretion
of
the
exoskeleton,
with
water
and
carbon
dioxide
as
byproducts.
As
the
reef
is
the
result
of
a
favorable
equilibrium
between
construction
by
the
corals
and
destruction
by
marine
erosion,
the
rate
at
which
metabolism
can
proceed
determines
the
growth
or
deterioration
of
the
reef.
Algae
of
the
Dinoflagellate
phylum
are
often
endosymbionts
in
the
cells
of
marine
invertebrates,
where
they
accelerate
host-cell
metabolism
by
generating
immediately
available
sugar
and
oxygen
through
photosynthesis
using
incident
light
and
the
carbon
dioxide
produced
in
the
host.
Endosymbiont
algae
in
the
Stony
Corals
are
described
by
the
term
zooxanthellae,
with
the
host
Stony
Corals
called
on
that
account
hermatypic
corals,
which
although
not
a
taxon
are
not
in
healthy
condition
without
their
endosymbionts.
Zooxanthellae
belong
almost
entirely
to
the
genus
"Symbiodinium".
The
loss
of
"Symbiodinium"
from
the
host
is
known
as
coral
bleaching,
a
condition
which
unless
corrected
leads
to
the
deterioration
and
loss
of
the
reef.
Sea
sponges.
Green
Algae
live
close
to
the
surface
of
some
sponges,
for
example,
breadcrumb
sponge
("Halichondria
panicea").
The
alga
is
thus
protected
from
predators;
the
sponge
is
provided
with
oxygen
and
sugars
which
can
account
for
50
to
80%
of
sponge
growth
in
some
species.
Life-cycle.
Rhodophyta,
Chlorophyta
and
Heterokontophyta,
the
three
main
algal
Phyla,
have
life-cycles
which
show
tremendous
variation
with
considerable
complexity.
In
general
there
is
an
asexual
phase
where
the
seaweed's
cells
are
diploid,
a
sexual
phase
where
the
cells
are
haploid
followed
by
fusion
of
the
male
and
female
gametes.
Asexual
reproduction
is
advantageous
in
that
it
permits
efficient
population
increases,
but
less
variation
is
possible.
Sexual
reproduction
allows
more
variation,
but
is
more
costly.
Often
there
is
no
strict
alternation
between
the
sporophyte
and
also
because
there
is
often
an
asexual
phase,
which
could
include
the
fragmentation
of
the
thallus.
Numbers.
The
"Algal
Collection
of
the
U.S.
National
Herbarium"
(located
in
the
National
Museum
of
Natural
History)
consists
of
approximately
320500
dried
specimens,
which,
although
not
exhaustive
(no
exhaustive
collection
exists),
gives
an
idea
of
the
order
of
magnitude
of
the
number
of
algal
species
(that
number
remains
unknown).
Estimates
vary
widely.
For
example,
according
to
one
standard
textbook,
in
the
British
Isles
the
"UK
Biodiversity
Steering
Group
Report"
estimated
there
to
be
20000
algal
species
in
the
UK.
Another
checklist
reports
only
about
5000
species.
Regarding
the
difference
of
about
15000
species,
the
text
concludes:
"It
will
require
many
detailed
field
surveys
before
it
is
possible
to
provide
a
reliable
estimate
of
the
total
number
of
species..."
Regional
and
group
estimates
have
been
made
as
well:
5000—5500
species
of
Red
Algae
worldwide,
"some
1300
in
Australian
Seas,"
400
seaweed
species
for
the
western
coastline
of
South
Africa,
669
marine
species
from
California
(U.S.A.),
642
in
the
check-list
of
Britain
and
Ireland,
and
so
on,
but
lacking
any
scientific
basis
or
reliable
sources,
these
numbers
have
no
more
credibility
than
the
British
ones
mentioned
above.
Most
estimates
also
omit
the
microscopic
Algae,
such
as
the
phytoplankta,
entirely.
Distribution.
The
topic
of
distribution
of
algal
species
has
been
fairly
well
studied
since
the
founding
of
phytogeography
in
the
mid-19th
century
AD.
Algae
spread
mainly
by
the
dispersal
of
spores
analogously
to
the
dispersal
of
Plantae
by
seeds
and
spores.
Spores
are
everywhere
in
all
parts
of
the
Earth:
the
waters
fresh
and
marine,
the
atmosphere,
free-floating
and
in
precipitation
or
mixed
with
dust,
the
humus
and
in
other
organisms,
such
as
humans.
Whether
a
spore
is
to
grow
into
an
organism
depends
on
the
combination
of
the
species
and
the
environmental
conditions.
The
spores
of
fresh-water
Algae
are
dispersed
mainly
by
running
water
and
wind,
as
well
as
by
living
carriers.
The
bodies
of
water
into
which
they
are
transported
are
chemically
selective.
Marine
spores
are
spread
by
currents.
Ocean
water
is
temperature
selective,
resulting
in
phytogeographic
zones,
regions
and
provinces.
To
some
degree
the
distribution
of
Algae
is
subject
to
floristic
discontinuities
caused
by
geographical
features,
such
as
Antarctica,
long
distances
of
ocean
or
general
land
masses.
It
is
therefore
possible
to
identify
species
occurring
by
locality,
such
as
"Pacific
Algae"
or
"North
Sea
Algae".
When
they
occur
out
of
their
localities,
it
is
usually
possible
to
hypothesize
a
transport
mechanism,
such
as
the
hulls
of
ships.
For
example,
"Ulva
reticulata"
and
"Ulva
fasciata"
travelled
from
the
mainland
to
Hawaii
in
this
manner.
Mapping
is
possible
for
select
species
only:
"there
are
many
valid
examples
of
confined
distribution
patterns."
For
example,
"Clathromorphum"
is
an
arctic
genus
and
is
not
mapped
far
south
of
there.
On
the
other
hand,
scientists
regard
the
overall
data
as
insufficient
due
to
the
"difficulties
of
undertaking
such
studies."
Locations.
Algae
are
prominent
in
bodies
of
water,
common
in
terrestrial
environments
and
are
found
in
unusual
environments,
such
as
on
snow
and
on
ice.
Seaweeds
grow
mostly
in
shallow
marine
waters,
under;
however
some
have
been
recorded
to
a
depth
of
The
various
sorts
of
algae
play
significant
roles
in
aquatic
ecology.
Microscopic
forms
that
live
suspended
in
the
water
column
(phytoplankton)
provide
the
food
base
for
most
marine
food
chains.
In
very
high
densities
(algal
blooms)
these
algae
may
discolor
the
water
and
outcompete,
poison,
or
asphyxiate
other
life
forms.
Algae
are
variously
sensitive
to
different
factors,
which
has
made
them
useful
as
biological
indicators
in
the
Ballantine
Scale
and
its
modification.
Agar.
Agar,
an
Algae
derivative,
has
a
number
of
commercial
uses.
Alginates.
Between
100,000
and
170,000
wet
tons
of
"Macrocystis"
are
harvested
annually
in
California
for
alginate
extraction
and
abalone
feed.
Energy
source.
To
be
competitive
and
independent
from
fluctuating
support
from
(local)
policy
on
the
long
run,
biofuels
should
equal
or
beat
the
cost
level
of
fossil
fuels.
Here,
algae
based
fuels
hold
great
promise,
directly
related
to
the
potential
to
produce
more
biomass
per
unit
area
in
a
year
than
any
other
form
of
biomass.
The
break-even
point
for
algae-based
biofuels
should
be
within
reach
in
about
ten
years.
Fertilizer.
For
centuries
seaweed
has
been
used
as
a
fertilizer;
George
Owen
of
Henllys
writing
in
the
16th
century
referring
to
drift
weed
in
South
Wales:This
kind
of
ore
they
often
gather
and
lay
on
great
heapes,
where
it
heteth
and
rotteth,
and
will
have
a
strong
and
loathsome
smell;
when
being
so
rotten
they
cast
on
the
land,
as
they
do
their
muck,
and
thereof
springeth
good
corn,
especially
barley...
After
spring-tydes
or
great
rigs
of
the
sea,
they
fetch
it
in
sacks
on
horse
backes,
and
carie
the
same
three,
four,
or
five
miles,
and
cast
it
on
the
lande,
which
doth
very
much
better
the
ground
for
corn
and
grass.
Today
Algae
are
used
by
humans
in
many
ways;
for
example,
as
fertilizers,
soil
conditioners
and
livestock
feed.
Aquatic
and
microscopic
species
are
cultured
in
clear
tanks
or
ponds
and
are
either
harvested
or
used
to
treat
effluents
pumped
through
the
ponds.
Algaculture
on
a
large
scale
is
an
important
type
of
aquaculture
in
some
places.
Maerl
is
commonly
used
as
a
soil
conditioner.
Nutrition.
Naturally
growing
seaweeds
are
an
important
source
of
food,
especially
in
Asia.
They
provide
many
vitamins
including:
A,
B1,
B2,
B6,
niacin
and
C,
and
are
rich
in
iodine,
potassium,
iron,
magnesium
and
calcium.
In
addition
commercially
cultivated
microalgae,
including
both
Algae
and
Cyanobacteria,
are
marketed
as
nutritional
supplements,
such
as
Spirulina,
Chlorella
and
the
Vitamin-C
supplement,
Dunaliella,
high
in
beta-carotene.
Algae
are
national
foods
of
many
nations:
China
consumes
more
than
70
species,
including
"fat
choy",
a
cyanobacterium
considered
a
vegetable;
Japan,
over
20
species;
Ireland,
dulse;
Chile,
cochayuyo.
Laver
is
used
to
make
"laver
bread"
in
Wales
where
it
is
known
as
"bara
lawr";
in
Korea,
gim;
in
Japan,
nori
and
aonori.
It
is
also
used
along
the
west
coast
of
North
America
from
California
to
British
Columbia,
in
Hawaii
and
by
the
Māori
of
New
Zealand.
Sea
lettuce
and
badderlocks
are
a
salad
ingredient
in
Scotland,
Ireland,
Greenland
and
Iceland.
The
oils
from
some
Algae
have
high
levels
of
unsaturated
fatty
acids.
For
example,
"Parietochloris
incisa"
is
very
high
in
arachidonic
acid,
where
it
reaches
up
to
47%
of
the
triglyceride
pool.
Some
varieties
of
Algae
favored
by
vegetarianism
and
veganism
contain
the
long-chain,
essential
omega-3
fatty
acids,
Docosahexaenoic
acid
(DHA)
and
Eicosapentaenoic
acid
(EPA),
in
addition
to
vitamin
B12.
The
vitamin
B12
in
algae
is
not
biologically
active.
Fish
oil
contains
the
omega-3
fatty
acids,
but
the
original
source
is
algae
(microalgae
in
particular),
which
are
eaten
by
marine
life
such
as
copepods
and
are
passed
up
the
food
chain.
Algae
has
emerged
in
recent
years
as
a
popular
source
of
omega-3
fatty
acids
for
vegetarians
who
cannot
get
long-chain
EPA
and
DHA
from
other
vegetarian
sources
such
as
flaxseed
oil,
which
only
contains
the
short-chain
Alpha-Linolenic
acid
(ALA).
Pigments.
The
natural
pigments
produced
by
algae
can
be
used
as
an
alternative
to
chemical
dyes
and
coloring
agents.
Stabilizing
substances.
Carrageenan,
from
the
red
alga
"Chondrus
crispus",
is
used
as
a
stabiliser
in
milk
products.
---END.OF.DOCUMENT---
Analysis
of
variance.
In
statistics,
analysis
of
variance
(ANOVA)
is
a
collection
of
statistical
models,
and
their
associated
procedures,
in
which
the
observed
variance
is
partitioned
into
components
due
to
different
explanatory
variables.
In
its
simplest
form
ANOVA
gives
a
statistical
test
of
whether
the
means
of
several
groups
are
all
equal,
and
therefore
generalizes
Student's
two-sample
"t"-test
to
more
than
two
groups.
ANOVAs
are
helpful
because
they
possess
a
certain
advantage
over
a
two-sample
t-test.
Doing
multiple
two-sample
t-tests
would
result
in
a
largely
increased
chance
of
committing
a
type
I
error.
For
this
reason,
ANOVAs
are
useful
in
comparing
three
or
more
means.
Fixed-effects
models
(Model
1).
The
fixed-effects
model
of
analysis
of
variance
applies
to
situations
in
which
the
experimenter
applies
several
treatments
to
the
subjects
of
the
experiment
to
see
if
the
response
variable
values
change.
This
allows
the
experimenter
to
estimate
the
ranges
of
response
variable
values
that
the
treatment
would
generate
in
the
population
as
a
whole.
Random-effects
models
(Model
2).
Random
effects
models
are
used
when
the
treatments
are
not
fixed.
This
occurs
when
the
various
treatments
(also
known
as
factor
levels)
are
sampled
from
a
larger
population.
Because
the
treatments
themselves
are
random
variables,
some
assumptions
and
the
method
of
contrasting
the
treatments
differ
from
ANOVA
model
1.
Most
random-effects
or
mixed-effects
models
are
not
concerned
with
making
inferences
concerning
the
particular
sampled
factors.
For
example,
consider
a
large
manufacturing
plant
in
which
many
machines
produce
the
same
product.
The
statistician
studying
this
plant
would
have
very
little
interest
in
comparing
the
three
particular
machines
to
each
other.
Rather,
inferences
that
can
be
made
for
"all"
machines
are
of
interest,
such
as
their
variability
and
the
mean.
Assumptions
of
anova.
There
are
several
approaches
to
the
analysis
of
variance.
A
model
often
presented
in
textbooks.
Levene's
test
for
homogeneity
of
variances
is
typically
used
to
examine
the
plausibility
of
homoscedasticity.
The
Kolmogorov–Smirnov
or
the
Shapiro–Wilk
test
may
be
used
to
examine
normality.
When
used
in
the
analysis
of
variance
to
test
the
hypothesis
that
all
treatments
have
exactly
the
same
effect,
the
F-test
is
robust
(Ferguson
&
Takane,
2005,
pp. 261–2).
The
Kruskal–Wallis
test
is
a
nonparametric
alternative
which
does
not
rely
on
an
assumption
of
normality.
And
the
Friedman
test
is
the
nonparametric
alternative
for
a
one
way
repeated
measures
ANOVA.
The
separate
assumptions
of
the
textbook
model
imply
that
the
errors
are
independently,
identically,
and
normally
distributed
for
fixed
effects
models,
that
is,
that
the
errors
are
independent
and
Randomization-based
analysis.
In
a
randomized
controlled
experiment,
the
treatments
are
randomly
assigned
to
experimental
units,
following
the
experimental
protocol.
This
randomization
is
objective
and
declared
before
the
experiment
is
carried
out.
The
objective
random-assignment
is
used
to
test
the
significance
of
the
null
hypothesis,
following
the
ideas
of
C.
S.
Peirce
and
Ronald
A.
Fisher.
This
design-based
analysis
was
advocated
and
developed
by
Oscar
Kempthorne
at
Iowa
State
University.
Kempthorne
and
his
students
make
an
assumption
of
"unit
treatment
additivity",
which
is
discussed
in
the
books
of
Kempthorne
and
David
R.
Cox.
Unit
treatment
additivity.
In
its
simplest
form,
the
assumption
of
treatment
unit
additivity
states
that
the
observed
response
formula_2
from
experimental
unit
formula_3
when
receiving
treatment
formula_4
can
be
written
as
the
sum
formula_5.
The
assumption
of
unit
treatment
addivity
implies
that
every
treatment
have
exactly
the
same
effect
on
every
experiment
unit.
The
assumption
of
unit
treatment
additivity
is
a
hypothesis
which
is
not
directly
falsifiable,
according
to
Cox
and
Kempthorne.
However,
many
consequences
of
treatment-unit
additivity
can
be
falsified.
For
a
randomized
experiment,
the
assumption
of
treatment
additivity
implies
that
the
variance
is
constant
for
all
treatments.
Therefore,
by
contraposition,
a
necessary
condition
for
unit
treatment
additivity
is
that
the
variance
is
constant.
The
property
of
unit
treatment
additivity
is
not
invariant
under
a
change
of
scale,
so
statisticians
often
use
transformations
to
achieve
unit
treatment
additivity.
If
the
response
variable
is
expected
to
follow
a
parametric
family
of
probability
distributions,
then
the
statistician
may
specify
(in
the
protocol
for
the
experiment
or
observational
study)
that
the
responses
be
tranformed
to
stabilize
the
variance.
Also,
a
statistician
may
specify
that
logarithmic
transforms
be
applied
to
the
responses,
which
are
believed
to
follow
a
multiplicative
model.
The
assumption
of
unit
treatment
additivity
was
enunciated
in
experimental
design
by
Kempthorne
and
Cox.
Kempthorne's
use
of
unit
treatment
additivity
and
randomization
is
similar
to
the
design-based
analysis
of
finite
population
survey
sampling.
Derived
linear
model.
Kempthorne
uses
the
randomization-distribution
and
the
assumption
of
"unit
treatment
additivity"
to
produce
a
"derived
linear
model",
very
similar
to
the
textbook
model
discussed
previously.
The
test
statistics
of
this
derived
linear
model
are
closely
approximated
by
the
test
statistics
of
an
appropriate
normal
linear
model,
according
to
approximation
theorems
and
simulation
studies
by
Kempthorne
and
his
students
(Hinkelmann
and
Kempthorne).
However,
there
are
differences.
For
example,
the
randomization-based
analysis
results
in
a
small
but
(strictly)
negative
correlation
between
the
observations
(Hinkelmann
and
Kempthorne,
volume
one,
chapter
7;
Bailey
chapter
1.14).
In
the
randomization-based
analysis,
there
is
"no
assumption"
of
a
"normal"
distribution
and
certainly
"no
assumption"
of
"independence".
On
the
contrary,
"the
observations
are
dependent"!
The
randomization-based
analysis
has
the
disadvantage
that
its
exposition
involves
tedious
algebra
and
extensive
time.
Since
the
randomization-based
analysis
is
complicated
and
is
closely
approximated
by
the
approach
using
a
normal
linear
model,
most
teachers
emphasize
the
normal
linear
model
approach.
Few
statisticians
object
to
model-based
analysis
of
balanced
randomized
experiments.
Statistical
models
for
observational
data.
However,
when
applied
to
data
from
non-randomized
experiments
or
observational
studies,
model-based
analysis
lacks
the
warrant
of
randomization.
For
observational
data,
the
derivation
of
confidence
intervals
must
use
"subjective"
models,
as
emphasized
by
Ronald
A.
Fisher
and
his
followers.
In
practice,
the
estimates
of
treatment-effects
from
observational
studies
generally
are
often
inconsistent
(Freedman).
In
practice,
"statistical
models"
and
observational
data
are
useful
for
suggesting
hypothesis
that
should
be
treated
very
cautiously
by
the
public
(Freedman).
Partitioning
of
the
sum
of
squares.
The
fundamental
technique
is
a
partitioning
of
the
total
sum
of
squares
(abbreviated
SS)
into
components
related
to
the
effects
used
in
the
model.
For
example,
we
show
the
model
for
a
simplified
ANOVA
with
one
type
of
treatment
at
different
levels.
So,
the
number
of
degrees
of
freedom
(abbreviated
df)
can
be
partitioned
in
a
similar
way
and
specifies
the
chi-square
distribution
which
describes
the
associated
sums
of
squares.
See
also
Lack-of-fit
sum
of
squares.
The
F-test.
The
F-test
is
used
for
comparisons
of
the
components
of
the
total
deviation.
For
example,
in
one-way,
or
single-factor
ANOVA,
statistical
significance
is
tested
for
by
comparing
the
F
test
statistic
to
the
F-distribution
with
"I" − 1,"n"T − "I"
degrees
of
freedom.
Using
the
F-distribution
is
a
natural
candidate
because
the
test
statistic
is
the
quotient
of
two
mean
sums
of
squares
which
have
a
chi-square
distribution.
ANOVA
on
ranks.
When
the
data
do
not
meet
the
assumptions
of
normality,
the
suggestion
has
arisen
to
replace
each
original
data
value
by
its
rank
(from
1
for
the
smallest
to
N
for
the
largest),
then
run
a
standard
ANOVA
calculation
on
the
rank-transformed
data.
Conover
and
Iman
(1981)
provided
a
review
of
the
four
main
types
of
rank
transformations.
Commercial
statistical
software
packages
(e.g.,
SAS,
1985,
1987,
2008)
followed
with
recommendations
to
data
analysts
to
run
their
data
sets
through
a
ranking
procedure
(e.g.,
PROC
RANK)
prior
to
conducting
standard
analyses
using
parametric
procedures.
This
rank-based
procedure
has
been
recommended
as
being
robust
to
non-normal
errors,
resistant
to
outliers,
and
highly
efficient
for
many
distributions.
It
may
result
in
a
known
statistic
(e.g.,
Wilcoxon
Rank-Sum
/
Mann-Whitney
U),
and
indeed
provide
the
desired
robustness
and
increased
statistical
power
that
is
sought.
For
example,
Monte
Carlo
studies
have
shown
that
the
rank
transformation
in
the
two
independent
samples
t
test
layout
can
be
successfully
extended
to
the
one-way
independent
samples
ANOVA,
as
well
as
the
two
independent
samples
multivariate
Hotelling's
T2
layouts
(Nanna,
2002).
Conducting
factorial
ANOVA
on
the
ranks
of
original
scores
has
also
been
suggested
(Conover
&
Iman,
1976,
Iman,
1974,
and
Iman
&
Conover,
1976).
However,
Monte
Carlo
studies
by
Sawilowsky
(1985a;
1989
et
al.;
1990)
and
Blair,
Sawilowsky,
and
Higgins
(1987),
and
subsequent
asymptotic
studies
(e.g.
Thompson
&
Ammann,
1989;
"there
exist
values
for
the
main
effects
such
that,
under
the
null
hypothesis
of
no
interaction,
the
expected
value
of
the
rank
transform
test
statistic
goes
to
infinity
as
the
sample
size
increases,"
Thompson,
1991,
p.
697),
found
that
the
rank
transformation
is
inappropriate
for
testing
interaction
effects
in
a
4x3
and
a
2x2x2
factorial
design.
As
the
number
of
effects
(i.e.,
main,
interaction)
become
non-null,
and
as
the
magnitude
of
the
non-null
effects
increase,
there
is
an
increase
in
Type
I
error,
resulting
in
a
complete
failure
of
the
statistic
with
as
high
as
a
100%
probability
of
making
a
false
positive
decision.
Similarly,
Blair
and
Higgins
(1985)
found
that
the
rank
transformation
increasingly
fails
in
the
two
dependent
samples
layout
as
the
correlation
between
pretest
and
posttest
scores
increase.
Headrick
(1997)
discovered
the
Type
I
error
rate
problem
was
exacerbated
in
the
context
of
Analysis
of
Covariance,
particularly
as
the
correlation
between
the
covariate
and
the
dependent
variable
increased.
For
a
review
of
the
properties
of
the
rank
transformation
in
designed
experiments
see
Sawilowsky
(2000).
A
variant
of
rank-transformation
is
'quantile
normalization'
in
which
a
further
transformation
is
applied
to
the
ranks
such
that
the
resulting
values
have
some
defined
distribution
(often
a
normal
distribution
with
a
specified
mean
and
variance).
Further
analyses
of
quantile-normalized
data
may
then
assume
that
distribution
to
compute
significance
values.
However,
two
specific
types
of
secondary
transformations,
the
random
normal
scores
and
expected
normal
scores
transformation,
have
been
shown
to
greatly
inflate
Type
I
errors
and
severely
reduce
statistical
power
(Sawilowsky,
1985a,
1985b).
Effect
size
measures.
Several
standardized
measures
of
effect
are
used
within
the
context
of
ANOVA
to
describe
the
degree
of
relationship
between
a
predictor
or
set
of
predictors
and
the
dependent
variable.
Effect
size
estimates
are
reported
to
allow
researchers
to
compare
findings
in
studies
and
across
disciplines.
Common
effect
size
estimates
reported
in
bivariate
(e.g.
ANOVA)
and
multivariate
(MANOVA,
ANCOVA,
Multiple
Discriminant
Analysis)
statistical
analysis
includes
eta-squared,
partial
eta-squared,
omega,
and
intercorrelation
(Strang,
2009).
Eta-squared
describes
the
ratio
of
variance
explained
in
the
dependent
variable
by
a
predictor
while
controlling
for
other
predictors.
Eta-squared
is
a
biased
estimator
of
the
variance
explained
by
the
model
in
the
population
(it
only
estimates
effect
size
in
the
sample).
On
average
it
overestimates
the
variance
explained
in
the
population.
As
the
sample
size
gets
larger
the
amount
of
bias
gets
smaller.
It
is,
however,
an
easily
calculated
estimator
of
the
proportion
of
the
variance
in
a
population
explained
by
the
treatment.
Note
that
earlier
versions
of
statistical
software
(such
as
SPSS)
incorrectly
reports
Partial
eta
squared
under
the
misleading
title
"Eta
squared".
Partial
eta-squared
describes
the
"proportion
of
total
variation
attributable
to
the
factor,
partialling
out
(excluding)
other
factors
from
the
total
nonerror
variation"
(Pierce,
Block
&
Aguinis,
2004,
p. 918).
Partial
eta
squared
is
normally
higher
than
eta
squared
(except
in
simple
one-factor
models).
The
generally
accepted
regression
benchmark
for
effect
size
comes
from
(Cohen,
1992;
1988):
0.20
is
a
minimal
solution
(but
significant
in
social
science
research);
0.50
is
a
medium
effect;
anything
equal
to
or
greater
than
0.80
is
a
large
effect
size
(Keppel
&
Wickens,
2004;
Cohen,
1992).
Because
this
common
interpretation
of
effect
size
has
been
repeated
from
Cohen
(1988)
over
the
years
with
no
change
or
comment
to
validity
for
contemporary
experimental
research,
it
is
questionable
outside
of
psychological/behavioural
studies,
and
more
so
questionable
even
then
without
a
full
understanding
of
the
limitations
ascribed
by
Cohen.
Note:
The
use
of
specific
partial
eta-square
values
for
large
medium
or
small
as
a
"rule
of
thumb"
should
be
avoided.
Nevertheless,
alternative
rules
of
thumb
have
emerged
in
certain
disciplines:
Small
=
0.01;
medium
=
0.06;
large
=
0.14
(Kittler,
Menard
&
Phillips,
2007).
This
measure
of
effect
size
is
frequently
encountered
when
performing
power
analysis
calculations.
Conceptually
it
represents
the
square
root
of
variance
explained
over
variance
not
explained.
Follow
up
tests.
A
statistically
significant
effect
in
ANOVA
is
often
followed
up
with
one
or
more
different
follow-up
tests.
This
can
be
done
in
order
to
assess
which
groups
are
different
from
which
other
groups
or
to
test
various
other
focused
hypotheses.
Follow
up
tests
are
often
distinguished
in
terms
of
whether
they
are
planned
(a
priori)
or
post
hoc.
Planned
tests
are
determined
before
looking
at
the
data
and
post
hoc
tests
are
performed
after
looking
at
the
data.
Post
hoc
tests
such
as
Tukey's
range
test
most
commonly
compare
every
group
mean
with
every
other
group
mean
and
typically
incorporate
some
method
of
controlling
for
Type
I
errors.
Comparisons,
which
are
most
commonly
planned,
can
be
either
simple
or
compound.
Simple
comparisons
compare
one
group
mean
with
one
other
group
mean.
Compound
comparisons
typically
compare
two
sets
of
groups
means
where
one
set
has
at
two
or
more
groups
(e.g.,
compare
average
group
means
of
group
A,
B
and
C
with
group
D).
Comparisons
can
also
look
at
tests
of
trend,
such
as
linear
and
quadratic
relationships,
when
the
independent
variable
involves
ordered
levels.
Power
analysis.
Power
analysis
is
often
applied
in
the
context
of
ANOVA
in
order
to
assess
the
probability
of
successfully
rejecting
the
null
hypothesis
if
we
assume
a
certain
ANOVA
design,
effect
size
in
the
population,
sample
size
and
alpha
level.
Power
analysis
can
assist
in
study
design
by
determining
what
sample
size
would
be
required
in
order
to
have
a
reasonable
chance
of
rejecting
the
null
hypothesis.
Examples.
In
a
first
experiment,
Group
A
is
given
vodka,
Group
B
is
given
gin,
and
Group
C
is
given
a
placebo.
All
groups
are
then
tested
with
a
memory
task.
A
one-way
ANOVA
can
be
used
to
assess
the
effect
of
the
various
treatments
(that
is,
the
vodka,
gin,
and
placebo).
In
a
second
experiment,
Group
A
is
given
vodka
and
tested
on
a
memory
task.
The
same
group
is
allowed
a
rest
period
of
five
days
and
then
the
experiment
is
repeated
with
gin.
The
procedure
is
repeated
using
a
placebo.
A
one-way
ANOVA
with
repeated
measures
can
be
used
to
assess
the
effect
of
the
vodka
versus
the
impact
of
the
placebo.
Each
group
is
then
tested
on
a
memory
task.
The
advantage
of
this
design
is
that
multiple
variables
can
be
tested
at
the
same
time
instead
of
running
two
different
experiments.
Also,
the
experiment
can
determine
whether
one
variable
affects
the
other
variable
(known
as
interaction
effects).
A
factorial
ANOVA
(2×2)
can
be
used
to
assess
the
effect
of
expecting
vodka
or
the
placebo
and
the
actual
reception
of
either.
History.
The
analysis
of
variance
was
used
informally
by
researchers
in
the
1800s
using
least
squares.
In
physics
and
psychology,
researchers
included
a
term
for
the
operator-effect,
the
influence
of
a
particular
person
on
measurements,
according
to
Stephen
Stigler's
histories.
In
its
modern
form,
the
analysis
of
variance
was
one
of
the
many
important
statistical
innovations
of
Ronald
A.
Fisher.
Fisher
proposed
a
formal
analysis
of
variance
in
his
1918
paper
The
Correlation
Between
Relatives
on
the
Supposition
of
Mendelian
Inheritance.
His
first
application
of
the
analysis
of
variance
was
published
in
1921.
Analysis
of
variance
became
widely
known
after
being
included
in
Fisher's
1925
book
"Statistical
Methods
for
Research
Workers".
---END.OF.DOCUMENT---
Alkane.
Alkanes,
also
known
as
paraffins,
are
chemical
compounds
that
consist
only
of
the
elements
carbon
(C)
and
hydrogen
(H)
(i.e.,
hydrocarbons),
wherein
these
atoms
are
linked
together
exclusively
by
single
bonds
(i.e.,
they
are
saturated
compounds)
without
any
cyclic
structure
(i.e.
loops).
Alkanes
belong
to
a
homologous
series
of
organic
compounds
in
which
the
members
differ
by
a
constant
relative
molecular
mass
of
14.
Each
carbon
atom
must
have
4
bonds
(either
C-H
or
C-C
bonds),
and
each
hydrogen
atom
must
be
joined
to
a
carbon
atom
(H-C
bonds).
A
series
of
linked
carbon
atoms
is
known
as
the
carbon
skeleton
or
carbon
backbone.
In
general,
the
number
of
carbon
atoms
is
often
used
to
define
the
size
of
the
alkane
(e.g.,
C2-alkane).
An
alkyl
group
is
a
functional
group
or
side-chain
that,
like
an
alkane,
consists
solely
of
singly-bonded
carbon
and
hydrogen
atoms,
for
example
a
methyl
or
ethyl
group.
Saturated
hydrocarbons
can
be
linear
(general
formula)
wherein
the
carbon
atoms
are
joined
in
a
snake-like
structure,
branched
(general
formula,
"n"
>
3)
wherein
the
carbon
backbone
splits
off
in
one
or
more
directions,
or
cyclic
(general
formula,
"n"
>
2)
wherein
the
carbon
backbone
is
linked
so
as
to
form
a
loop.
According
to
the
definition
by
IUPAC,
the
former
two
are
alkanes,
whereas
the
third
group
is
called
cycloalkanes.
Saturated
hydrocarbons
can
also
combine
any
of
the
linear,
cyclic
(e.g.,
polycyclic)
and
branching
structures,
and
they
are
still
alkanes
(no
general
formula)
as
long
as
they
are
acyclic
(i.e.,
having
no
loops).
The
simplest
possible
alkane
(the
parent
molecule)
is
methane,
CH4.
There
is
no
limit
to
the
number
of
carbon
atoms
that
can
be
linked
together,
the
only
limitation
being
that
the
molecule
is
acyclic,
is
saturated,
and
is
a
hydrocarbon.
Saturated
oils
and
waxes
are
examples
of
larger
alkanes
where
the
number
of
carbons
in
the
carbon
backbone
tends
to
be
greater
than
10.
Alkanes
are
not
very
reactive
and
have
little
biological
activity.
Alkanes
can
be
viewed
as
a
molecular
tree
upon
which
can
be
hung
the
interesting
biologically
active/reactive
portions
(functional
groups)
of
the
molecule.
Isomerism.
butane
is
the
only
C4H6
compound
and
has
no
isomer;
tetrahedrane
(not
shown)
is
the
only
C4H4
compound
and
has
also
no
isomer.
Branched
alkanes
can
be
chiral:
3-methylhexane
and
its
higher
homologues
are
chiral
due
to
their
stereogenic
center
at
carbon
atom
number
3.
Chiral
alkanes
are
of
certain
importance
in
biochemistry,
as
they
occur
as
sidechains
in
chlorophyll
and
tocopherol
(vitamin
E).
Chiral
alkanes
can
be
resolved
into
their
enantiomers
by
enantioselective
chromatography.
In
addition
to
these
isomers,
the
chain
of
carbon
atoms
may
form
one
or
more
loops.
Such
compounds
are
called
cycloalkanes.
Nomenclature.
The
IUPAC
nomenclature
(systematic
way
of
naming
compounds)
for
alkanes
is
based
on
identifying
hydrocarbon
chains.
Unbranched,
saturated
hydrocarbon
chains
are
named
systematically
with
a
Greek
numerical
prefix
denoting
the
number
of
carbons
and
the
suffix
"-ane".
August
Wilhelm
von
Hofmann
suggested
systematizing
nomenclature
by
using
the
whole
sequence
of
vowels
a,
e,
i,
o
and
u
to
create
suffixes
-ane,
-ene,
-ine
(or
-yne),
-one,
-une,
for
the
hydrocarbons.
The
first
three
name
hydrocarbons
with
single,
double
and
triple
bonds;
"-one"
represents
a
ketone;
"-ol"
represents
an
alcohol
or
OH
group;
"-oxy-"
means
an
ether
and
refers
to
oxygen
between
two
carbons,
so
that
methoxy-methane
is
the
IUPAC
name
for
dimethyl
ether.
It
is
difficult
or
impossible
to
find
compounds
with
more
than
one
IUPAC
name.
This
is
because
shorter
chains
attached
to
longer
chains
are
prefixes
and
the
convention
includes
brackets.
Numbers
in
the
name,
referring
to
which
carbon
a
group
is
attached
to,
should
be
as
low
as
possible,
so
that
1-
is
implied
and
usually
omitted
from
names
of
organic
compounds
with
only
one
side-group;
"1-"
is
implied
in
Nitro-octane.
Symmetric
compounds
will
have
two
ways
of
arriving
at
the
same
name.
Linear
alkanes.
Straight-chain
alkanes
are
sometimes
indicated
by
the
prefix
"n-"
(for
"normal")
where
a
non-linear
isomer
exists.
Although
this
is
not
strictly
necessary,
the
usage
is
still
common
in
cases
where
there
is
an
important
difference
in
properties
between
the
straight-chain
and
branched-chain
isomers,
e.g.,
"n"-hexane
or
2-
or
3-methylpentane.
These
names
were
derived
from
methanol,
ether,
propionic
acid
and
butyric
acid,
respectively.
Alkanes
with
five
or
more
carbon
atoms
are
named
by
adding
the
suffix
-ane
to
the
appropriate
numerical
multiplier
prefix
with
elision
of
any
terminal
vowel
("-a"
or
"-o")
from
the
basic
numerical
term.
Hence,
pentane,
C5H12;
hexane,
C6H14;
heptane,
C7H16;
octane,
C8H18;
etc.
The
prefix
is
generally
Greek,
with
the
exceptions
of
nonane
which
has
a
Latin
prefix,
and
undecane
and
tridecane
which
have
mixed-language
prefixes.
For
a
more
complete
list,
see
List
of
alkanes.
Branched
alkanes.
Simple
branched
alkanes
often
have
a
common
name
using
a
prefix
to
distinguish
them
from
linear
alkanes,
for
example
"n"-pentane,
isopentane,
and
neopentane.
IUPAC
naming
conventions
can
be
used
to
produce
a
systematic
name.
Cyclic
alkanes.
So-called
cyclic
alkanes
are,
in
the
technical
sense,
"not"
alkanes,
but
cycloalkanes.
They
are
hydrocarbons
just
like
alkanes,
but
contain
one
or
more
rings.
Simple
cycloalkanes
have
a
prefix
"cyclo-"
to
distinguish
them
from
alkanes.
Cycloalkanes
are
named
as
per
their
acyclic
counterparts
with
respect
to
the
number
of
carbon
atoms,
e.g.,
cyclopentane
(C5H10)
is
a
cycloalkane
with
5
carbon
atoms
just
like
pentane
(C5H12),
but
they
are
joined
up
in
a
five-membered
ring.
In
a
similar
manner,
propane
and
cyclopropane,
butane
and
cyclobutane,
etc.
Substituted
cycloalkanes
are
named
similar
to
substituted
alkanes
—
the
cycloalkane
ring
is
stated,
and
the
substituents
are
according
to
their
position
on
the
ring,
with
the
numbering
decided
by
Cahn-Ingold-Prelog
rules.
Trivial
names.
The
trivial
(non-systematic)
name
for
alkanes
is
"paraffins."
Together,
alkanes
are
known
as
the
"paraffin
series".
Trivial
names
for
compounds
are
usually
historical
artifacts.
They
were
coined
before
the
development
of
systematic
names,
and
have
been
retained
due
to
familiar
usage
in
industry.
Cycloalkanes
are
also
called
naphthenes.
It
is
almost
certain
that
the
term
paraffin
stems
from
the
petrochemical
industry.
Branched-chain
alkanes
are
called
"isoparaffins".
The
use
of
the
term
"paraffin"
is
a
general
term
and
often
does
not
distinguish
between
a
pure
compounds
and
mixtures
of
isomers
with
the
same
chemical
formula
(i.e.,
like
a
chemical
anagram),
e.g.,
pentane
and
isopentane.
Boiling
point.
Alkanes
experience
inter-molecular
van
der
Waals
forces.
Stronger
inter-molecular
van
der
Waals
forces
give
rise
to
greater
boiling
points
of
alkanes.
Under
standard
conditions,
from
CH4
to
C4H10
alkanes
are
gaseous;
from
C5H12
to
C17H36
they
are
liquids;
and
after
C18H38
they
are
solids.
As
the
boiling
point
of
alkanes
is
primarily
determined
by
weight,
it
should
not
be
a
surprise
that
the
boiling
point
has
almost
a
linear
relationship
with
the
size
(molecular
weight)
of
the
molecule.
As
a
rule
of
thumb,
the
boiling
point
rises
20
-
30
°C
for
each
carbon
added
to
the
chain;
this
rule
applies
to
other
homologous
series.
A
straight-chain
alkane
will
have
a
boiling
point
higher
than
a
branched-chain
alkane
due
to
the
greater
surface
area
in
contact,
thus
the
greater
van
der
Waals
forces,
between
adjacent
molecules.
For
example,
compare
isobutane
(2-methylpropane)
and
n-butane
(butane),
which
boil
at
-12
and
0
°C,
and
2,2-dimethylbutane
and
2,3-dimethylbutane
which
boil
at
50
and
58
°C,
respectively.
For
the
latter
case,
two
molecules
2,3-dimethylbutane
can
"lock"
into
each
other
better
than
the
cross-shaped
2,2-dimethylbutane,
hence
the
greater
van
der
Waals
forces.
On
the
other
hand,
cycloalkanes
tend
to
have
higher
boiling
points
than
their
linear
counterparts
due
to
the
locked
conformations
of
the
molecules,
which
give
a
plane
of
intermolecular
contact.
Melting
point.
The
melting
points
of
the
alkanes
follow
a
similar
trend
to
boiling
points
for
the
same
reason
as
outlined
above.
That
is,
(all
other
things
being
equal)
the
larger
the
molecule
the
higher
the
melting
point.
There
is
one
significant
difference
between
boiling
points
and
melting
points.
Solids
have
more
rigid
and
fixed
structure
than
liquids.
This
rigid
structure
requires
energy
to
break
down.
Thus
the
stronger
better
put
together
solid
structures
will
require
more
energy
to
break
apart.
For
alkanes,
this
can
be
seen
from
the
graph
above
(i.e.,
the
blue
line).
The
odd-numbered
alkanes
have
a
lower
trend
in
melting
points
than
even
numbered
alkanes.
This
is
because
even
numbered
alkanes
pack
well
in
the
solid
phase,
forming
a
well-organised
structure,
which
requires
more
energy
to
break
apart.
The
odd-number
alkanes
pack
less
well
and
so
the
"looser"
organised
solid
packing
structure
requires
less
energy
to
break
apart.
The
melting
points
of
branched-chain
alkanes
can
be
either
higher
or
lower
than
those
of
the
corresponding
straight-chain
alkanes,
again
depending
on
the
ability
of
the
alkane
in
question
to
packing
well
in
the
solid
phase:
This
is
particularly
true
for
isoalkanes
(2-methyl
isomers),
which
often
have
melting
points
higher
than
those
of
the
linear
analogues.
Conductivity.
Alkanes
do
not
conduct
electricity,
nor
are
they
substantially
polarized
by
an
electric
field.
For
this
reason
they
do
not
form
hydrogen
bonds
and
are
insoluble
in
polar
solvents
such
as
water.
Since
the
hydrogen
bonds
between
individual
water
molecules
are
aligned
away
from
an
alkane
molecule,
the
coexistence
of
an
alkane
and
water
leads
to
an
increase
in
molecular
order
(a
reduction
in
entropy).
As
there
is
no
significant
bonding
between
water
molecules
and
alkane
molecules,
the
second
law
of
thermodynamics
suggests
that
this
reduction
in
entropy
should
be
minimised
by
minimising
the
contact
between
alkane
and
water:
Alkanes
are
said
to
be
hydrophobic
in
that
they
repel
water.
Their
solubility
in
nonpolar
solvents
is
relatively
good,
a
property
that
is
called
lipophilicity.
Different
alkanes
are,
for
example,
miscible
in
all
proportions
among
themselves.
The
density
of
the
alkanes
usually
increases
with
increasing
number
of
carbon
atoms,
but
remains
less
than
that
of
water.
Hence,
alkanes
form
the
upper
layer
in
an
alkane-water
mixture.
Molecular
geometry===.
The
molecular
structure
of
the
alkanes
directly
affects
their
physical
and
chemical
characteristics.
It
is
derived
from
the
electron
configuration
of
carbon,
which
has
four
valence
electrons.
The
carbon
atoms
in
alkanes
are
always
sp3
hybridised,
that
is
to
say
that
the
valence
electrons
are
said
to
be
in
four
equivalent
orbitals
derived
from
the
combination
of
the
2s
orbital
and
the
three
2p
orbitals.
These
orbitals,
which
have
identical
energies,
are
arranged
spatially
in
the
form
of
a
tetrahedron,
the
angle
of
cos−1(−⅓)
≈
109.47°
between
them.
Bond
lengths
and
bond
angles.
An
alkane
molecule
has
only
C
–
H
and
C
–
C
single
bonds.
The
former
result
from
the
overlap
of
a
sp³-orbital
of
carbon
with
the
1s-orbital
of
a
hydrogen;
the
latter
by
the
overlap
of
two
sp³-orbitals
on
different
carbon
atoms.
The
bond
lengths
amount
to
1.09×10−10 m
for
a
C
–
H
bond
and
1.54×10−10 m
for
a
C
–
C
bond.
The
spatial
arrangement
of
the
bonds
is
similar
to
that
of
the
four
sp³-orbitals—they
are
tetrahedrally
arranged,
with
an
angle
of
109.47°
between
them.
Structural
formulae
that
represent
the
bonds
as
being
at
right
angles
to
one
another,
while
both
common
and
useful,
do
not
correspond
with
the
reality.
Conformation.
The
structural
formula
and
the
bond
angles
are
not
usually
sufficient
to
completely
describe
the
geometry
of
a
molecule.
There
is
a
further
degree
of
freedom
for
each
carbon
–
carbon
bond:
the
torsion
angle
between
the
atoms
or
groups
bound
to
the
atoms
at
each
end
of
the
bond.
The
spatial
arrangement
described
by
the
torsion
angles
of
the
molecule
is
known
as
its
conformation.
Ethane
forms
the
simplest
case
for
studying
the
conformation
of
alkanes,
as
there
is
only
one
C
–
C
bond.
If
one
looks
down
the
axis
of
the
C
–
C
bond,
one
will
see
the
so-called
Newman
projection.
The
hydrogen
atoms
on
both
the
front
and
rear
carbon
atoms
have
an
angle
of
120°
between
them,
resulting
from
the
projection
of
the
base
of
the
tetrahedron
onto
a
flat
plane.
However,
the
torsion
angle
between
a
given
hydrogen
atom
attached
to
the
front
carbon
and
a
given
hydrogen
atom
attached
to
the
rear
carbon
can
vary
freely
between
0°
and
360°.
This
is
a
consequence
of
the
free
rotation
about
a
carbon
–
carbon
single
bond.
Despite
this
apparent
freedom,
only
two
limiting
conformations
are
important:
eclipsed
conformation
and
staggered
conformation.
The
two
conformations,
also
known
as
rotamers,
differ
in
energy:
The
staggered
conformation
is
12.6
kJ/mol
lower
in
energy
(more
stable)
than
the
eclipsed
conformation
(the
least
stable).
This
difference
in
energy
between
the
two
conformations,
known
as
the
torsion
energy,
is
low
compared
to
the
thermal
energy
of
an
ethane
molecule
at
ambient
temperature.
There
is
constant
rotation
about
the
C-C
bond.
The
time
taken
for
an
ethane
molecule
to
pass
from
one
staggered
conformation
to
the
next,
equivalent
to
the
rotation
of
one
CH3-group
by
120°
relative
to
the
other,
is
of
the
order
of
10−11 seconds.
The
case
of
higher
alkanes
is
more
complex
but
based
on
similar
principles,
with
the
antiperiplanar
conformation
always
being
the
most
favoured
around
each
carbon-carbon
bond.
For
this
reason,
alkanes
are
usually
shown
in
a
zigzag
arrangement
in
diagrams
or
in
models.
The
actual
structure
will
always
differ
somewhat
from
these
idealised
forms,
as
the
differences
in
energy
between
the
conformations
are
small
compared
to
the
thermal
energy
of
the
molecules:
Alkane
molecules
have
no
fixed
structural
form,
whatever
the
models
may
suggest.
Spectroscopic
properties.
Virtually
all
organic
compounds
contain
carbon
–
carbon
and
carbon
–
hydrogen
bonds,
and
so
show
some
of
the
features
of
alkanes
in
their
spectra.
Alkanes
are
notable
for
having
no
other
groups,
and
therefore
for
the
"absence"
of
other
characteristic
spectroscopic
features.
Infrared
spectroscopy.
The
carbon–hydrogen
stretching
mode
gives
a
strong
absorption
between
2850
and
2960 cm−1,
while
the
carbon–carbon
stretching
mode
absorbs
between
800
and
1300 cm−1.
The
carbon–hydrogen
bending
modes
depend
on
the
nature
of
the
group:
methyl
groups
show
bands
at
1450 cm−1
and
1375 cm−1,
while
methylene
groups
show
bands
at
1465 cm−1
and
1450 cm−1.
Carbon
chains
with
more
than
four
carbon
atoms
show
a
weak
absorption
at
around
725 cm−1.
NMR
spectroscopy.
The
proton
resonances
of
alkanes
are
usually
found
at
δH
=
0.5
–
1.5.
The
carbon-13
resonances
depend
on
the
number
of
hydrogen
atoms
attached
to
the
carbon:
δC
=
8
–
30
(primary,
methyl,
-CH3),
15
–
55
(secondary,
methylene,
-CH2-),
20
–
60
(tertiary,
methyne,
C-H)
and
quaternary.
The
carbon-13
resonance
of
quaternary
carbon
atoms
is
characteristically
weak,
due
to
the
lack
of
Nuclear
Overhauser
effect
and
the
long
relaxation
time,
and
can
be
missed
in
weak
samples,
or
sample
that
have
not
been
run
for
a
sufficiently
long
time.
Mass
spectrometry.
Alkanes
have
a
high
ionisation
energy,
and
the
molecular
ion
is
usually
weak.
The
fragmentation
pattern
can
be
difficult
to
interpret,
but,
in
the
case
of
branched
chain
alkanes,
the
carbon
chain
is
preferentially
cleaved
at
tertiary
or
quaternary
carbons
due
to
the
relative
stability
of
the
resulting
free
radicals.
The
fragment
resulting
from
the
loss
of
a
single
methyl
group
(M−15)
is
often
absent,
and
other
fragment
are
often
spaced
by
intervals
of
fourteen
mass
units,
corresponding
to
sequential
loss
of
CH2-groups.
Chemical
properties.
In
general,
alkanes
show
a
relatively
low
reactivity,
because
their
C
bonds
are
relatively
stable
and
cannot
be
easily
broken.
Unlike
most
other
organic
compounds,
they
possess
no
functional
groups.
They
react
only
very
poorly
with
ionic
or
other
polar
substances.
The
acid
dissociation
constant
(pKa)
values
of
all
alkanes
are
above
60,
hence
they
are
practically
inert
to
acids
and
bases
(see:
carbon
acids).
This
inertness
is
the
source
of
the
term
"paraffins"
(with
the
meaning
here
of
"lacking
affinity").
In
crude
oil
the
alkane
molecules
have
remained
chemically
unchanged
for
millions
of
years.
However
redox
reactions
of
alkanes,
in
particular
with
oxygen
and
the
halogens,
are
possible
as
the
carbon
atoms
are
in
a
strongly
reduced
condition;
in
the
case
of
methane,
the
lowest
possible
oxidation
state
for
carbon
(−4)
is
reached.
Reaction
with
oxygen
leads
to
combustion
without
any
smoke;
with
halogens,
substitution.
In
addition,
alkanes
have
been
shown
to
interact
with,
and
bind
to,
certain
transition
metal
complexes
in
(See:
carbon-hydrogen
bond
activation).
Free
radicals,
molecules
with
unpaired
electrons,
play
a
large
role
in
most
reactions
of
alkanes,
such
as
cracking
and
reformation
where
long-chain
alkanes
are
converted
into
shorter-chain
alkanes
and
straight-chain
alkanes
into
branched-chain
isomers.
In
highly
branched
alkanes,
the
bond
angle
may
differ
significantly
from
the
optimal
value
(109.5°)
in
order
to
allow
the
different
groups
sufficient
space.
This
causes
a
tension
in
the
molecule,
known
as
steric
hindrance,
and
can
substantially
increase
the
reactivity.
Reactions
with
oxygen
(combustion
reaction).
See
the
alkane
heat
of
formation
table
for
detailed
data.
The
standard
enthalpy
change
of
combustion,
Δc"H"o,
for
alkanes
increases
by
about
650 kJ/mol
per
CH2
group.
Branched-chain
alkanes
have
lower
values
of
Δc"H"o
than
straight-chain
alkanes
of
the
same
number
of
carbon
atoms,
and
so
can
be
seen
to
be
somewhat
more
stable.
Reactions
with
halogens.
Alkanes
react
with
halogens
in
a
so-called
"free
radical
halogenation"
reaction.
The
hydrogen
atoms
of
the
alkane
are
progressively
replaced
by
halogen
atoms.
Free-radicals
are
the
reactive
species
that
participate
in
the
reaction,
which
usually
leads
to
a
mixture
of
products.
The
reaction
is
highly
exothermic,
and
can
lead
to
an
explosion.
Cracking.
Cracking
breaks
larger
molecules
into
smaller
ones.
This
can
be
done
with
a
thermal
or
catalytic
method.
The
thermal
cracking
process
follows
a
homolytic
mechanism
with
formation
of
free-radicals.
The
catalytic
cracking
process
involves
the
presence
of
acid
catalysts
(usually
solid
acids
such
as
silica-alumina
and
zeolites),
which
promote
a
heterolytic
(asymmetric)
breakage
of
bonds
yielding
pairs
of
ions
of
opposite
charges,
usually
a
carbocation
and
the
very
unstable
hydride
anion.
Carbon-localized
free-radicals
and
cations
are
both
highly
unstable
and
undergo
processes
of
chain
rearrangement,
C-C
scission
in
position
beta
(i.e.,
cracking)
and
intra-
and
intermolecular
hydrogen
transfer
or
hydride
transfer.
In
both
types
of
processes,
the
corresponding
reactive
intermediates
(radicals,
ions)
are
permanently
regenerated,
and
thus
they
proceed
by
a
self-propagating
chain
mechanism.
The
chain
of
reactions
is
eventually
terminated
by
radical
or
ion
recombination.
Isomerization
and
reformation.
Isomerization
and
reformation
are
processes
in
which
straight-chain
alkanes
are
heated
in
the
presence
of
a
platinum
catalyst.
In
isomerization,
the
alkanes
become
branched-chain
isomers.
In
reformation,
the
alkanes
become
cycloalkanes
or
aromatic
hydrocarbons,
giving
off
hydrogen
as
a
by-product.
Both
of
these
processes
raise
the
octane
number
of
the
substance.
Other
reactions.
Alkanes
will
react
with
steam
in
the
presence
of
a
nickel
catalyst
to
give
hydrogen.
Alkanes
can
be
chlorosulfonated
and
nitrated,
although
both
reactions
require
special
conditions.
The
fermentation
of
alkanes
to
carboxylic
acids
is
of
some
technical
importance.
In
the
Reed
reaction,
sulfur
dioxide,
chlorine
and
light
convert
hydrocarbons
to
sulfonyl
chlorides.
Occurrence
of
alkanes
in
the
Universe.
Alkanes
form
a
small
portion
of
the
atmospheres
of
the
outer
gas
planets
such
as
Jupiter
(0.1%
methane,
0.0002%
ethane),
Saturn
(0.2%
methane,
0.0005%
ethane),
Uranus
(1.99%
methane,
0.00025%
ethane)
and
Neptune
(1.5%
methane,
1.5
ppm
ethane).
Titan
(1.6%
methane),
a
satellite
of
Saturn,
was
examined
by
the
"Huygens"
probe,
which
indicate
that
Titan's
atmosphere
periodically
rains
liquid
methane
onto
the
moon's
surface.
Also
on
Titan,
a
methane-spewing
volcano
was
spotted
and
this
volcanism
is
believed
to
be
a
significant
source
of
the
methane
in
the
atmosphere.
There
also
appear
to
be
Methane/Ethane
lakes
near
the
north
polar
regions
of
Titan,
as
discovered
by
Cassini's
radar
imaging.
Methane
and
ethane
have
also
been
detected
in
the
tail
of
the
comet
Hyakutake.
Chemical
analysis
showed
that
the
abundances
of
ethane
and
methane
were
roughly
equal,
which
is
thought
to
imply
that
its
ices
formed
in
interstellar
space,
away
from
the
Sun,
which
would
have
evaporated
these
volatile
molecules.
Alkanes
have
also
been
detected
in
meteorites
such
as
carbonaceous
chondrites.
Occurrence
of
alkanes
on
Earth.
Traces
of
methane
gas
(about
0.0001%
or
1
ppm)
occur
in
the
Earth's
atmosphere,
produced
primarily
by
organisms
such
as
Archaea,
found
for
example
in
the
gut
of
cows.
These
hydrocarbons
collected
in
porous
rocks,
located
beneath
an
impermeable
cap
rock
and
so
are
trapped.
Unlike
methane,
which
is
constantly
reformed
in
large
quantities,
higher
alkanes
(alkanes
with
9
or
more
carbon
atoms)
rarely
develop
to
a
considerable
extent
in
nature.
These
deposits,
e.g.,
oil
fields,
have
formed
over
millions
of
years
and
once
exhausted
cannot
be
readily
replaced.
The
depletion
of
these
hydrocarbons
is
the
basis
for
what
is
known
as
the
energy
crisis.
Solid
alkanes
are
known
as
tars
and
are
formed
when
more
volatile
alkanes
such
as
gases
and
oil
evaporate
from
hydrocarbon
deposits.
One
of
the
largest
natural
deposits
of
solid
alkanes
is
in
the
asphalt
lake
known
as
the
Pitch
Lake
in
Trinidad
and
Tobago.
Methane
is
also
present
in
what
is
called
biogas,
produced
by
animals
and
decaying
matter,
which
is
a
possible
renewable
energy
source.
Alkanes
have
a
low
solubility
in
water,
so
the
content
in
the
oceans
is
negligible;
however,
at
high
pressures
and
low
temperatures
(such
as
at
the
bottom
of
the
oceans),
methane
can
co-crystallize
with
water
to
form
a
solid
methane
hydrate.
Although
this
cannot
be
commercially
exploited
at
the
present
time,
the
amount
of
combustible
energy
of
the
known
methane
hydrate
fields
exceeds
the
energy
content
of
all
the
natural
gas
and
oil
deposits
put
together;methane
extracted
from
methane
hydrate
is
considered
therefore
a
candidate
for
future
fuels.
Biological
occurrence.
Although
alkanes
occur
in
nature
in
various
way,
they
do
not
rank
biologically
among
the
essential
materials.
Cycloalkanes
with
14
to
18
carbon
atoms
occur
in
musk,
extracted
from
deer
of
the
family
Moschidae.
All
further
information
refers
to
(acyclic)
alkanes.
Certain
types
of
bacteria
can
metabolise
alkanes:
they
prefer
even-numbered
carbon
chains
as
they
are
easier
to
degrade
than
odd-numbered
chains.
Methanogens
are
also
the
producers
of
marsh
gas
in
wetlands,
and
release
about
two
billion
tonnes
of
methane
per
year—the
atmospheric
content
of
this
gas
is
produced
nearly
exclusively
by
them.
The
methane
output
of
cattle
and
other
herbivores,
which
can
release
up
to
150 litres
per
day,
and
of
termites,
is
also
due
to
methanogens.
They
also
produce
this
simplest
of
all
alkanes
in
the
intestines
of
humans.
Methanogenic
archaea
are,
hence,
at
the
end
of
the
carbon
cycle,
with
carbon
being
released
back
into
the
atmosphere
after
having
been
fixed
by
photosynthesis.
It
is
probable
that
our
current
deposits
of
natural
gas
were
formed
in
a
similar
way.
Alkanes
also
play
a
role,
if
a
minor
role,
in
the
biology
of
the
three
eukaryotic
groups
of
organisms:
fungi,
plants
and
animals.
Some
specialised
yeasts,
e.g.,
"Candida
tropicale",
"Pichia"
sp.,
"Rhodotorula"
sp.,
can
use
alkanes
as
a
source
of
carbon
and/or
energy.
The
fungus
"Amorphotheca
resinae"
prefers
the
longer-chain
alkanes
in
aviation
fuel,
and
can
cause
serious
problems
for
aircraft
in
tropical
regions.
In
plants,
the
solid
long-chain
alkanes
are
found
in
the
plant
cuticle
and
epicuticular
wax
of
many
species,
but
are
only
rarely
major
constituents.
They
protect
the
plant
against
water
loss,
prevent
the
leaching
of
important
minerals
by
the
rain,
and
protect
against
bacteria,
fungi,
and
harmful
insects.
The
carbon
chains
in
plant
alkanes
are
usually
odd-numbered,
between
twenty-seven
and
thirty-three
carbon
atoms
in
length
and
are
made
by
the
plants
by
decarboxylation
of
even-numbered
fatty
acids.
The
exact
composition
of
the
layer
of
wax
is
not
only
species-dependent,
but
changes
also
with
the
season
and
such
environmental
factors
as
lighting
conditions,
temperature
or
humidity.
Alkanes
are
found
in
animal
products,
although
they
are
less
important
than
unsaturated
hydrocarbons.
One
example
is
the
shark
liver
oil,
which
is
approximately
14%
pristane
(2,6,10,14-tetramethylpentadecane,
C19H40).
Their
occurrence
is
more
important
in
pheromones,
chemical
messenger
materials,
on
which
above
all
insects
are
dependent
for
communication.
With
some
kinds,
as
the
support
beetle
"Xylotrechus
colonus",
primarily
pentacosane
(C25H52),
3-methylpentaicosane
(C26H54)
and
9-methylpentaicosane
(C26H54),
they
are
transferred
by
body
contact.
With
others
like
the
tsetse
fly
"Glossina
morsitans
morsitans",
the
pheromone
contains
the
four
alkanes
2-methylheptadecane
(C18H38),
17,21-dimethylheptatriacontane
(C39H80),
15,19-dimethylheptatriacontane
(C39H80)
and
15,19,23-trimethylheptatriacontane
(C40H82),
and
acts
by
smell
over
longer
distances,
a
useful
characteristic
for
pest
control.
Waggle-dancing
honeybees
produce
and
release
two
alkanes,
tricosane
and
pentacosane.
Ecological
relations.
One
example,
in
which
both
plant
and
animal
alkanes
play
a
role,
is
the
ecological
relationship
between
the
sand
bee
("Andrena
nigroaenea")
and
the
early
spider
orchid
("Ophrys
sphegodes");
the
latter
is
dependent
for
pollination
on
the
former.
Sand
bees
use
pheromones
in
order
to
identify
a
mate;
in
the
case
of
"A.
nigroaenea",
the
females
emit
a
mixture
of
tricosane
(C23H48),
pentacosane
(C25H52)
and
heptacosane
(C27H56)
in
the
ratio
3:3:1,
and
males
are
attracted
by
specifically
this
odour.
The
orchid
takes
advantage
of
this
mating
arrangement
to
get
the
male
bee
to
collect
and
disseminate
its
pollen;
parts
of
its
flower
not
only
resemble
the
appearance
of
sand
bees,
but
also
produce
large
quantities
of
the
three
alkanes
in
the
same
ratio
as
female
sand
bees.
As
a
result
numerous
males
are
lured
to
the
blooms
and
attempt
to
copulate
with
their
imaginary
partner:
although
this
endeavour
is
not
crowned
with
success
for
the
bee,
it
allows
the
orchid
to
transfer
its
pollen,
which
will
be
dispersed
after
the
departure
of
the
frustrated
male
to
different
blooms.
Petroleum
refining.
As
stated
earlier,
the
most
important
source
of
alkanes
is
natural
gas
and
crude
oil.
Alkanes
are
separated
in
an
oil
refinery
by
fractional
distillation
and
processed
into
many
different
products.
Fischer-Tropsch.
The
Fischer-Tropsch
process
is
a
method
to
synthesize
liquid
hydrocarbons,
including
alkanes,
from
carbon
monoxide
and
hydrogen.
This
method
is
used
to
produce
substitutes
for
petroleum
distillates.
Laboratory
preparation.
Alkanes
or
alkyl
groups
can
also
be
prepared
directly
from
alkyl
halides
in
the
Corey-House-Posner-Whitesides
reaction.
The
Barton-McCombie
deoxygenation
removes
hydroxyl
groups
from
alcohols
e.g.
Applications.
The
applications
of
a
certain
alkane
can
be
determined
quite
well
according
to
the
number
of
carbon
atoms.
The
first
four
alkanes
are
used
mainly
for
heating
and
cooking
purposes,
and
in
some
countries
for
electricity
generation.
Methane
and
ethane
are
the
main
components
of
natural
gas;
they
are
normally
stored
as
gases
under
pressure.
It
is,
however,
easier
to
transport
them
as
liquids:
This
requires
both
compression
and
cooling
of
the
gas.
Propane
and
butane
can
be
liquefied
at
fairly
low
pressures,
and
are
well
known
as
liquified
petroleum
gas
(LPG).
Propane,
for
example,
is
used
in
the
propane
gas
burner,
butane
in
disposable
cigarette
lighters.
The
two
alkanes
are
used
as
propellants
in
aerosol
sprays.
From
pentane
to
octane
the
alkanes
are
reasonably
volatile
liquids.
They
are
used
as
fuels
in
internal
combustion
engines,
as
they
vaporise
easily
on
entry
into
the
combustion
chamber
without
forming
droplets,
which
would
impair
the
uniformity
of
the
combustion.
Branched-chain
alkanes
are
preferred
as
they
are
much
less
prone
to
premature
ignition,
which
causes
knocking,
than
their
straight-chain
homologues.
This
propensity
to
premature
ignition
is
measured
by
the
octane
rating
of
the
fuel,
where
2,2,4-trimethylpentane
("isooctane")
has
an
arbitrary
value
of
100,
and
heptane
has
a
value
of
zero.
Apart
from
their
use
as
fuels,
the
middle
alkanes
are
also
good
solvents
for
nonpolar
substances.
Alkanes
from
nonane
to,
for
instance,
hexadecane
(an
alkane
with
sixteen
carbon
atoms)
are
liquids
of
higher
viscosity,
less
and
less
suitable
for
use
in
gasoline.
They
form
instead
the
major
part
of
diesel
and
aviation
fuel.
Diesel
fuels
are
characterised
by
their
cetane
number,
cetane
being
an
old
name
for
hexadecane.
However,
the
higher
melting
points
of
these
alkanes
can
cause
problems
at
low
temperatures
and
in
polar
regions,
where
the
fuel
becomes
too
thick
to
flow
correctly.
Alkanes
from
hexadecane
upwards
form
the
most
important
components
of
fuel
oil
and
lubricating
oil.
In
latter
function,
they
work
at
the
same
time
as
anti-corrosive
agents,
as
their
hydrophobic
nature
means
that
water
cannot
reach
the
metal
surface.
Many
solid
alkanes
find
use
as
paraffin
wax,
for
example,
in
candles.
This
should
not
be
confused
however
with
true
wax,
which
consists
primarily
of
esters.
Alkanes
with
a
chain
length
of
approximately
35
or
more
carbon
atoms
are
found
in
bitumen,
used,
for
example,
in
road
surfacing.
However,
the
higher
alkanes
have
little
value
and
are
usually
split
into
lower
alkanes
by
cracking.
Some
synthetic
polymers
such
as
polyethylene
and
polypropylene
are
alkanes
with
chains
containing
hundreds
of
thousands
of
carbon
atoms.
These
materials
are
used
in
innumerable
applications,
and
billions
of
kilograms
of
these
materials
are
made
and
used
each
year.
Environmental
transformations.
When
released
in
the
environment,
alkanes
don't
undergo
rapid
biodegradation,
because
they
haven't
functional
groups
(like
hydroxyl
or
carbonyl)
that
are
needed
by
most
organismsm
in
order
to
metabolize
the
compound.
However,
some
bacteria
can
metabolize
some
alkanes
(expecially
those
linear
and
short),
by
oxidizing
the
terminal
carbon
atom.
The
product
is
an
alcohol,
that
could
be
next
oxidized
to
an
aldehyde,
and
finally
to
a
carboxylic
acid.
The
resulting
fatty
acid
could
be
metabolized
through
the
fatty
acid
degradation
pathway.
Hazards.
Methane
is
explosive
when
mixed
with
air
(1
–
8%
CH4)
and
is
a
strong
greenhouse
gas:
Other
lower
alkanes
can
also
form
explosive
mixtures
with
air.
The
lighter
liquid
alkanes
are
highly
flammable,
although
this
risk
decreases
with
the
length
of
the
carbon
chain.
Pentane,
hexane,
heptane,
and
octane
are
classed
as
"dangerous
for
the
environment"
and
"harmful".
The
straight-chain
isomer
of
hexane
is
a
neurotoxin.
Halogen-rich
alkanes,
like
chloroform,
can
be
carcinogenic
as
well.
---END.OF.DOCUMENT---
Appeal.
In
law,
an
appeal
is
a
process
for
requesting
a
formal
change
to
an
official
decision.
The
specific
procedures
for
appealing,
including
even
whether
there
is
a
right
of
appeal
from
a
particular
type
of
decision,
can
vary
greatly
from
country
to
country.
Even
within
a
jurisdiction,
the
nature
of
an
appeal
can
vary
greatly
depending
on
the
type
of
case.
An
appellate
court
is
a
court
that
hears
cases
on
appeal
from
another
court.
Depending
on
the
particular
legal
rules
that
apply
to
each
circumstance,
a
party
to
a
court
case
who
is
unhappy
with
the
result
might
be
able
to
challenge
that
result
in
an
appellate
court
on
specific
grounds.
These
grounds
typically
could
include
errors
of
law,
fact,
or
procedure
(in
the
United
States,
due
process).
In
different
jurisdictions,
appellate
courts
are
also
called
appeals
courts,
courts
of
appeals,
superior
courts,
or
supreme
courts.
Who
can
appeal.
A
party
who
files
an
appeal
is
called
an
"appellant"
or
"petitioner",
and
a
party
on
the
other
side
is
called
a
"respondent"
(in
most
common-law
countries)
or
an
"appellee"
(in
the
United
States).
A
"cross-appeal"
is
an
appeal
brought
by
the
respondent.
For
example,
suppose
at
trial
the
judge
found
for
the
plaintiff
and
ordered
the
defendant
to
pay
$50,000.
If
the
defendant
files
an
appeal
arguing
that
he
should
not
have
to
pay
any
money,
then
the
plaintiff
might
file
a
cross-appeal
arguing
that
the
defendant
should
have
to
pay
$200,000
instead
of
$50,000.
The
appellant
is
the
party
who,
having
lost
part
or
all
their
claim
in
a
lower
court
decision,
is
appealing
to
a
higher
court
to
have
their
case
reconsidered.
This
is
usually
done
on
the
basis
that
the
lower
court
judge
erred
in
the
application
of
law,
but
it
may
also
be
possible
to
appeal
on
the
basis
of
court
misconduct,
or
that
a
finding
of
fact
was
entirely
unreasonable
to
make
on
the
evidence.
The
appellant
in
the
new
case
can
be
either
the
plaintiff
(or
"claimant"),
defendant,
or
respondent
(appellee)
from
the
lower
case,
depending
on
who
was
the
losing
party.
The
winning
party
from
the
lower
court,
however,
is
now
the
respondent.
In
unusual
cases
the
appellant
can
be
the
victor
in
the
court
below,
but
still
appeal.
For
example,
in
"Doyle
v
Olby
(Ironmongers)
Ltd"
[1969]
2
QB
158,
the
claimant
appealed
(successfully)
on
the
basis
that,
although
he
won
in
the
court
below,
the
lower
court
had
applied
the
wrong
measure
of
damages
and
he
had
not
been
fully
recompensed.
An
appellee
is
the
party
to
an
appeal
in
which
the
lower
court
judgment
was
in
its
favor.
The
appellee
is
required
to
respond
to
the
petition,
oral
arguments,
and
legal
briefs
of
the
appellant.
In
general,
the
appellee
takes
the
procedural
posture
that
the
lower
court's
decision
should
be
affirmed.
Ability
to
appeal.
An
appeal
"as
of
right"
is
one
that
is
guaranteed
by
statute
or
some
underlying
constitutional
or
legal
principle.
The
appellate
court
cannot
refuse
to
listen
to
the
appeal.
An
appeal
"by
leave"
or
"permission"
requires
the
appellant
to
move
for
leave
to
appeal;
in
such
a
situation
either
or
both
of
the
lower
court
and
the
appellate
court
may
have
the
discretion
to
grant
or
refuse
the
appellant's
demand
to
appeal
the
lower
court's
decision.
A
good
example
of
this
is
the
U.S.
Supreme
Court
in
which
at
least
three
justices
must
agree
to
hear
the
case
if
there
is
a
constitutional
issue.
In
tort,
equity,
or
other
civil
matters
either
party
to
a
previous
case
may
file
an
appeal.
In
criminal
matters,
however,
the
state
or
prosecution
generally
has
no
appeal
"as
of
right".
And
due
to
the
double
jeopardy
principle,
in
the
United
States
the
state
or
prosecution
may
never
appeal
a
jury
or
bench
verdict
of
acquittal.
But
in
some
jurisdictions,
the
state
or
prosecution
may
appeal
"as
of
right"
from
a
trial
court's
dismissal
of
an
indictment
in
whole
or
in
part
or
from
a
trial
court's
granting
of
a
defendant's
suppression
motion.
Likewise,
in
some
jurisdictions,
the
state
or
prosecution
may
appeal
an
issue
of
law
"by
leave"
from
the
trial
court
and/or
the
appellate
court.
The
ability
of
the
prosecution
to
appeal
a
decision
in
favor
of
a
defendant
varies
significantly
internationally.
All
parties
must
present
grounds
to
appeal,
or
it
will
not
be
heard.
By
convention
in
some
law
reports,
the
appellant
is
named
first.
This
can
mean
that
where
it
is
the
defendant
who
appeals,
the
name
of
the
case
in
the
law
reports
reverses
(in
some
cases
twice)
as
the
appeals
work
their
way
up
the
court
hierarchy.
This
is
not
always
true,
however.
In
the
United
States
federal
courts,
the
parties'
names
always
stay
in
the
same
order
as
the
lower
court
when
an
appeal
is
taken
to
the
circuit
courts
of
appeals,
and
are
re-ordered
only
if
the
appeal
reaches
the
United
States
Supreme
Court.
Direct
or
collateral.
Many
jurisdictions
recognize
two
types
of
appeals,
particularly
in
the
criminal
context.
The
first
is
the
traditional
"direct"
appeal
in
which
the
appellant
files
an
appeal
with
the
next
higher
court
of
review.
The
second
is
the
collateral
appeal
or
post-conviction
petition,
in
which
the
petitioner-appellant
files
the
appeal
in
a
court
of
first
instance—usually
the
court
that
tried
the
case.
The
key
distinguishing
factor
between
direct
and
collateral
appeals
is
that
the
former
only
reviews
evidence
that
was
presented
in
the
trial
court,
but
the
latter
allows
review
of
evidence
dehors
the
record:
depositions,
affidavits,
and
witness
statements
that
did
not
come
in
at
trial.
The
standard
for
post-conviction
relief
is
high,
typically
requiring
the
petitioner
to
demonstrate
that
the
evidence
presented
was
not
available
in
the
usual
course
of
trial
discovery.
Relief
in
post-conviction
is
rare
and
is
most
often
found
in
capital
or
violent
felony
cases.
The
typical
scenario
involves
an
incarcerated
defendant
locating
DNA
evidence
demonstrating
the
defendant's
actual
innocence.
Types
of
appeal.
There
are
a
number
of
appeal
actions,
their
differences
being
potentially
confusing,
thus
bearing
some
explanation.
Three
of
the
most
common
are
an
appeal
to
which
the
defendant
has
as
a
right,
a
writ
of
certiorari
and
a
writ
of
habeas
corpus.
An
appeal
to
which
the
defendant
has
a
right
cannot
be
abridged
by
the
court
which
is,
by
designation
of
its
jurisdiction,
obligated
to
hear
the
appeal.
In
such
an
appeal,
the
appellant
feels
that
some
error
has
been
made
in
his
trial,
necessitating
an
appeal.
A
matter
of
importance
is
the
basis
on
which
such
an
appeal
might
be
filed:
generally
appeals
as
a
matter
of
right
may
only
address
issues
which
were
originally
raised
in
trial
(as
evidenced
by
documentation
in
the
official
record).
Any
issue
not
raised
in
the
original
trial
may
not
be
considered
on
appeal
and
will
be
considered
estoppel.
A
convenient
test
for
whether
a
petition
is
likely
to
succeed
on
the
grounds
of
error
is
confirming
that
(1)
a
mistake
was
indeed
made
(2)
an
objection
to
that
mistake
was
presented
by
counsel
and
(3)
that
mistake
negatively
affected
the
defendant’s
trial.
A
writ
of
certiorari,
otherwise
know
as
simply
as
cert,
is
an
order
by
a
higher
court
directing
a
lower
court
to
send
record
of
a
case
for
review,
and
is
the
next
logical
step
in
post-trial
procedure.
While
states
may
have
similar
processes,
a
writ
of
cert
is
usually
only
issued,
in
the
United
States,
by
the
Supreme
Court,
although
some
states
retain
this
procedure.
Unlike
the
aforementioned
appeal,
a
writ
of
cert
is
not
a
matter
of
right.
A
writ
of
cert
will
have
to
be
petitioned
for,
the
higher
court
issuing
such
writs
on
limited
bases
according
to
constraints
such
as
time.
In
another
sense,
a
writ
of
cert
is
like
an
appeal
in
its
constraints;
it
too
may
only
seek
relief
on
grounds
raised
in
the
original
trial.
A
writ
of
habeas
corpus
is
the
last
opportunity
for
the
defendant
to
find
relief
against
his
guilty
conviction.
Habeas
corpus
may
be
pursued
if
a
defendant
is
unsatisfied
with
the
outcome
of
his
appeal
and
has
been
refused
(or
did
not
pursue)
a
writ
of
cert,
at
which
point
he
may
petition
one
of
several
courts
for
a
writ
of
habeas
corpus.
Again,
these
are
granted
at
the
discretion
of
the
court
and
require
a
petition.
Like
appeals
or
writs
of
cert,
a
writ
of
habeas
corpus
may
overturn
a
defendant's
guilty
conviction
by
finding
some
error
in
the
original
trial.
The
major
difference
is
that
writs
of
habeas
corpus
may,
and
often,
focus
on
issues
that
lay
outside
the
original
premises
of
the
trial,
i.e.,
issues
that
could
not
be
raised
by
appeal
or
writs
of
cert.
These
often
fall
in
two
logical
categories:
(1)
that
the
trial
lawyer
was
ineffectual
or
incompetent
or
(2)
that
some
constitutional
right
has
been
violated.
Notice
of
appeal.
A
notice
of
appeal
is
a
form
or
document
that
in
many
cases
is
required
to
begin
an
appeal.
The
form
is
completed
by
the
appellant
or
by
the
appellant's
legal
representative.
The
nature
of
this
form
can
vary
greatly
from
country
to
country
and
from
court
to
court
within
a
country.
The
specific
rules
of
the
legal
system
will
dictate
exactly
how
the
appeal
is
officially
begun.
For
example,
the
appellant
might
have
to
file
the
notice
of
appeal
with
the
appellate
court,
or
with
the
court
from
which
the
appeal
is
taken,
or
both.
Some
courts
have
samples
of
a
notice
of
appeal
on
the
court's
own
web
site.
The
deadline
for
beginning
an
appeal
can
often
be
very
short:
traditionally,
it
is
measured
in
days,
not
years.
This
can
vary
from
country
to
country,
as
well
as
within
a
country,
depending
on
the
specific
rules
in
force.
How
an
appeal
is
processed.
Generally
speaking
the
appellate
court
examines
the
record
of
evidence
presented
in
the
trial
court
and
the
law
that
the
lower
court
applied
and
decides
whether
that
decision
was
legally
sound
or
not.
The
appellate
court
will
typically
be
deferential
to
the
lower
court's
findings
of
fact
(such
as
whether
a
defendant
committed
a
particular
act),
unless
clearly
erroneous,
and
so
will
focus
on
the
court's
application
of
the
law
to
those
facts
(such
as
whether
the
act
found
by
the
court
to
have
occurred
fits
a
legal
definition
at
issue).
If
the
appellate
court
finds
no
defect,
it
"affirms"
the
judgment.
If
the
appellate
court
does
find
a
legal
defect
in
the
decision
"below"
(i.e.,
in
the
lower
court),
it
may
"modify"
the
ruling
to
correct
the
defect,
or
it
may
nullify
("reverse"
or
"vacate")
the
whole
decision
or
any
part
of
it.
It
may,
in
addition,
send
the
case
back
("remand"
or
"remit")
to
the
lower
court
for
further
proceedings
to
remedy
the
defect.
In
some
cases,
an
appellate
court
may
review
a
lower
court
decision
"de
novo"
(or
completely),
challenging
even
the
lower
court's
findings
of
fact.
This
might
be
the
proper
standard
of
review,
for
example,
if
the
lower
court
resolved
the
case
by
granting
a
pre-trial
motion
to
dismiss
or
motion
for
summary
judgment
which
is
usually
based
only
upon
written
submissions
to
the
trial
court
and
not
on
any
trial
testimony.
Another
situation
is
where
appeal
is
by
way
of
"re-hearing".
Certain
jurisdictions
permit
certain
appeals
to
cause
the
trial
to
be
heard
afresh
in
the
appellate
court.
An
example
would
be
an
appeal
from
a
magistrates'
court
to
the
Crown
Court
in
England
and
Wales.
Sometimes,
the
appellate
court
finds
a
defect
in
the
procedure
the
parties
used
in
filing
the
appeal
and
dismisses
the
appeal
without
considering
its
merits,
which
has
the
same
effect
as
affirming
the
judgment
below.
(This
would
happen,
for
example,
if
the
appellant
waited
too
long,
under
the
appellate
court's
rules,
to
file
the
appeal.)
In
England
and
many
other
jurisdictions,
however,
the
phrase
"appeal
dismissed"
is
equivalent
to
the
U.S.
term
"affirmed";
and
the
phrase
"appeal
allowed"
is
equivalent
to
the
U.S.
term
"reversed".
Generally,
there
is
no
trial
in
an
appellate
court,
only
consideration
of
the
record
of
the
evidence
presented
to
the
trial
court
and
all
the
pre-trial
and
trial
court
proceedings
are
reviewed—unless
the
appeal
is
by
way
of
re-hearing,
new
evidence
will
usually
only
be
considered
on
appeal
in
"very"
rare
instances,
for
example
if
that
material
evidence
was
unavailable
to
a
party
for
some
very
significant
reason
such
as
prosecutorial
misconduct.
In
some
systems,
an
appellate
court
will
only
consider
the
written
decision
of
the
lower
court,
together
with
any
written
evidence
that
was
before
that
court
and
is
relevant
to
the
appeal.
In
other
systems,
the
appellate
court
will
normally
consider
the
record
of
the
lower
court.
In
those
cases
the
record
will
first
be
certified
by
the
lower
court.
The
appellant
has
the
opportunity
to
present
arguments
for
the
granting
of
the
appeal
and
the
appellee
(or
respondent)
can
present
arguments
against
it.
Arguments
of
the
parties
to
the
appeal
are
presented
through
their
appellate
lawyers,
if
represented,
or
"pro
se"
if
the
party
has
not
engaged
legal
representation.
Those
arguments
are
presented
in
written
briefs
and
sometimes
in
oral
argument
to
the
court
at
a
hearing.
At
such
hearings
each
party
is
allowed
a
brief
presentation
at
which
the
appellate
judges
ask
questions
based
on
their
review
of
the
record
below
and
the
submitted
briefs.
It
is
important
to
note
that
in
an
adversarial
system
appellate
courts
do
not
have
the
power
to
review
lower
court
decisions
unless
a
party
appeals
it.
Therefore
if
a
lower
court
has
ruled
in
an
improper
manner
or
against
legal
precedent
that
judgment
will
stand
even
if
it
might
have
been
overturned
on
appeal.
United
States.
The
United
States
legal
system
generally
recognizes
two
types
of
appeals:
a
trial
"de
novo"
or
an
appeal
on
the
record.
A
trial
de
novo
is
usually
available
for
review
of
informal
proceedings
conducted
by
some
minor
judicial
tribunals
in
proceedings
that
do
not
provide
all
the
procedural
attributes
of
a
formal
judicial
trial.
If
unchallenged,
these
decisions
have
the
power
to
settle
more
minor
legal
disputes
once
and
for
all.
If
a
party
is
dissatisfied
with
the
finding
of
such
a
tribunal,
one
generally
has
the
power
to
request
a
trial
"de
novo"
by
a
court
of
record.
In
such
a
proceeding,
all
issues
and
evidence
may
be
developed
newly,
as
though
never
heard
before,
and
one
is
not
restricted
to
the
evidence
heard
in
the
lower
proceeding.
Sometimes,
however,
the
decision
of
the
lower
proceeding
is
itself
admissible
as
evidence,
thus
helping
to
curb
frivolous
appeals.
In
an
appeal
on
the
record
from
a
decision
in
a
judicial
proceeding,
both
appellant
and
respondent
are
bound
to
base
their
arguments
wholly
on
the
proceedings
and
body
of
evidence
as
they
were
presented
in
the
lower
tribunal.
Each
seeks
to
prove
to
the
higher
court
that
the
result
they
desired
was
the
just
result.
Precedent
and
case
law
figure
prominently
in
the
arguments.
In
order
for
the
appeal
to
succeed,
the
appellant
must
prove
that
the
lower
court
committed
reversible
error,
that
is,
an
impermissible
action
by
the
court
acted
to
cause
a
result
that
was
unjust,
and
which
would
not
have
resulted
had
the
court
acted
properly.
Some
examples
of
reversible
error
would
be
erroneously
instructing
the
jury
on
the
law
applicable
to
the
case,
permitting
seriously
improper
argument
by
an
attorney,
admitting
or
excluding
evidence
improperly,
acting
outside
the
court's
jurisdiction,
injecting
bias
into
the
proceeding
or
appearing
to
do
so,
juror
misconduct,
etc.
The
failure
to
formally
object
at
the
time,
to
what
one
views
as
improper
action
in
the
lower
court,
may
result
in
the
affirmance
of
the
lower
court's
judgment
on
the
grounds
that
one
did
not
"preserve
the
issue
for
appeal"
by
objecting.
In
cases
where
a
judge
rather
than
a
jury
decided
issues
of
fact,
an
appellate
court
will
apply
an
"abuse
of
discretion"
standard
of
review.
Under
this
standard,
the
appellate
court
gives
deference
to
the
lower
court's
view
of
the
evidence,
and
reverses
its
decision
only
if
it
were
a
clear
abuse
of
discretion.
This
is
usually
defined
as
a
decision
outside
the
bounds
of
reasonableness.
On
the
other
hand,
the
appellate
court
normally
gives
less
deference
to
a
lower
court's
decision
on
issues
of
law,
and
may
reverse
if
it
finds
that
the
lower
court
applied
the
wrong
legal
standard.
In
some
rare
cases,
an
appellant
may
successfully
argue
that
the
law
under
which
the
lower
decision
was
rendered
was
unconstitutional
or
otherwise
invalid,
or
may
convince
the
higher
court
to
order
a
new
trial
on
the
basis
that
evidence
earlier
sought
was
concealed
or
only
recently
discovered.
In
the
case
of
new
evidence,
there
must
be
a
high
probability
that
its
presence
or
absence
would
have
made
a
material
difference
in
the
trial.
Another
issue
suitable
for
appeal
in
criminal
cases
is
effective
assistance
of
counsel.
If
a
defendant
has
been
convicted
and
can
prove
that
his
lawyer
did
not
adequately
handle
his
case
"and"
that
there
is
a
reasonable
probability
that
the
result
of
the
trial
would
have
been
different
had
the
lawyer
given
competent
representation,
he
is
entitled
to
a
new
trial.
In
the
United
States,
a
lawyer
traditionally
starts
an
oral
argument
to
any
appellate
court
with
the
words
"May
it
please
the
court."
After
an
appeal
is
heard,
the
"mandate"
is
a
formal
notice
of
a
decision
by
a
court
of
appeal;
this
notice
is
transmitted
to
the
trial
court
and,
when
filed
by
the
clerk
of
the
trial
court,
constitutes
the
final
judgment
on
the
case,
unless
the
appeal
court
has
directed
further
proceedings
in
the
trial
court.
The
mandate
is
distinguished
from
the
appeal
court's
opinion,
which
sets
out
the
legal
reasoning
for
its
decision.
In
some
U.S.
jurisdictions
the
mandate
is
known
as
the
"remittitur".
Appellate
review.
Appellate
review
is
the
general
term
for
the
process
by
which
courts
with
appellate
jurisdiction
take
jurisdiction
of
matters
decided
by
lower
courts.
It
is
distinguished
from
judicial
review,
which
refers
to
the
court's
overriding
constitutional
or
statutory
right
to
determine
if
a
legislative
act
or
administrative
decision
is
defective
for
jurisdictional
or
other
reasons
(which
may
vary
by
jurisdiction).
In
most
jurisdictions
the
normal
and
preferred
way
of
seeking
appellate
review
is
by
filing
an
appeal
of
the
final
judgment.
Generally,
an
appeal
of
the
judgment
will
also
allow
appeal
of
all
other
orders
or
rulings
made
by
the
trial
court
in
the
course
of
the
case.
This
is
because
such
orders
cannot
be
appealed
"as
of
right".
However,
certain
critical
interlocutory
court
orders,
such
as
the
denial
of
a
request
for
an
interim
injunction,
or
an
order
holding
a
person
in
contempt
of
court,
can
be
appealed
immediately
although
the
case
may
otherwise
not
have
been
fully
disposed
of.
In
American
law,
there
are
two
distinct
forms
of
appellate
review,
"direct"
and
"collateral".
For
example,
a
criminal
defendant
may
be
convicted
in
state
court,
and
lose
on
"direct
appeal"
to
higher
state
appellate
courts,
and
if
unsuccessful,
mount
a
"collateral"
action
such
as
filing
for
a
writ
of
habeas
corpus
in
the
federal
courts.
Generally
speaking,
"[d]irect
appeal
statutes
afford
defendants
the
opportunity
to
challenge
the
merits
of
a
judgment
and
allege
errors
of
law
or
fact....
[Collateral
review],
on
the
other
hand,
provide[s]
an
independent
and
civil
inquiry
into
the
validity
of
a
conviction
and
sentence,
and
as
such
are
generally
limited
to
challenges
to
constitutional,
jurisdictional,
or
other
fundamental
violations
that
occurred
at
trial."
"Graham
v.
Borgen",
__
F
3d.
__
(7th
Cir.
2007)
(no.
04-4103)
(slip
op.
at
7)
(citation
omitted).
In
Anglo-American
common
law
courts,
appellate
review
of
lower
court
decisions
may
also
be
obtained
by
filing
a
petition
for
review
by
prerogative
writ
in
certain
cases.
There
is
no
corresponding
right
to
a
writ
in
any
pure
or
continental
civil
law
legal
systems,
though
some
mixed
systems
such
as
Quebec
recognize
these
prerogative
writs.
---END.OF.DOCUMENT---
Answer.
Generally,
an
answer
is
a
reply
to
a
question
or
is
a
solution,
a
retaliation,
or
a
response
that
is
relevant
to
the
said
question.
In
law,
an
answer
was
originally
a
solemn
assertion
in
opposition
to
some
one
or
something,
and
thus
generally
any
counter-statement
or
defense,
a
reply
to
a
question
or
objection,
or
a
correct
solution
of
a
problem.
In
the
common
law,
an
answer
is
the
first
pleading
by
a
defendant,
usually
filed
and
served
upon
the
plaintiff
within
a
certain
strict
time
limit
after
a
civil
complaint
or
criminal
information
or
indictment
has
been
served
upon
the
defendant.
It
may
have
been
preceded
by
an
"optional"
"pre-answer"
motion
to
dismiss
or
demurrer;
if
such
a
motion
is
unsuccessful,
the
defendant
"must"
file
an
answer
to
the
complaint
or
risk
an
adverse
default
judgment.
The
"answer"
establishes
which
allegations
(cause
of
action
in
civil
matters)
set
forth
by
the
complaining
party
will
be
contested
by
the
defendant,
and
states
all
the
defendant's
defenses,
thus
establishing
the
nature
and
parameters
of
the
controversy
to
be
decided
by
the
court.
In
a
criminal
case,
there
is
usually
an
arraignment
or
some
other
kind
of
appearance
before
defendant
comes
to
court.
The
pleading
in
the
criminal
case,
which
is
entered
on
the
record
in
open
court,
is
usually
either
guilty
or
not
guilty.
Generally
speaking
in
private,
civil
cases
there
is
no
plea
entered
of
guilt
or
innocence.
There
is
only
a
judgment
that
grants
money
damages
or
some
other
kind
of
equitable
remedy
such
as
restitution
or
a
permanent
injunction.
Criminal
cases
may
lead
to
fines
or
other
punishment,
such
as
imprisonment.
The
famous
Latin
"Responsa
Prudentium"
("answers
of
the
learned
ones")
were
the
accumulated
views
of
many
successive
generations
of
Roman
lawyers,
a
body
of
legal
opinion
which
gradually
became
authoritative.
In
music
an
"answer"
(also
known
as
countersubject)
is
the
technical
name
in
counterpoint
for
the
repetition
or
modification
by
one
part
or
instrument
of
a
theme
proposed
by
another.
---END.OF.DOCUMENT---
Appellate
court.
An
appellate
court
is
any
court
of
law
that
is
empowered
to
hear
an
appeal
of
a
trial
court
or
other
lower
tribunal.
In
most
jurisdictions,
the
court
system
is
divided
into
at
least
three
levels:
the
trial
court,
which
initially
hears
cases
and
reviews
evidence
and
testimony
to
determine
the
facts
of
the
case;
at
least
one
intermediate
appellate
court;
and
a
supreme
court
(or
court
of
last
resort)
which
primarily
reviews
the
decisions
of
the
intermediate
courts.
A
supreme
court
is
therefore
itself
a
kind
of
appellate
court.
Appellate
courts
worldwide
can
operate
by
varying
rules.
For
example,
the
Isle
of
Man's
traditional
local
appellate
court
is
the
Staff
of
Government
Division
which
has
only
two
Justices,
titled
"Deemsters,"
whose
decisions
are
joined
to
the
original
trial
decision.
They
almost
always
have
a
majority,
if
either
Deemster
agrees
with
the
trial
Judge.
Institutional
titles.
Many
US
jurisdictions
title
their
appellate
court
a
Court
of
Appeal
or
Court
of
Appeals.
Historically,
others
have
titled
their
appellate
court
a
Court
of
Errors
(or
Court
of
Errors
and
Appeals),
on
the
premise
that
it
was
intended
to
correct
errors
made
by
lower
courts.
Examples
of
such
courts
include
the
New
Jersey
Court
of
Errors
and
Appeals
(which
existed
from
1844
to
1947),
the
Connecticut
Supreme
Court
of
Errors
(which
has
been
renamed
the
Connecticut
Supreme
Court),
the
Kentucky
Court
of
Errors
(since
renamed
the
Kentucky
Supreme
Court),
and
the
Mississippi
High
Court
of
Errors
and
Appeals
(since
renamed
the
Supreme
Court
of
Mississippi).
In
some
jurisdictions,
courts
able
to
hear
appeals
are
known
as
an
Appellate
Division.
Depending
on
the
system,
certain
courts
may
serve
as
both
trial
courts
and
appellate
courts,
hearing
appeals
of
decisions
made
by
courts
with
more
limited
jurisdiction.
Some
jurisdictions
have
specialized
appellate
courts,
such
as
the
Texas
Court
of
Criminal
Appeals,
which
only
hears
appeals
raised
in
criminal
cases,
and
the
United
States
Court
of
Appeals
for
the
Federal
Circuit,
which
has
general
jurisdiction
but
derives
most
of
its
caseload
from
patent
cases,
on
the
one
hand,
and
appeals
from
the
Court
of
Federal
Claims
on
the
other.
Authority
to
review.
The
authority
of
appellate
courts
to
review
a
decisions
of
lower
courts
varies
widely
from
one
jurisdiction
to
another.
In
some
places,
the
appellate
court
has
limited
powers
of
review.
For
example,
in
the
United
States,
both
state
and
federal
appellate
courts
are
usually
restricted
to
examining
whether
the
court
below
made
the
correct
legal
determinations,
rather
than
hearing
direct
evidence
and
determining
what
the
facts
of
the
case
were.
Furthermore,
U.S.
appellate
courts
are
usually
restricted
to
hearing
appeals
based
on
matters
that
were
originally
brought
up
before
the
trial
court.
Hence,
such
an
appellate
court
will
not
consider
an
appellant's
argument
if
it
is
based
on
a
theory
that
is
raised
for
the
first
time
in
the
appeal.
In
most
U.S.
states,
and
in
U.S.
federal
courts,
parties
before
the
court
are
allowed
one
appeal
as
of
right.
This
means
that
a
party
who
is
unsatisfied
with
the
outcome
of
a
trial
may
bring
an
appeal
to
contest
that
outcome.
However,
appeals
may
be
costly,
and
the
appellate
court
must
find
an
error
on
the
part
of
the
court
below
that
justifies
upsetting
the
verdict.
Therefore,
only
a
small
proportion
of
trial
court
decisions
result
in
appeals.
Some
appellate
courts,
particularly
supreme
courts,
have
the
power
of
discretionary
review,
meaning
that
they
can
decide
whether
they
will
hear
an
appeal
brought
in
a
particular
case.
---END.OF.DOCUMENT---
Arraignment.
Arraignment
is
a
formal
reading
of
a
criminal
complaint
in
the
presence
of
the
defendant
to
inform
the
defendant
of
the
charges
against
him
or
her.
In
response
to
arraignment,
the
accused
is
expected
to
enter
a
plea.
Acceptable
pleas
vary
among
jurisdictions,
but
they
generally
include
"guilty",
"not
guilty",
and
the
peremptory
pleas
(or
pleas
in
bar)
setting
out
reasons
why
a
trial
cannot
proceed.
Pleas
of
"nolo
contendere"
(no
contest)
and
the
"Alford
plea"
are
allowed
in
some
circumstances.
In
England,
Wales,
and
Northern
Ireland,
arraignment
is
the
first
of
eleven
stages
in
a
criminal
trial,
and
involves
the
clerk
of
the
court
reading
out
the
indictment.
The
defendant
is
asked
whether
he
or
she
pleads
guilty
or
not
guilty
to
each
individual
charge.
This
process
is
the
same
in
Australian
jurisdictions.
In
the
U.S.
District
Court,
Central
District
of
California,
arraignment
takes
place
in
two
stages.
The
first
is
called
the
initial
arraignment
and
must
take
place
within
48
hours
of
an
individual's
arrest.
During
this
arraignment
the
defendant
is
informed
of
any
pending
legal
charges
and
is
informed
of
his
or
her
right
to
retain
counsel.
The
presiding
judge
will
also
decide
whether
or
not
to
set
bail,
and,
if
so,
for
how
much
money.
The
second
arraignment
is
called
a
post-indictment
arraignment
or
PIA.
It
is
during
this
second
arraignment
that
a
defendant
will
be
allowed
to
enter
a
plea.
Guilty
and
not
guilty
pleas.
If
the
defendant
pleads
guilty,
an
evidentiary
hearing
usually
follows.
The
court
is
not
required
to
accept
a
guilty
plea.
During
the
hearing,
the
judge
will
assess
the
offense,
mitigating
factors,
and
the
defendant's
character,
and
pass
sentence.
If
the
defendant
pleads
not
guilty,
a
date
will
be
set
for
a
preliminary
hearing
or
a
trial.
In
the
past,
a
defendant
who
refused
to
plead
(or
"stood
mute")
would
be
subject
to
peine
forte
et
dure
(Law
French
for
"strong
and
hard
punishment").
Today
in
common
law
jurisdictions,
defendants
who
refuse
to
enter
a
plea
will
have
a
plea
of
not
guilty
entered
for
them
by
the
court.
The
rationale
for
this
is
the
defendant's
right
to
silence.
Pre-trial
Release.
This
is
also
often
the
stage
at
which
arguments
in
favor
or
against
pre-trial
release
and
bail
are
made,
depending
on
the
alleged
crime
and
jurisdiction.
United
States
Federal
Rules
of
Criminal
Procedure.
Under
the
Federal
Rules
of
Criminal
Procedure,
"arraignment
shall...[consist
of
an]
open...reading
[of]
the
indictment...to
the
defendant...and
calling
on
him
to
plead
thereto.
He
shall
be
given
a
copy
of
the
indictment...before
he
is
called
upon
to
plead."
---END.OF.DOCUMENT---
America
the
Beautiful.
"America
the
Beautiful"
is
an
American
patriotic
song.
The
lyrics
were
written
by
Katharine
Lee
Bates
and
the
music
composed
by
church
organist
and
choirmaster
Samuel
A.
Ward.
Bates
originally
wrote
the
words
as
a
poem,
"Pikes
Peak",
first
published
in
the
July
4th
edition
of
the
church
periodical
"The
Congregationalist"
in
1895.
The
poem
was
titled
"America"
for
publication.
Ward
had
originally
written
the
music,
"Materna",
for
the
1600s
hymn
"O
Mother
dear,
Jerusalem"
in
1882.
Ward's
music
combined
with
the
Bates
poem
was
first
published
in
1910
and
titled
"America
the
Beautiful".
The
song
is
one
of
the
most
beloved
and
popular
of
the
many
American
patriotic
songs.
From
time
to
time
it
has
been
proposed
as
a
replacement
for
"The
Star-Spangled
Banner"
as
the
National
Anthem.
History.
In
1893,
at
the
age
of
thirty-three
Katharine
Lee
Bates,
an
English
professor
at
Wellesley
College,
had
taken
a
train
trip
to
Colorado
Springs,
Colorado,
to
teach
a
short
summer
school
session
at
Colorado
College.
Several
of
the
sights
on
her
trip
inspired
her,
and
they
found
their
way
into
her
poem,
including
the
World's
Columbian
Exposition
in
Chicago,
the
"White
City"
with
its
promise
of
the
future
contained
within
its
alabaster
buildings;
the
wheat
fields
of
America's
heartland
Kansas,
through
which
her
train
was
riding
on
July
4;
and
the
majestic
view
of
the
Great
Plains
from
high
atop
Zebulon's
Pikes
Peak.
On
the
pinnacle
of
that
mountain,
the
words
of
the
poem
started
to
come
to
her,
and
she
wrote
them
down
upon
returning
to
her
hotel
room
at
the
original
Antlers
Hotel.
The
poem
was
initially
published
two
years
later
in
"The
Congregationalist,"
to
commemorate
the
Fourth
of
July.
It
quickly
caught
the
public's
fancy.
Amended
versions
were
published
in
1904
and
1913.
Several
existing
pieces
of
music
were
adapted
to
the
poem.
A
hymn
tune
composed
by
Samuel
A.
Ward
was
generally
considered
the
best
music
as
early
as
1910
and
is
still
the
popular
tune
today.
Just
as
Bates
had
been
inspired
to
write
her
poem,
Ward
too
was
inspired
to
compose
his
tune.
The
tune
came
to
him
while
he
was
on
a
ferryboat
trip
from
Coney
Island
back
to
his
home
in
New
York
City,
after
a
leisurely
summer
day
in
1882,
and
he
immediately
wrote
it
down.
He
was
so
anxious
to
capture
the
tune
in
his
head,
he
asked
fellow
passenger
friend
Harry
Martin
for
his
shirt
cuff
to
write
the
tune
on,
thus
perhaps
the
"off
the
cuff"
analogy.
He
composed
the
tune
for
the
old
hymn
"O
Mother
Dear,
Jerusalem",
retitling
the
work
"Materna".
Ward's
music
combined
with
Bates'
poem
were
first
published
together
in
1910
and
titled,
"America
the
Beautiful".
Ward
died
in
1903,
not
knowing
the
national
stature
his
music
would
attain,
as
the
music
was
only
first
applied
to
the
song
in
1904.
Miss
Bates
was
more
fortunate,
as
the
song's
popularity
was
well-established
by
her
death
in
1929.
At
various
times
in
the
more
than
100
years
that
have
elapsed
since
the
song
as
we
know
it
was
born,
particularly
during
the
John
F.
Kennedy
administration,
there
have
been
efforts
to
give
"America
the
Beautiful"
legal
status
either
as
a
national
hymn,
or
as
a
national
anthem
equal
to,
or
in
place
of,
"The
Star-Spangled
Banner",
but
so
far
this
has
not
succeeded.
Proponents
prefer
"America
the
Beautiful"
for
various
reasons,
saying
it
is
easier
to
sing,
more
melodic,
and
more
adaptable
to
new
orchestrations
while
still
remaining
as
easily
recognizable
as
"The
Star-Spangled
Banner."
Some
prefer
"America
the
Beautiful"
over
"The
Star-Spangled
Banner"
due
to
the
latter's
war-oriented
imagery.
(Others
prefer
"The
Star-Spangled
Banner"
for
the
same
reason.)
While
that
national
dichotomy
has
stymied
any
effort
at
changing
the
tradition
of
the
national
anthem,
"America
the
Beautiful"
continues
to
be
held
in
high
esteem
by
a
large
number
of
Americans.
Popularity
of
the
song
increased
greatly
following
the
September
11,
2001
attacks;
at
some
sporting
events
it
was
sung
in
addition
to
the
traditional
singing
of
the
national
anthem.
During
the
first
taping
of
the
"Late
Show
with
David
Letterman"
following
the
attacks,
CBS
newsman
Dan
Rather
cried
briefly
as
he
quoted
the
fourth
verse.
Ray
Charles
is
credited
with
the
song's
most
well
known
rendition
in
current
times
(although
Elvis
Presley
had
good
success
with
it
in
the
1970s).
His
recording
is
very
commonly
played
at
major
sporting
events,
such
as
the
Super
Bowl;
Charles
gave
a
live
performance
of
the
song
prior
to
Super
Bowl
XXXV,
the
last
Super
Bowl
played
before
the
September
11
terrorist
attacks.
His
unique
take
on
it
places
the
third
verse
first,
after
which
he
sings
the
usual
first
verse.
In
the
third
verse
(see
below),
the
author
scolds
the
materialistic
and
self-serving
robber
barons
of
her
day,
and
urges
America
to
live
up
to
its
noble
ideals
and
to
honor,
with
both
word
and
deed,
the
memory
of
those
who
died
for
their
country.
Symbolically,
Marian
Anderson
(a
noted
opera
singer
of
her
day)
sang
a
rendition
of
America
on
the
steps
of
the
Lincoln
Memorial
in
1939
after
being
refused
use
of
Constitution
Hall
by
the
Daughters
of
the
American
Revolution
because
of
her
skin
color.
An
all-star
version
of
"America
the
Beautiful"
performed
by
country
music
singers
Trace
Adkins,
Billy
Dean,
Vince
Gill,
Carolyn
Dawn
Johnson,
Toby
Keith,
Brenda
Lee,
Lonestar,
Martina
McBride,
Jamie
O'Neal,
Kenny
Rogers
and
Keith
Urban
reached
#58
on
the
"Billboard"
Hot
Country
Singles
&
Tracks
chart
in
July
2001.
The
song
re-entered
the
chart
following
the
September
11
terrorist
attacks.
When
Richard
Nixon
visited
the
People's
Republic
of
China
in
1972,
this
song
was
played
by
Chinese
as
the
welcome
music.
Interestingly,
the
Chinese
characters
for
United
States
literally
mean
"Beautiful
Country."
The
song
is
often
included
in
songbooks
in
a
wide
variety
of
religious
congregations
in
the
United
States.
Idioms.
"From
sea
to
shining
sea"
is
an
American
idiom
meaning
from
the
Pacific
Ocean
to
the
Atlantic
Ocean
(or
vice
versa).
Many
songs
have
used
this
term,
including
the
American
patriotic
songs
"America,
The
Beautiful"
and
"God
Bless
the
USA".
In
addition
to
these,
it
is
also
featured
in
Schoolhouse
Rock's
"Elbow
Room".
A
term
similar
to
this
is
the
Canadian
motto
"A
Mari
Usque
Ad
Mare"
("From
sea
to
sea.")
Lyrics.
God
shed
his
grace
on
thee
And
crown
thy
good
with
brotherhood
Who
more
than
self
their
country
loved
God
shed
his
grace
on
thee
And
crown
thy
good
with
brotherhood
God
shed
his
grace
on
thee
Till
souls
wax
fair
as
earth
and
air
God
shed
his
grace
on
thee
God
shed
his
grace
on
thee
Till
selfish
gain
no
longer
stain
God
shed
his
grace
on
thee
Till
nobler
men
keep
once
again
---END.OF.DOCUMENT---
Assistive
technology.
Assistive
technology
(AT)
is
a
generic
term
that
includes
assistive,
adaptive,
and
rehabilitative
devices
for
people
with
disabilities
and
includes
the
process
used
in
selecting,
locating,
and
using
them.
The
Technology-Related
Assistance
for
Individuals
with
Disabilities
Act
of
1988
(US
Public
Law
100-407)
states
that
it
is
"technology
designed
to
be
utilized
in
an
assistive
technology
device
or
assistive
technology
service."
AT
promotes
greater
independence
by
enabling
people
to
perform
tasks
that
they
were
formerly
unable
to
accomplish,
or
had
great
difficulty
accomplishing,
by
providing
enhancements
to
or
changed
methods
of
interacting
with
the
technology
needed
to
accomplish
such
tasks.
Likewise,
disability
advocates
point
out
that
technology
is
often
created
without
regard
to
people
with
disabilities,
creating
unnecessary
barriers
to
hundreds
of
millions
of
people.
Assistive
technology
and
universal
accessibility.
Universal
(or
broadened)
accessibility,
or
universal
design
means
greater
usability,
particularly
for
people
with
disabilities.
Universally
accessible
technology
yields
great
rewards
to
the
typical
user
as
well;
good
accessible
design
"is"
universal
design.
One
example
is
the
"curb
cuts"
(or
dropped
curbs)
in
the
sidewalk
at
street
crossings.
While
these
curb
cuts
enable
pedestrians
with
mobility
impairments
to
cross
the
street,
they
also
aid
parents
with
carriages
and
strollers,
shoppers
with
carts,
and
travellers
and
workers
with
pull-type
bags.
As
an
example,
the
modern
telephone
is
inaccessible
to
people
who
are
deaf
or
hard
of
hearing.
Combined
with
a
text
telephone
(also
known
as
a
TDD
Telecommunications
device
for
the
deaf
and
in
the
USA
generally
called
a
TTY[TeleTYpewriter]),
which
converts
typed
characters
into
tones
that
may
be
sent
over
the
telephone
line,
a
deaf
person
is
able
to
communicate
immediately
at
a
distance.
Together
with
"relay"
services,
in
which
an
operator
reads
what
the
deaf
person
types
and
types
what
a
hearing
person
says,
the
deaf
person
is
then
given
access
to
everyone's
telephone,
not
just
those
of
people
who
possess
text
telephones.
Many
telephones
now
have
volume
controls,
which
are
primarily
intended
for
the
benefit
of
people
who
are
hard
of
hearing,
but
can
be
useful
for
all
users
at
times
and
places
where
there
is
significant
background
noise.
Some
have
larger
keys
well-spaced
to
facilitate
accurate
dialing.
Also,
a
person
with
a
mobility
impairment
can
have
difficulty
using
calculators.
Speech
recognition
software
recognizes
short
commands
and
makes
use
of
calculators
easier.
People
with
learning
disabilities
like
dyslexia
or
dysgraphia
are
using
text-to-speech
(TTS)
software
for
reading
and
spelling
programs
for
assistance
in
writing
texts.
Computers
with
their
peripheral
devices,
editing,
spellchecking
and
speech
synthesis
software
are
becoming
the
core-stones
of
the
assistive
technologies
coming
for
relief
to
the
people
with
learning
disabilities
and
to
the
people
with
visual
impairments.
The
assisting
spelling
programs
and
voice
facilities
are
bringing
better
and
more
convenient
text
reading
and
writing
experience
to
the
general
public.
Toys
which
have
been
adapted
to
be
used
by
children
with
disabilities
may
have
advantages
for
non-disabled
children
as
well.
The
Lekotek
movement
assists
parents
by
lending
assistive
technology
toys
and
expertise
to
families.
The
following
professionals
may
be
certified
by
RESNA
(RESNA.org)
to
serve
the
assistive
technology
needs
of
individuals:
occupational
therapists,
physical
therapists,
speech
language
pathologists/audiologists,
orthotists
and
prosthetists,
educators,
and
a
variety
of
other
rehabilitation
and
health
professionals.
Personal
Emergency
Response
Systems.
Personal
Emergency
Response
Systems
(PERS),
or
Telecare
(UK
term),
are
a
particular
sort
of
assistive
technology
that
use
electronic
sensors
connected
to
an
alarm
system
to
help
caregivers
manage
risk
and
help
vulnerable
people
stay
independent
at
home
longer.
An
example
would
be
the
systems
being
put
in
place
for
senior
people
such
as
fall
detectors,
thermometers
(for
hypothermia
risk),
flooding
and
unlit
gas
sensors
(for
people
with
mild
dementia).
Notably,
these
alerts
can
be
customized
to
the
particular
person's
risks.
When
the
alert
is
triggered,
a
message
is
sent
to
a
carer
or
contact
centre
who
can
respond
appropriately.
Technology
similar
to
PERS
can
also
be
used
to
act
within
a
person's
home
rather
than
just
to
respond
to
a
detected
crisis.
Using
one
of
the
examples
above,
gas
sensors
for
people
with
dementia
can
be
used
to
trigger
a
device
that
turns
off
the
gas
and
tells
someone
what
has
happened.
Designing
for
people
with
dementia
is
a
good
example
of
how
the
design
of
the
interface
of
a
piece
of
AT
is
critical
to
its
usefulness.
People
with
dementia
or
any
other
identified
user
group
must
be
involved
in
the
design
process
to
make
sure
that
the
design
is
accessible
and
usable.
In
the
example
above,
a
voice
message
could
be
used
to
remind
the
person
with
dementia
to
turn
off
the
gas
himself,
but
whose
voice
should
be
used,
and
what
should
the
message
say?
Questions
like
these
must
be
answered
through
user
consultation,
involvement
and
evaluation.
Accessible
computer
input.
Sitting
at
a
desk
with
a
QWERTY
keyboard
and
a
mouse
remains
the
dominant
way
of
interacting
with
a
personal
computer.
Some
Assistive
Technology
reduces
the
strain
of
this
way
of
work
through
ergonomic
accessories
with
height-adjustable
furniture,
footrests,
wrist
rests,
and
arm
supports
to
ensure
correct
posture.
Keyguards
fit
over
the
keyboard
to
help
prevent
unintentional
keypresses.
More
ambitiously,
and
quite
crucially
when
keyboard
or
mouse
prove
unusable,
AT
can
also
replace
the
keyboard
and
mouse
with
alternative
devices
such
as
the
LOMAK
keyboard,
trackballs,
joysticks,
graphics
tablets,
touchpads,
touch
screens,
foot
mice,
a
microphone
with
speech
recognition
software,
sip-and-puff
input,
switch
access,
and
vision-based
input
devices,
such
as
eye
trackers
which
allow
the
user
to
control
the
mouse
with
their
eyes.
Visual
impairment.
Choice
of
appropriate
hardware
and
software
will
depend
on
the
user's
level
of
functional
vision.
Augmentative
and
Alternative
Communication
(AAC).
Augmentative
and
alternative
communication
is
a
well
defined
specialty
within
AT.
It
involves
ways
of
communication
that
either
enhance
or
replace
verbal
language.
When
combined
with
Applied
Behavior
Analysis
(ABA)
teaching
methods,
AAC
has
improved
communication
skills
in
children
with
Autism.
---END.OF.DOCUMENT---
Abacus.
The
abacus,
also
called
a
counting
frame,
is
a
calculating
tool
used
primarily
in
parts
of
Asia
for
performing
arithmetic
processes.
Today,
abacuses
are
often
constructed
as
a
bamboo
frame
with
beads
sliding
on
wires,
but
originally
they
were
beans
or
stones
moved
in
grooves
in
sand
or
on
tablets
of
wood,
stone,
or
metal.
The
abacus
was
in
use
centuries
before
the
adoption
of
the
written
modern
numeral
system
and
is
still
widely
used
by
merchants,
traders
and
clerks
in
Asia,
Africa,
and
elsewhere.
The
user
of
an
abacus
is
called
an
abacist.
Etymology.
The
use
of
the
word
"abacus"
dates
before
1387
AD,
when
a
Middle
English
work
borrowed
the
word
from
Latin
to
describe
a
sandboard
abacus.
The
Latin
word
came
from
"abakos",
the
Greek
genitive
form
of
"abax"
("calculating-table"),
from
Hebrew
"ābāq"
(אבק),
"dust".
The
preferred
plural
of
"abacus"
is
a
subject
of
disagreement,
with
both
"abacuses"
and
"abaci"
in
use.
Mesopotamian
abacus.
The
period
2700–2300
BC
saw
the
first
appearance
of
the
Sumerian
abacus,
a
table
of
successive
columns
which
delimited
the
successive
orders
of
magnitude
of
their
sexagesimal
number
system.
Some
scholars
point
to
a
character
from
the
Babylonian
cuneiform
which
may
have
been
derived
from
a
representation
of
the
abacus.
It
is
the
belief
of
Carruccio
(and
other
Old
Babylonian
scholars)
that
Babylonians
"may
have
used
the
abacus
for
the
operations
of
addition
and
subtraction;
however,
this
primitive
device
proved
difficult
to
use
for
more
complex
calculations".
Egyptian
abacus.
The
use
of
the
abacus
in
Ancient
Egypt
is
mentioned
by
the
Greek
historian
Herodotus,
who
writes
that
the
manner
of
this
disk's
usage
by
the
Egyptians
was
opposite
in
direction
when
compared
with
the
Greek
method.
Archaeologists
have
found
ancient
disks
of
various
sizes
that
are
thought
to
have
been
used
as
counters.
However,
wall
depictions
of
this
instrument
have
not
been
discovered,
casting
some
doubt
over
the
extent
to
which
this
instrument
was
used.
Iranian
Persian
abacus.
During
the
Achaemenid
Persian
Empire,
around
600
BC,
Iranians
first
began
to
use
the
abacus.
Under
Parthian
and
Sassanian
Iranian
empires,
scholars
concentrated
on
exchanging
knowledge
and
inventions
by
the
countries
around
them
–
India,
China,
and
the
Roman
Empire,
when
it
is
thought
to
be
expanded
over
the
other
countries.
Greek
abacus.
The
earliest
archaeological
evidence
for
the
use
of
the
Greek
abacus
dates
to
the
5th
century
BC.
The
Greek
abacus
was
a
table
of
wood
or
marble,
pre-set
with
small
counters
in
wood
or
metal
for
mathematical
calculations.
This
Greek
abacus
saw
use
in
Achaemenid
Persia,
the
Etruscan
civilization,
Ancient
Rome
and,
until
the
French
Revolution,
the
Western
Christian
world.
A
tablet
found
on
the
Greek
island
Salamis
in
1846
AD
dates
back
to
300
BC,
making
it
the
oldest
counting
board
discovered
so
far.
It
is
a
slab
of
white
marble
long,
wide,
and
thick,
on
which
are
5
groups
of
markings.
In
the
center
of
the
tablet
is
a
set
of
5
parallel
lines
equally
divided
by
a
vertical
line,
capped
with
a
semicircle
at
the
intersection
of
the
bottom-most
horizontal
line
and
the
single
vertical
line.
Below
these
lines
is
a
wide
space
with
a
horizontal
crack
dividing
it.
Below
this
crack
is
another
group
of
eleven
parallel
lines,
again
divided
into
two
sections
by
a
line
perpendicular
to
them,
but
with
the
semicircle
at
the
top
of
the
intersection;
the
third,
sixth
and
ninth
of
these
lines
are
marked
with
a
cross
where
they
intersect
with
the
vertical
line.
Roman
abacus.
The
normal
method
of
calculation
in
ancient
Rome,
as
in
Greece,
was
by
moving
counters
on
a
smooth
table.
Originally
pebbles,
calculi,
were
used.
Later,
and
in
medieval
Europe,
jetons
were
manufactured.
Marked
lines
indicated
units,
fives,
tens
etc.
as
in
the
Roman
numeral
system.
This
system
of
'counter
casting'
continued
into
the
late
Roman
empire
and
in
medieval
Europe,
and
persisted
in
limited
use
into
the
nineteenth
century.
Writing
in
the
1st
century
BC,
Horace
refers
to
the
wax
abacus,
a
board
covered
with
a
thin
layer
of
black
wax
on
which
columns
and
figures
were
inscribed
using
a
stylus.
One
example
of
archaeological
evidence
of
the
Roman
abacus,
shown
here
in
reconstruction,
dates
to
the
1st
century
AD.
It
has
eight
long
grooves
containing
up
to
five
beads
in
each
and
eight
shorter
grooves
having
either
one
or
no
beads
in
each.
The
groove
marked
I
indicates
units,
X
tens,
and
so
on
up
to
millions.
The
beads
in
the
shorter
grooves
denote
fives
–five
units,
five
tens
etc.,
essentially
in
a
bi-quinary
coded
decimal
system,
obviously
related
to
the
Roman
numerals.
The
short
grooves
on
the
right
may
have
been
used
for
marking
Roman
ounces.
Chinese
abacus.
The
earliest
known
written
documentation
of
the
Chinese
abacus
dates
to
the
2nd
century
BC.
The
Chinese
abacus,
known
as
the
"suànpán"(算盤,
lit.
"Counting
tray"),
is
typically
tall
and
comes
in
various
widths
depending
on
the
operator.
It
usually
has
more
than
seven
rods.
There
are
two
beads
on
each
rod
in
the
upper
deck
and
five
beads
each
in
the
bottom
for
both
decimal
and
hexadecimal
computation.
The
beads
are
usually
rounded
and
made
of
a
hardwood.
The
beads
are
counted
by
moving
them
up
or
down
towards
the
beam.
If
you
move
them
toward
the
beam,
you
count
their
value.
If
you
move
away,
you
don't
count
their
value.
The
suanpan
can
be
reset
to
the
starting
position
instantly
by
a
quick
jerk
along
the
horizontal
axis
to
spin
all
the
beads
away
from
the
horizontal
beam
at
the
center.
Suanpans
can
be
used
for
functions
other
than
counting.
Unlike
the
simple
counting
board
used
in
elementary
schools,
very
efficient
suanpan
techniques
have
been
developed
to
do
multiplication,
division,
addition,
subtraction,
square
root
and
cube
root
operations
at
high
speed.
There
are
currently
schools
teaching
students
how
to
use
it.
In
the
famous
long
scroll
"Along
the
River
During
the
Qingming
Festival"
painted
by
Zhang
Zeduan
(1085–1145
AD)
during
the
Song
Dynasty
(960–1297
AD),
a
suanpan
is
clearly
seen
lying
beside
an
account
book
and
doctor's
prescriptions
on
the
counter
of
an
apothecary's
(Feibao).
The
similarity
of
the
Roman
abacus
to
the
Chinese
one
suggests
that
one
could
have
inspired
the
other,
as
there
is
some
evidence
of
a
trade
relationship
between
the
Roman
Empire
and
China.
However,
no
direct
connection
can
be
demonstrated,
and
the
similarity
of
the
abaci
may
be
coincidental,
both
ultimately
arising
from
counting
with
five
fingers
per
hand.
Where
the
Roman
model
(like
most
modern
Japanese)
has
4
plus
1
bead
per
decimal
place,
the
standard
suanpan
has
5
plus
2,
allowing
use
with
a
hexadecimal
numeral
system.
Instead
of
running
on
wires
as
in
the
Chinese
and
Japanese
models,
the
beads
of
Roman
model
run
in
grooves,
presumably
making
arithmetic
calculations
much
slower.
Another
possible
source
of
the
suanpan
is
Chinese
counting
rods,
which
operated
with
a
decimal
system
but
lacked
the
concept
of
zero
as
a
place
holder.
The
zero
was
probably
introduced
to
the
Chinese
in
the
Tang
Dynasty
(618-907
AD)
when
travel
in
the
Indian
Ocean
and
the
Middle
East
would
have
provided
direct
contact
with
India,
allowing
them
to
acquire
the
concept
of
zero
and
the
decimal
point
from
Indian
merchants
and
mathematicians.
Indian
abacus.
First
century
sources,
such
as
the
"Abhidharmakosa"
describe
the
knowledge
and
use
of
abacus
in
India.
Around
the
5th
century,
Indian
clerks
were
already
finding
new
ways
of
recording
the
contents
of
the
Abacus.
Hindu
texts
used
the
term
"shunya"
(zero)
to
indicate
the
empty
column
on
the
abacus..
Japanese
abacus.
In
Japanese,
the
abacus
is
called
"soroban"
(,
lit.
"Counting
tray"),
imported
from
China
around
1600.
The
1/4
abacus
appeared
circa
1930,
and
it
is
preferred
and
still
manufactured
in
Japan
today
even
with
the
proliferation,
practicality,
and
affordability
of
pocket
electronic
calculators.
The
use
of
the
soroban
is
still
taught
in
Japanese
primary
schools
as
a
part
of
mathematics.
Korean
abacus.
The
Chinese
abacus
migrated
from
China
to
Korea
around
1400
AD.
Koreans
call
it
"jupan"
(주판),
"supan"
(수판)
or
"jusan"
(주산).
Native
American
abaci.
Some
sources
mention
the
use
of
an
abacus
called
a
"nepohualtzintzin"
in
ancient
Mayan
culture.
This
Mesoamerican
abacus
used
a
5-digit
base-20
system.
The
word
Nepohualtzintzin
comes
from
the
Nahuatl
and
it
is
formed
by
the
roots;
Ne
-
personal
-;
pohual
or
pohualli
-
the
account
-;
and
tzintzin
-
small
similar
elements.
And
its
complete
meaning
is
taken
as:
counting
with
small
similar
elements
by
somebody.
Its
use
was
taught
in
the
"Kalmekak"
to
the
"temalpouhkeh",
who
were
students
dedicated
to
take
the
accounts
of
skies,
from
childhood.
Unfortunately
the
Nepohualtzintzin
and
its
teaching
were
among
the
victims
of
the
conquering
destruction,
when
a
diabolic
origin
was
attributed
to
them
after
observing
the
tremendous
properties
of
representation,
precision
and
speed
of
calculations..
This
arithmetic
tool
is
based
on
the
vigesimal
system
(base
20).
For
the
aztec
the
count
by
20s
was
completely
natural,
since
the
use
of
"huaraches"
(native
sandals)
allowed
them
to
also
use
the
toes
for
their
calculations.
In
this
way,
the
amount
of
20
meant
to
them
a
complete
human
being.
The
Nepohualtzintzin
is
divided
in
two
main
parts
separated
by
a
bar
or
intermediate
cord.
In
the
left
part
there
are
four
beads,
which
in
the
first
row
have
unitary
values
(1,
2,
3,
and
4),
and
in
the
right
side
there
are
three
beads
with
values
of
5,
10,
and
15
respectively.
In
order
to
know
the
value
of
the
respective
beads
of
the
upper
rows,
it
is
enough
to
multiply
by
20
(by
each
row),
the
value
of
the
corresponding
account
in
the
first
row.
Altogether,
there
are
13
rows
with
7
beads
in
each
one,
which
makes
up
91
beads
in
each
Nepohualtzintzin.
This
is
a
basic
number
to
understand
the
close
relation
conceived
between
the
exact
accounts
and
the
natural
phenomena.
This
is
so
that
one
Nepohualtzintzin
(91)
represents
the
number
of
days
that
a
season
of
the
year
lasts,
two
Nepohualtzitzin
(182)
is
the
number
of
days
of
the
corn's
cycle,
from
its
sowing
to
its
harvest,
three
Nepohualtzintzin
(273)
is
the
number
of
days
of
a
baby's
gestation,
and
four
Nepohualtzintzin
(364)
complete
a
cycle
and
approximate
a
year
(1
1/4
days
short).
It
is
worth
to
mention
that
in
the
Nepohualtzintzin,
amounts
in
the
rank
from
10
to
the
18
can
be
calculated,
with
floating
point,
which
allows
calculating
stellar
as
well
as
infinitesimal
amounts
with
absolute
precision.
The
rediscovering
of
the
Nepohualtzintzin
is
due
to
the
teacher
David
Esparza
Hidalgo,
who
in
his
wandering
by
all
Mexico
has
found
diverse
engravings
and
paintings
of
this
instrument
and
has
reconstructed
several
of
them
made
in
gold,
jade,
incrustations
of
shell,
etc.
There
have
also
been
found
very
old
Nepohualtzintzin
attributed
to
the
Olmeca
culture,
and
even
some
bracelets
of
Mayan
origin,
as
well
as
a
diversity
of
forms
and
materials
in
other
cultures.
The
quipu
of
the
Incas
was
a
system
of
knotted
cords
used
to
record
numerical
data,
like
advanced
tally
sticks
–
but
not
used
to
perform
calculations.
Calculations
were
carried
out
using
a
yupana
(quechua
for
"counting
tool";
see
figure)
which
was
still
in
use
after
the
conquest
of
Peru.
The
working
principle
of
a
yupana
is
unknown,
but
in
2001
an
explanation
of
the
mathematical
basis
of
these
instruments
was
proposed.
By
comparing
the
form
of
several
yupanas,
researchers
found
that
calculations
were
based
using
the
Fibonacci
sequence
1,
1,
2,
3,
5
and
powers
of
10,
20
and
40
as
place
values
for
the
different
fields
in
the
instrument.
Using
the
Fibonacci
sequence
would
keep
the
number
of
grains
within
any
one
field
at
minimum.
Russian
abacus.
The
Russian
abacus,
the
"schety"
(счёты),
usually
has
a
single
slanted
deck,
with
ten
beads
on
each
wire
(except
one
wire
which
has
four
beads,
for
quarter-ruble
fractions.
This
wire
is
usually
near
the
user).
(Older
models
have
another
4-bead
wire
for
quarter-kopeks,
which
were
minted
until
1916.)
The
Russian
abacus
is
often
used
vertically,
with
wires
from
left
to
right
in
the
manner
of
a
book.
The
wires
are
usually
bowed
to
bulge
upward
in
the
center,
to
keep
the
beads
pinned
to
either
of
the
two
sides.
It
is
cleared
when
all
the
beads
are
moved
to
the
right.
During
manipulation,
beads
are
moved
to
the
left.
For
easy
viewing,
the
middle
2
beads
on
each
wire
(the
5th
and
6th
bead)
usually
are
of
a
different
colour
from
the
other
eight
beads.
Likewise,
the
left
bead
of
the
thousands
wire
(and
the
million
wire,
if
present)
may
have
a
different
color.
As
a
simple,
cheap
and
reliable
device,
the
Russian
abacus
was
in
use
in
all
shops
and
markets
throughout
the
former
Soviet
Union,
and
the
usage
of
it
was
taught
in
most
schools
until
the
1990s.
Even
the
1874
invention
of
mechanical
calculator,
Odhner
arithmometer,
had
not
replaced
them
in
Russia
and
likewise
the
mass
production
of
Felix
arithmometers
since
1924
did
not
significantly
reduce
their
use
in
the
Soviet
Union.
Russian
abacus
began
to
lose
popularity
only
after
the
mass
production
of
microcalculators
had
started
in
the
Soviet
Union
in
1974.
On
Today
it
is
regarded
as
an
archaism
and
replaced
by
microcalculator.
The
Russian
abacus
was
brought
to
France
around
1820
by
the
mathematician
Jean-Victor
Poncelet,
who
served
in
Napoleon's
army
and
had
been
a
prisoner
of
war
in
Russia.
The
abacus
had
fallen
out
of
use
in
western
Europe
in
the
16th
century
with
the
rise
of
decimal
notation
and
algorismic
methods.
To
Poncelet's
French
contemporaries,
it
was
something
new.
Poncelet
used
it,
not
for
any
applied
purpose,
but
as
a
teaching
and
demonstration
aid.
School
abacus.
Around
the
world,
abaci
have
been
used
in
pre-schools
and
elementary
schools
as
an
aid
in
teaching
the
numeral
system
and
arithmetic.
In
Western
countries,
a
bead
frame
similar
to
the
Russian
abacus
but
with
straight
wires
and
a
vertical
frame
has
been
common
(see
image).
It
is
still
often
seen
as
a
plastic
or
wooden
toy.
The
type
of
abacus
shown
here
is
often
used
to
represent
numbers
without
the
use
of
place
value.
Each
bead
and
each
wire
has
the
same
value
and
used
in
this
way
it
can
represent
numbers
up
to
100.
Uses
by
the
blind.
An
adapted
abacus,
invented
by
Tim
Cranmer,
called
a
Cranmer
abacus
is
still
commonly
used
by
individuals
who
are
blind.
A
piece
of
soft
fabric
or
rubber
is
placed
behind
the
beads
so
that
they
do
not
move
inadvertently.
This
keeps
the
beads
in
place
while
the
users
feel
or
manipulate
them.
They
use
an
abacus
to
perform
the
mathematical
functions
multiplication,
division,
addition,
subtraction,
square
root
and
cubic
root.
Although
blind
students
have
benefited
from
talking
calculators,
the
abacus
is
still
very
often
taught
to
these
students
in
early
grades,
both
in
public
schools
and
state
schools
for
the
blind.
The
abacus
teaches
mathematical
skills
that
can
never
be
replaced
with
talking
calculators
and
is
an
important
learning
tool
for
blind
students.
Blind
students
also
complete
mathematical
assignments
using
a
braille-writer
and
Nemeth
code
(a
type
of
braille
code
for
mathematics)
but
large
multiplication
and
long
division
problems
can
be
long
and
difficult.
The
abacus
gives
blind
and
visually
impaired
students
a
tool
to
compute
mathematical
problems
that
equals
the
speed
and
mathematical
knowledge
required
by
their
sighted
peers
using
pencil
and
paper.
Many
blind
people
find
this
number
machine
a
very
useful
tool
throughout
life.
Binary
Abacus.
An
abacus
that
explains
how
computers
manipulate
numbers.
The
abacus
shows
how
numbers,
letters,
and
signs
can
be
stored
in
a
binary
system
on
a
computer,
or
via
ASCII.
The
device
consists
of
a
series
of
beads
on
parallel
wires
arranged
in
three
separate
rows.
The
beads
represent
a
switch
on
the
computer
in
either
an
'on'
or
'off'
position.
In
1985,
Dr.
Robert
C.
Good,
Jr.
of
the
Widener
University
School
of
Engineering
published
on
the
binary
abacus.
---END.OF.DOCUMENT---
Acid.
An
acid
(...from
the
Latin
"acidus"
meaning
"sour")
is
traditionally
considered
any
chemical
compound
that,
when
dissolved
in
water,
gives
a
solution
with
a
hydrogen
ion
activity
greater
than
in
pure
water,
i.e.
a
pH
less
than
7.0
in
its
standard
state.
That
approximates
the
modern
definition
of
Johannes
Nicolaus
Brønsted
and
Martin
Lowry,
who
independently
defined
an
acid
as
a
compound
which
donates
a
hydrogen
ion
(H+)
to
another
compound
(called
a
base).
Common
examples
include
acetic
acid
(in
vinegar)
and
sulfuric
acid
(used
in
car
batteries).
Acid/base
systems
are
different
from
redox
reactions
in
that
there
is
no
change
in
oxidation
state.
Acids
can
occur
in
solid,
liquid
or
gaseous
form,
depending
on
the
temperature.
They
can
exist
as
pure
substances
or
in
solution.
Chemicals
or
substances
having
the
property
of
an
acid
are
said
to
be
acidic.
Arrhenius
acids.
In
pure
water
the
majority
of
molecules
exist
as
H2O,
but
a
small
number
of
molecules
are
constantly
dissociating
and
re-associating.
Pure
water
is
neutral
with
respect
to
acidity
or
basicity
because
the
concentration
of
hydroxide
ions
is
always
equal
to
the
concentration
of
hydronium
ions.
An
Arrhenius
base
is
a
molecule
which
increases
the
concentration
of
the
hydroxide
ion
when
dissolved
in
water.
Note
that
chemists
often
write
H+("aq")
and
refer
to
the
hydrogen
ion
when
describing
acid-base
reactions
but
the
free
hydrogen
nucleus,
a
proton,
does
not
exist
alone
in
water,
it
exists
as
the
hydronium
ion,
H3O+.
Brønsted
acids.
As
with
the
acetic
acid
reactions,
both
definitions
work
for
the
first
example,
where
water
is
the
solvent
and
hydronium
ion
is
formed.
The
next
two
reactions
do
not
involve
the
formation
of
ions
but
can
still
be
viewed
as
proton
transfer
reactions.
In
the
second
reaction
hydrogen
chloride
and
ammonia
(dissolved
in
benzene)
react
to
form
solid
ammonium
chloride
in
a
benzene
solvent
and
in
the
third
gaseous
HCl
and
NH3
combine
to
form
the
solid.
Lewis
acids.
A
third
concept
was
proposed
by
Gilbert
N.
Lewis
which
includes
reactions
with
acid-base
characteristics
that
do
not
involve
a
proton
transfer.
A
Lewis
acid
is
a
species
that
accepts
a
pair
of
electrons
from
another
species;
in
other
words,
it
is
an
electron
pair
acceptor.
Brønsted
acid-base
reactions
are
proton
transfer
reactions
while
Lewis
acid-base
reactions
are
electron
pair
transfers.
All
Brønsted
acids
are
also
Lewis
acids,
but
not
all
Lewis
acids
are
Brønsted
acids.
Contrast
the
following
reactions
which
could
be
described
in
terms
of
acid-base
chemistry.
In
the
first
reaction
a
fluoride
ion,
F-,
gives
up
an
electron
pair
to
boron
trifluoride
to
form
the
product
tetrafluoroborate.
Fluoride
"loses"
a
pair
of
valence
electrons
because
the
electrons
shared
in
the
B—F
bond
are
located
in
the
region
of
space
between
the
two
atomic
nuclei
and
are
therefore
more
distant
from
the
fluoride
nucleus
than
they
are
in
the
lone
fluoride
ion.
BF3
is
a
Lewis
acid
because
it
accepts
the
electron
pair
from
fluoride.
This
reaction
cannot
be
described
in
terms
of
Brønsted
theory
because
there
is
no
proton
transfer.
The
second
reaction
can
be
described
using
either
theory.
A
proton
is
transferred
from
an
unspecified
Brønsted
acid
to
ammonia,
a
Brønsted
base;
alternatively,
ammonia
acts
as
a
Lewis
base
and
transfers
a
lone
pair
of
electrons
to
form
a
bond
with
a
hydrogen
ion.
The
species
that
gains
the
electron
pair
is
the
Lewis
acid;
for
example,
the
oxygen
atom
in
H3O+
gains
a
pair
of
electrons
when
one
of
the
H—O
bonds
is
broken
and
the
electrons
shared
in
the
bond
become
localized
on
oxygen.
Depending
on
the
context,
a
Lewis
acid
may
also
be
described
as
an
oxidizer
or
an
electrophile.
The
Brønsted-Lowry
definition
is
the
most
widely
used
definition;
unless
otherwise
specified
acid-base
reactions
are
assumed
to
involve
the
transfer
of
a
proton
(H+)
from
an
acid
to
a
base.
Dissociation
and
equilibrium.
Reactions
of
acids
are
often
generalized
in
the
form
HA
H+
+
A-,
where
HA
represents
the
acid
and
A-
is
the
conjugate
base.
Acid-base
conjugate
pairs
differ
by
one
proton,
and
can
be
interconverted
by
the
addition
or
removal
of
a
proton
(protonation
and
deprotonation,
respectively).
Note
that
the
acid
can
be
the
charged
species
and
the
conjugate
base
can
be
neutral
in
which
case
the
generalized
reaction
scheme
could
be
written
as
HA+
H+
+
A.
In
solution
there
exists
an
equilibrium
between
the
acid
and
its
conjugate
base.
The
equilibrium
constant
"K"
is
an
expression
of
the
equilibrium
concentrations
of
the
molecules
or
the
ions
in
solution.
Brackets
indicate
concentration,
such
that
[H2O]
means
"the
concentration
of
H2O".
The
acid
dissociation
constant
"K"a
is
generally
used
in
the
context
of
acid-base
reactions.
The
numerical
value
of
"K"a
is
equal
to
the
concentration
of
the
products
divided
by
the
concentration
of
the
reactants,
where
the
reactant
is
the
acid
(HA)
and
the
products
are
the
conjugate
base
and
H+.
The
stronger
of
two
acids
will
have
a
higher
"K"a
than
the
weaker
acid;
the
ratio
of
hydrogen
ions
to
acid
will
be
higher
for
the
stronger
acid
as
the
stronger
acid
has
a
greater
tendency
to
lose
its
proton.
Because
the
range
of
possible
values
for
"K"a
spans
many
orders
of
magnitude,
a
more
manageable
constant,
p"K"a
is
more
frequently
used,
where
p"K"a
=
-log10
"K"a.
Stronger
acids
have
a
smaller
p"K"a
than
weaker
acids.
Experimentally
determined
p"K"a
at
25°C
in
aqueous
solution
are
often
quoted
in
textbooks
and
reference
material.
Nomenclature.
In
the
classical
naming
system,
acids
are
named
according
to
their
anions.
That
ionic
suffix
is
dropped
and
replaced
with
a
new
suffix
(and
sometimes
prefix),
according
to
the
table
below.
For
example,
HCl
has
chloride
as
its
anion,
so
the
-ide
suffix
makes
it
take
the
form
hydrochloric
acid.
In
the
IUPAC
naming
system,
"aqueous"
is
simply
added
to
the
name
of
the
ionic
compound.
Thus,
for
hydrogen
chloride,
the
IUPAC
name
would
be
aqueous
hydrogen
chloride.
The
prefix
"hydro-"
is
added
only
if
the
acid
is
made
up
of
just
hydrogen
and
one
other
element.
Acid
strength.
The
strength
of
an
acid
refers
to
its
ability
or
tendency
to
lose
a
proton.
A
strong
acid
is
one
that
completely
dissociates
in
water;
in
other
words,
one
mole
of
a
strong
acid
HA
dissolves
in
water
yielding
one
mole
of
H+
and
one
mole
of
the
conjugate
base,
A-,
and
none
of
the
protonated
acid
HA.
In
contrast
a
weak
acid
only
partially
dissociates
and
at
equilibrium
both
the
acid
and
the
conjugate
base
are
in
solution.
Examples
of
strong
acids
are
hydrochloric
acid
(HCl),
hydroiodic
acid
(HI),
hydrobromic
acid
(HBr),
perchloric
acid
(HClO4),
nitric
acid
(HNO3)
and
sulfuric
acid
(H2SO4).
In
water
each
of
these
essentially
ionizes
100%.
The
stronger
an
acid
is,
the
more
easily
it
loses
a
proton,
H+.
Two
key
factors
that
contribute
to
the
ease
of
deprotonation
are
the
polarity
of
the
H—A
bond
and
the
size
of
atom
A,
which
determines
the
strength
of
the
H—A
bond.
Acid
strengths
are
also
often
discussed
in
terms
of
the
stability
of
the
conjugate
base.
Stronger
acids
have
a
higher
"K"a
and
a
lower
p"K"a
than
weaker
acids.
Sulfonic
acids,
which
are
organic
oxyacids,
are
a
class
of
strong
acids.
A
common
example
is
toluenesulfonic
acid
(tosylic
acid).
Unlike
sulfuric
acid
itself,
sulfonic
acids
can
be
solids.
In
fact,
polystyrene
functionalized
into
polystyrene
sulfonate
is
a
solid
strongly
acidic
plastic
that
is
filterable.
Superacids
are
acids
stronger
than
100%
sulfuric
acid.
Examples
of
superacids
are
fluoroantimonic
acid,
magic
acid
and
perchloric
acid.
Superacids
can
permanently
protonate
water
to
give
ionic,
crystalline
hydronium
"salts".
They
can
also
quantitatively
stabilize
carbocations.
Polarity
and
the
inductive
effect.
The
electronegative
element
need
not
be
directly
bonded
to
the
acidic
hydrogen
to
increase
its
acidity.
An
electronegative
atom
can
pull
electron
density
out
of
an
acidic
bond
through
the
inductive
effect.
The
electron-withdrawing
ability
diminishes
quickly
as
the
electronegative
atom
moves
away
from
the
acidic
bond.
The
effect
is
illustrated
by
the
following
series
of
halogenated
butanoic
acids.
Chlorine
is
more
electronegative
than
bromine
and
therefore
has
a
stronger
effect.
The
hydrogen
atom
bonded
to
the
oxygen
is
the
acidic
hydrogen.
Butanoic
acid
is
a
carboxylic
acid.
As
the
chlorine
atom
moves
further
away
from
the
acidic
O—H
bond,
its
effect
diminishes.
When
the
chlorine
atom
is
just
one
carbon
removed
from
the
carboxylic
acid
group
the
acidity
of
the
compound
increases
significantly,
compared
to
butanoic
acid
(a.k.a.
butyric
acid).
However,
when
the
chlorine
atom
is
separated
by
several
bonds
the
effect
is
much
smaller.
Bromine
is
much
more
electronegative
than
either
carbon
or
hydrogen,
but
not
as
electronegative
as
chlorine,
so
the
p"K"a
of
2-bromobutanoic
acid
is
slightly
greater
than
the
p"K"a
of
2-chlorobutanoic
acid.
The
number
of
electronegative
atoms
adjacent
an
acidic
bond
also
affects
acid
strength.
Oxoacids
have
the
general
formula
HOX
where
X
can
be
any
atom
and
may
or
may
not
share
bonds
to
other
atoms.
Increasing
the
number
of
electronegative
atoms
or
groups
on
atom
X
decreases
the
electron
density
in
the
acidic
bond,
making
the
loss
of
the
proton
easier.
Perchloric
acid
is
a
very
strong
acid
(p"K"a
≈
-8)
and
completely
dissociates
in
water.
Its
chemical
formula
is
HClO4
and
it
comprises
a
central
chlorine
atom
with
three
chlorine-oxygen
double
bonds
(Cl=O)
and
one
chlorine-oxygen
single
bond
(Cl—O).
The
singly
bonded
oxygen
bears
an
extremely
acidic
hydrogen
atom
which
is
easily
abstracted.
In
contrast,
chloric
acid
(HClO3)
is
a
weaker
acid,
though
still
quite
strong
(p"K"a
=
-1.0),
while
chlorous
acid
(HClO2,
p"K"a
=
+2.0)
and
hypochlorous
acid
(HClO,
p"K"a
=
+7.53)
acids
are
weak
acids.
Carboxylic
acids
are
organic
acids
that
contain
an
acidic
hydroxyl
group
and
a
carbonyl
(C=O
bond).
Carboxylic
acids
can
be
reduced
to
the
corresponding
alcohol;
the
replacement
of
an
electronegative
oxygen
atom
with
two
electropositive
hydrogens
yields
a
product
which
is
essentially
non-acidic.
The
reduction
of
acetic
acid
to
ethanol
using
LiAlH4
(lithium
aluminium
hydride
or
LAH)
and
ether
is
an
example
of
such
a
reaction.
The
p"K"a
for
ethanol
is
16,
compared
to
4.76
for
acetic
acid.
Atomic
radius
and
bond
strength.
Another
factor
that
contributes
to
the
ability
of
an
acid
to
lose
a
proton
is
the
strength
of
the
bond
between
the
acidic
hydrogen
and
the
atom
that
bears
it.
This,
in
turn,
is
dependent
on
the
size
of
the
atoms
sharing
the
bond.
For
an
acid
HA,
as
the
size
of
atom
A
increases,
the
strength
of
the
bond
decreases,
meaning
that
it
is
more
easily
broken,
and
the
strength
of
the
acid
increases.
Bond
strength
is
a
measure
of
how
much
energy
it
takes
to
break
a
bond.
In
other
words,
it
takes
less
energy
to
break
the
bond
as
atom
A
grows
larger,
and
the
proton
is
more
easily
removed
by
a
base.
This
partially
explains
why
hydrofluoric
acid
is
considered
a
weak
acid
while
the
other
hydrohalic
acids
(HCl,
HBr,
HI)
are
strong
acids.
Although
fluorine
is
more
electronegative
than
the
other
halogens,
its
atomic
radius
is
also
much
smaller,
so
it
shares
a
stronger
bond
with
hydrogen.
Moving
down
a
column
on
the
periodic
table
atoms
become
less
electronegative
but
also
significantly
larger,
and
the
size
of
the
atom
tends
to
dominate
its
acidity
when
sharing
a
bond
to
hydrogen.
Hydrogen
sulfide,
H2S,
is
a
stronger
acid
than
water,
even
though
oxygen
is
more
electronegative
than
sulfur.
Just
as
with
the
halogens,
this
is
because
sulfur
is
larger
than
oxygen
and
the
H—S
bond
is
more
easily
broken
than
the
H—O
bond.
Monoprotic
acids.
Common
examples
of
monoprotic
acids
in
mineral
acids
include
hydrochloric
acid
(HCl)
and
nitric
acid
(HNO3).
On
the
other
hand,
for
organic
acids
the
term
mainly
indicates
the
presence
of
one
carboxyl
group
and
sometimes
these
acids
are
known
as
monocarboxylic
acid.
Examples
in
organic
acids
include
formic
acid
(HCOOH),
acetic
acid
(CH3COOH)
and
benzoic
acid
(C6H5COOH).
Polyprotic
acids.
Polyprotic
acids
are
able
to
donate
more
than
one
proton
per
acid
molecule,
in
contrast
to
monoprotic
acids
that
only
donate
one
proton
per
molecule.
Specific
types
of
polyprotic
acids
have
more
specific
names,
such
as
diprotic
acid
(two
potential
protons
to
donate)
and
triprotic
acid
(three
potential
protons
to
donate).
A
diprotic
acid
(here
symbolized
by
H2A)
can
undergo
one
or
two
dissociations
depending
on
the
pH.
Each
dissociation
has
its
own
dissociation
constant,
Ka1
and
Ka2.
The
first
dissociation
constant
is
typically
greater
than
the
second;
i.e.,
"K"a1
>
"K"a2.
For
example,
sulfuric
acid
(H2SO4)
can
donate
one
proton
to
form
the
bisulfate
anion
(HSO4-),
for
which
"K"a1
is
very
large;
then
it
can
donate
a
second
proton
to
form
the
sulfate
anion
(SO42-),
wherein
the
"K"a2
is
intermediate
strength.
The
large
"K"a1
for
the
first
dissociation
makes
sulfuric
a
strong
acid.
In
a
similar
manner,
the
weak
unstable
carbonic
acid
(H2CO3)
can
lose
one
proton
to
form
bicarbonate
anion
(HCO3-)
and
lose
a
second
to
form
carbonate
anion
(CO32-).
Both
"K"a
values
are
small,
but
"K"a1
>
"K"a2.
A
triprotic
acid
(H3A)
can
undergo
one,
two,
or
three
dissociations
and
has
three
dissociation
constants,
where
"K"a1
>
"K"a2
>
"K"a3.
An
inorganic
example
of
a
triprotic
acid
is
orthophosphoric
acid
(H3PO4),
usually
just
called
phosphoric
acid.
All
three
protons
can
be
successively
lost
to
yield
H2PO4-,
then
HPO42-,
and
finally
PO43-,
the
orthophosphate
ion,
usually
just
called
phosphate.
An
organic
example
of
a
triprotic
acid
is
citric
acid,
which
can
successively
lose
three
protons
to
finally
form
the
citrate
ion.
Even
though
the
positions
of
the
protons
on
the
original
molecule
may
be
equivalent,
the
successive
"K"a
values
will
differ
since
it
is
energetically
less
favorable
to
lose
a
proton
if
the
conjugate
base
is
more
negatively
charged.
Neutralization.
Neutralization
is
the
basis
of
titration,
where
a
pH
indicator
shows
equivalence
point
when
the
equivalent
number
of
moles
of
a
base
have
been
added
to
an
acid.
It
is
often
wrongly
assumed
that
neutralization
should
result
in
a
solution
with
pH
7.0,
which
is
only
the
case
with
similar
acid
and
base
strengths
during
a
reaction.
Neutralization
with
a
base
weaker
than
the
acid
results
in
a
weakly
acidic
salt.
An
example
is
the
weakly
acidic
ammonium
chloride,
which
is
produced
from
the
strong
acid
hydrogen
chloride
and
the
weak
base
ammonia.
Conversely,
neutralizing
a
weak
acid
with
a
strong
base
gives
a
weakly
basic
salt,
e.g.
sodium
fluoride
from
hydrogen
fluoride
and
sodium
hydroxide.
Weak
acid/weak
base
equilibria.
In
order
to
lose
a
proton,
it
is
necessary
that
the
pH
of
the
system
rise
above
the
p"K"a
of
the
protonated
acid.
The
decreased
concentration
of
H+
in
that
basic
solution
shifts
the
equilibrium
towards
the
conjugate
base
form
(the
deprotonated
form
of
the
acid).
In
lower-pH
(more
acidic)
solutions,
there
is
a
high
enough
H+
concentration
in
the
solution
to
cause
the
acid
to
remain
in
its
protonated
form,
or
to
protonate
its
conjugate
base
(the
deprotonated
form).
Solutions
of
weak
acids
and
salts
of
their
conjugate
bases
form
buffer
solutions.
Applications
of
acids.
There
are
numerous
uses
for
acids.
Acids
are
often
used
to
remove
rust
and
other
corrosion
from
metals
in
a
process
known
as
pickling.
They
may
be
used
as
an
electrolyte
in
a
wet
cell
battery,
such
as
sulfuric
acid
in
a
car
battery.
Strong
acids,
sulfuric
acid
in
particular,
are
widely
used
in
mineral
processing.
For
example,
phosphate
minerals
react
with
sulfuric
acid
to
produce
phosphoric
acid
for
the
production
of
phosphate
fertilizers,
and
zinc
is
produced
by
dissolving
zinc
oxide
into
sulfuric
acid,
purifying
the
solution
and
electrowinning.
In
the
chemical
industry,
acids
react
in
neutralization
reactions
to
produce
salts.
For
example,
nitric
acid
reacts
with
ammonia
to
produce
ammonium
nitrate,
a
fertilizer.
Additionally,
carboxylic
acids
can
be
esterified
with
alcohols,
to
produce
esters.
Acids
are
used
as
catalysts;
for
example,
sulfuric
acid
is
used
in
very
large
quantities
in
the
alkylation
process
to
produce
gasoline.
Strong
acids,
such
as
sulfuric,
phosphoric
and
hydrochloric
acids
also
effect
dehydration
and
condensation
reactions.
Acids
are
used
as
additives
to
drinks
and
foods,
as
they
alter
their
taste
and
serve
as
preservatives.
Phosphoric
acid,
for
example,
is
a
component
of
cola
drinks.
Biological
occurrence.
Many
biologically
important
molecules
are
acids.
Nucleic
acids,
including
DNA
and
RNA
contain
the
genetic
code
that
determines
much
of
an
organism's
characteristics,
and
is
passed
from
parents
to
offspring.
DNA
contains
the
chemical
blueprint
for
the
synthesis
of
proteins
which
are
made
up
of
amino
acid
subunits.
Cell
membranes
contain
fatty
acid
esters
such
as
phospholipids.
An
α-amino
acid
has
a
central
carbon
(the
α
or
"alpha"
carbon)
which
is
covalently
bonded
to
a
carboxyl
group
(thus
they
are
carboxylic
acids),
an
amino
group,
a
hydrogen
atom
and
a
variable
group.
The
variable
group,
also
called
the
R
group
or
side
chain,
determines
the
identity
and
many
of
the
properties
of
the
a
specific
amino
acid.
In
glycine,
the
simplest
amino
acid,
the
R
group
is
a
hydrogen
atom,
but
in
all
other
amino
acids
it
is
contains
one
or
more
carbon
atoms
bonded
to
hydrogens,
and
may
contain
other
elements
such
as
sulfur,
oxygen
or
nitrogen.
With
the
exception
of
glycine,
naturally
occurring
amino
acids
are
chiral
and
almost
invariably
occur
in
the
L-configuration.
Peptidoglycan,
found
in
some
bacterial
cell
walls
contains
some
D-amino
acids.
At
physiologic
pH,
typically
around
7,
free
fatty
acids
exist
in
a
charged
form,
where
the
acidic
carboxyl
group
(-COOH)
loses
a
proton
(-COO-)
and
the
basic
amine
group
(-NH2)
gains
a
proton
(-NH3+).
The
entire
molecule
has
a
net
neutral
charge
and
is
a
zwitterion.
Fatty
acids
and
fatty
acid
derivatives
are
another
group
of
carboxylic
acids
that
play
a
significant
role
in
biology.
These
contain
long
hydrocarbon
chains
and
a
carboxylic
acid
group
on
one
end.
The
cell
membrane
of
nearly
all
organisms
is
primarily
made
up
of
a
phospholipid
bilayer,
a
micelle
of
hydrophobic
fatty
acid
esters
with
polar,
hydrophilic
phosphate
"head"
groups.
Membranes
contain
additional
components,
some
of
which
can
participate
in
acid-base
reactions.
In
humans
and
many
other
animals,
hydrochloric
acid
is
a
part
of
the
gastric
acid
secreted
within
the
stomach
to
help
hydrolyze
proteins
and
polysaccharides,
as
well
as
converting
the
inactive
pro-enzyme,
pepsinogen
into
the
enzyme,
pepsin.
Some
organisms
produce
acids
for
defense;
for
example,
ants
produce
formic
acid.
Acid-base
equilibrium
plays
a
critical
role
in
regulating
mammalian
breathing.
Oxygen
gas
(O2)
drives
cellular
respiration,
the
process
by
which
animals
release
the
chemical
potential
energy
stored
in
food,
producing
carbon
dioxide
(CO2)
as
a
byproduct.
Oxygen
and
carbon
dioxide
are
exchanged
in
the
lungs,
and
the
body
responds
to
changing
energy
demands
by
adjusting
the
rate
of
ventilation.
For
example,
during
periods
of
exertion
the
body
rapidly
breaks
down
stored
carbohydrates
and
fat,
releasing
CO2
into
the
blood
stream.
In
aqueous
solutions
such
as
blood
CO2
exists
in
equilibrium
with
carbonic
acid
and
bicarbonate
ion.
It
is
the
decrease
in
pH
that
signals
the
brain
to
breath
faster
and
deeper,
expelling
the
excess
CO2
and
resupplying
the
cells
with
O2.
Cell
membranes
are
generally
impermeable
to
charged
or
large,
polar
molecules
because
of
the
lipophilic
fatty
acyl
chains
comprising
their
interior.
Many
biologically
important
molecules,
including
a
number
of
pharmaceutical
agents,
are
organic
weak
acids
which
can
cross
the
membrane
in
their
protonated,
uncharged
form
but
not
in
their
charged
form
(i.e.
as
the
conjugate
base).
For
this
reason
the
activity
of
many
drugs
can
be
enhanced
or
inhibited
by
the
use
of
antacids
or
acidic
foods.
The
charged
form,
however,
is
often
more
soluble
in
blood
and
cytosol,
both
aqueous
environments.
When
the
extracellular
environment
is
more
acidic
than
the
neutral
pH
within
the
cell,
certain
acids
will
exist
in
their
neutral
form
and
will
be
membrane
soluble,
allowing
them
to
cross
the
phospholipid
bilayer.
Acids
that
lose
a
proton
at
the
intracellular
pH
will
exist
in
their
soluble,
charged
form
and
are
thus
able
to
diffuse
through
the
cytosol
to
their
target.
Ibuprofen,
aspirin
and
penicillin
are
examples
of
drugs
that
are
weak
acids.
---END.OF.DOCUMENT---
Asphalt.
Asphalt
()
is
a
sticky,
black
and
highly
viscous
liquid
or
semi-solid
that
is
present
in
most
crude
petroleums
and
in
some
natural
deposits
sometimes
termed
asphaltum.
It
is
most
commonly
modelled
as
a
colloid,
with
"asphaltenes"
as
the
dispersed
phase
and
'
as
the
continuous
phase
(though
there
is
some
disagreement
amongst
chemists
regarding
its
structure).
One
writer
states
that
although
a
"considerable
amount
of
work
has
been
done
on
the
composition
of
asphalt,
it
is
exceedingly
difficult
to
separate
individual
hydrocarbon
in
pure
form",
and
"it
is
almost
impossible
to
separate
and
identify
all
the
different
molecules
of
asphalt,
because
the
number
of
molecules
with
different
chemical
structure
is
extremely
large".
In
U.S.
and
Polish
terminology,
asphalt
(or
asphalt
cement)
is
the
carefully
refined
residue
from
the
distillation
process
of
selected
crude
oils.
Outside
these
countries,
the
product
is
often
called
bitumen.
The
primary
use
of
asphalt
is
in
road
construction,
where
it
is
used
as
the
glue
or
binder
for
the
aggregate
particles.
The
road
surfacing
material
is
usually
called
'asphaltic
concrete',
AC
in
North
America,
or
'asphalt'
elsewhere.
Within
North
America
the
apparent
interchangeability
of
the
words
asphalt
and
'bitumen'
causes
confusion
outside
the
road
construction
industry
despite
quite
clear
definitions
within
industry
circles.
Etymology.
The
word
asphalt
is
derived
from
the
late
Middle
English:
from
French
asphalte,
based
on
Late
Latin
asphalton,
asphaltum,
from
the
Greek
ásphalton,
ásphaltos
("άσφαλτος"),
a
word
of
uncertain
origin
meaning
"asphalt/bitumen/pitch"
which
some
derive
from
α-
"without"
and
σφάλλω
"to
make
fall".
Note
that
in
French,
the
term
asphalte
is
used
for
naturally-occurring
bitumen-soaked
limestone
deposits,
and
for
specialised
manufactured
products
with
fewer
voids
or
greater
bitumen
content
than
the
"asphaltic
concrete"
used
to
pave
roads.
Another
description
has
it
that
the
term
derives
from
the
Accadian
term
"asphaltu"
or
"sphallo,"
meaning
"to
split."
It
was
later
adopted
from
the
Homeric
Greeks
as
a
verb
meaning
"to
make
firm
or
stable,"
"to
secure".
It
is
a
significant
fact
that
the
first
use
of
asphalt
by
the
ancients
was
in
the
nature
of
a
cement
for
securing
or
joining
together
various
objects,
and
it
thus
seems
likely
that
the
name
itself
was
expressive
of
this
application.
From
the
Greek,
the
word
passed
into
late
Latin,
and
thence
into
French
("asphalte")
and
English
("asphalt").
The
expression
"bitumen"
originated
in
the
Sanskrit,
where
we
find
the
words
"jatu,"
meaning
"pitch,"
and
"jatu-krit,"
meaning
"pitch
creating,"
"pitch
producing"
(referring
to
coniferous
or
resinous
trees).
The
Latin
equivalent
is
claimed
by
some
to
be
originally
'gwitu-men'
(pertaining
to
pitch),
and
by
others,
"pixtumens"
(exuding
or
bubbling
pitch),
which
was
subsequently
shortened
to
"bitumen,"
thence
passing
via
French
into
English.
From
the
same
root
is
derived
the
Anglo
Saxon
word
"cwidu"
(Mastix),
the
German
word
"Kitt"
(cement
or
mastic)
and
the
old
Norse
word
"kvada".
Background.
Asphalt
or
bitumen
can
sometimes
be
confused
with
tar,
which
is
a
similar
black
thermo-plastic
material
produced
by
the
destructive
distillation
of
coal.
During
the
early-
and
mid-twentieth
century
when
town
gas
was
produced,
tar
was
a
readily
available
product
and
extensively
used
as
the
binder
for
road
aggregates.
The
addition
of
tar
to
macadam
roads
led
to
the
word
tarmac,
which
is
now
used
in
common
parlance
to
refer
to
road
making
materials.
However,
since
the
1970s,
when
natural
gas
succeeded
town
gas,
asphalt
(bitumen)
has
completely
overtaken
the
use
of
tar
in
these
applications.
Asphalt
can
be
separated
from
the
other
components
in
crude
oil
(such
as
naphtha,
gasoline
and
diesel)
by
the
process
of
fractional
distillation,
usually
under
vacuum
conditions.
A
better
separation
can
be
achieved
by
further
processing
of
the
heavier
fractions
of
the
crude
oil
in
a
de-asphalting
unit,
which
uses
either
propane
or
butane
in
a
supercritical
phase
to
dissolve
the
lighter
molecules
which
are
then
separated.
Further
processing
is
possible
by
"blowing"
the
product:
namely
reacting
it
with
oxygen.
This
makes
the
product
harder
and
more
viscous.
Natural
deposits
of
asphalt
include
lake
asphalts
(primarily
from
the
Pitch
Lake
in
Trinidad
and
Tobago
and
Bermudez
Lake
in
Venezuela),
Gilsonite,
the
Dead
Sea,
and
Tar
Sands.
Asphalt
was
mined
at
Ritchie
Mines
in
Macfarlan
in
Ritchie
County,
West
Virginia
in
the
United
States
from
1852
to
1873.
Asphalt
is
typically
stored
and
transported
at
temperatures
around
150
degrees
Celsius
(300
°F).
Sometimes
diesel
oil
or
kerosene
are
mixed
in
before
shipping
to
retain
liquidity;
upon
delivery,
these
lighter
materials
are
separated
out
of
the
mixture.
This
mixture
is
often
called
bitumen
feedstock,
or
BFS.
Some
dump
trucks
route
the
hot
engine
exhaust
through
pipes
in
the
dump
body
to
keep
the
material
warm.
The
backs
of
tippers
carrying
asphalt,
as
well
as
some
handling
equipment,
are
also
commonly
sprayed
with
a
releasing
agent
before
filling
to
aid
release.
Diesel
oil
is
sometimes
used
as
a
release
agent,
although
it
can
mix
with
and
thereby
reduce
the
quality
of
the
asphalt.
Ancient
times.
In
the
ancient
Middle
East,
natural
asphalt
deposits
were
used
for
mortar
between
bricks
and
stones,
to
cement
parts
of
carvings
such
as
eyes
into
place,
for
ship
caulking,
and
for
waterproofing.
The
Persian
word
for
asphalt
is
"mumiya",
which
is
related
to
the
English
word
mummy.
Asphalt
was
also
used
by
ancient
Egyptians
to
embalm
mummies.
In
the
ancient
Far
East,
natural
asphalt
was
slowly
boiled
to
get
rid
of
the
higher
fractions,
leaving
a
material
of
higher
molecular
weight
which
is
thermoplastic
and
when
layered
on
objects,
became
quite
hard
upon
cooling.
This
was
used
to
cover
objects
that
needed
waterproofing,
such
as
scabbards
and
other
items.
Statuettes
of
household
deities
were
also
cast
with
this
type
of
material
in
Japan,
and
probably
also
in
China.
In
North
America,
archaeological
recovery
has
indicated
that
asphaltum
was
sometimes
used
to
apply
stone
projectile
points
to
a
wooden
shaft.
Early
use
in
Europe.
The
use
of
asphalt
in
the
United
Kingdom
and
United
States
was
preceded
by
its
use
in
Europe.
An
1838
edition
of
"Mechanics
Magazine"
cites
an
early
use
of
asphalt
in
France.
A
pamphlet
dated
1621,
by
"a
certain
Monsieur
d'Eyrinys,
states
that
he
had
discovered
the
existence
(of
asphaltum)
in
large
quantities
in
the
vicinity
of
Neufchatel",
and
that
he
proposed
to
use
it
in
a
variety
of
ways
-
"principally
in
the
construction
of
air-proof
granaries,
and
in
protecting,
by
means
of
the
arches,
the
water-courses
in
the
city
of
Paris
from
the
intrusin
of
dirt
and
filth",
which
at
that
time
made
the
water
unusable.
"He
expatiates
also
on
the
excellence
of
this
material
for
forming
level
and
durable
terraces"
in
palaces,
"the
notion
of
forming
such
terraces
in
the
streets
not
one
likely
to
cross
the
brain
of
a
Parisian
of
that
generation".
But
it
was
generally
neglected
in
France
until
the
revolution
of
1830.
Then,
in
the
1830s,
there
was
a
surge
of
interest,
and
asphalt
became
widely
used
"for
pavements,
flat
roofs,
and
the
lining
of
cisterns,
and
in
England,
some
use
of
it
had
been
made
of
it
for
similar
purposes".
Its
rise
in
Europe
was
"a
sudden
phenomenon",
after
natural
deposits
were
found
"in
France
at
Osbann
(BasRhin),
the
Parc
(l'Ain)
and
the
Puy-de-la-Poix
(Puy-de-Dome)",
although
it
could
also
be
made
artificially.
Early
use
in
the
United
Kingdom.
William
Salmon's
"Polygraphice"
(1673)
provides
a
recipe
for
varnish
used
in
etching,
consisting
of
three
ounces
of
virgin
wax,
two
ounces
of
mastic,
and
one
ounce
of
asphaltum.
In
Britain,
the
first
patent
was
'Cassell's
patent
asphalte
or
bitumen'
in
1834.
Then
on
25
November
1837,
Richard
Tappin
Claridge
patented
the
use
of
Seyssel
asphalt
(patent
#7849),
for
use
in
asphalte
pavement,
having
seen
it
employed
in
France
and
Belgium
when
visiting
with
Frederick
Walter
Simms,
who
worked
with
him
on
the
introduction
of
asphalt
to
Britain.
Dr
T.
Lamb
Phipson
claims
that
his
father,
Samuel
Ryland
Phipson,
a
friend
of
Claridge,
was
also
"instrumental
in
introducing
the
asphalte
pavement
(in
1836)".
In
1838,
Claridge
obtained
patents
in
Scotland
on
27
March,
and
Ireland
on
23
April,
and
in
1851
he
sought
to
extend
the
duration
of
all
three
patents.
He
formed
"Claridge's
Patent
Asphalte
Company"
for
the
purpose
of
introducing
to
Britain
"Asphalte
in
its
natural
state
from
the
mine
at
Pyrimont
Seysell
in
France",
and
"laid
one
of
the
first
asphalt
pavements
in
Whitehall".
Trials
were
made
of
the
pavement
in
1838
on
the
footway
in
Whitehall,
the
stable
at
Knightsbridge
Barracks,
"and
subsequently
on
the
space
at
the
bottom
of
the
steps
leading
from
Waterloo
Place
to
St.
James
Park".
"The
formation
in
1838
of
Claridge's
Patent
Asphalte
Company
(with
a
distinguished
list
of
aristocratic
patrons,
and
Marc
and
Isambard
Brunel
as,
respectively,
a
trustee
and
consulting
engineer),
gave
an
enormous
impetus
to
the
development
of
a
British
asphalt
industry".
"By
the
end
of
1838,
at
least
two
other
companies,
Robinson's
and
the
Bastenne
company,
were
in
production",
with
asphalt
being
laid
as
paving
at
Brighton,
Herne
Bay,
Canterbury,
Kensington,
the
Strand,
and
a
large
floor
area
in
Bunhill-row,
while
meantime
Claridge's
Whitehall
paving
"continue(d)
in
good
order".
Indeed
in
1838,
there
was
a
flurry
of
entrepreneurial
activity
over
asphalt.
On
the
London
stockmarket,
there
were
various
claims
as
to
the
priority
of
asphalt
quality
from
France,
Germany
and
England.
And
numerous
patents
were
granted
in
France,
with
similar
numbers
of
patent
applications
being
denied
in
England
due
to
their
similarity
to
each
other.
In
England,
"Claridge's
was
the
type
most
used
in
the
1840s
and
50s"
Claridge's
own
company
ceased
operating
in
1917.
Early
use
in
the
United
States.
The
first
use
of
asphaltum
in
the
New
World
was
by
indigenous
Indian
tribes.
On
the
west
coast,
as
early
as
the
1200s,
the
Tongva
and
Chumash
Nations
collected
the
naturally
occurring
asphaltum
that
seeped
to
the
surface
above
underlying
petroleum
deposits.
Both
tribes
used
the
substance
as
an
adhesive.
It
is
found
on
many
different
artifacts
of
tools
and
ceremonial
items.
For
example,
it
was
used
on
rattles
to
adhere
gourds
or
turtle
shells
to
rattle
handles.
It
was
also
used
in
decorations.
Small
round
shell
beads
were
often
set
in
asphatum
to
provide
decorations.
It
was
used
as
a
sealant
on
baskets
to
make
them
water
tight
for
carrying
water.
Asphaltum
was
used
also
to
seal
the
planks
on
ocean-going
canoes.
Roads
in
the
US
have
been
paved
with
asphalt
since
at
least
1870,
when
a
street
in
front
of
Newark,
NJ's
City
Hall
was
paved.
In
1876,
asphalt
was
used
to
pave
Pennsylvania
Avenue
in
Washington,
DC,
in
time
for
the
celebration
of
the
national
centennial.
Asphalt
was
also
used
for
flooring,
paving
and
waterproofing
of
baths
and
swimming
pools
during
the
early
1900s,
following
similar
trends
in
Europe.
Rolled
asphalt
concrete.
The
largest
use
of
asphalt
is
for
making
asphalt
concrete
for
road
surfaces
and
accounts
for
approximately
85%
of
the
asphalt
consumed
in
the
United
States.
Asphalt
pavement
material
is
commonly
composed
of
5
percent
asphalt
cement
and
95
percent
aggregates
(stone,
sand,
and
gravel).
Due
to
its
highly
viscous
nature,
asphalt
cement
must
be
heated
so
that
it
can
be
mixed
with
the
aggregates
at
the
asphalt
mixing
plant.
There
are
about
4,000
asphalt
mixing
plants
in
the
U.S.
Asphalt
road
surface
is
the
most
widely
recycled
material
in
the
US,
both
by
gross
tonnage
and
by
percentage.
According
to
a
report
issued
by
the
Federal
Highway
Administration
and
the
United
States
Environmental
Protection
Agency,
80%
of
the
asphalt
from
road
surfaces'
that
is
removed
each
year
during
widening
and
resurfacing
projects
is
reused
as
part
of
new
roads,
roadbeds,
shoulders
and
embankments.
Roofing
shingles
account
for
most
of
the
remaining
asphalt
consumption.
Other
uses
include
cattle
sprays,
fence
post
treatments,
and
waterproofing
for
fabrics.
Asphalt
is
widely
used
in
airports
around
the
world.
Due
to
the
sturdiness,
it
is
widely
used
for
runways
dedicated
to
aircraft
landing
and
taking
off.
Mastic
asphalt.
Mastic
asphalt
is
a
type
of
asphalt
which
differs
from
dense
graded
asphalt
(asphalt
concrete)
in
that
it
has
a
higher
bitumen
(binder)
content,
usually
around
7–10%
of
the
whole
aggregate
mix,
as
opposed
to
rolled
asphalt,
which
has
only
around
5%
added
bitumen.
This
thermoplastic
substance
is
widely
used
in
the
building
industry
for
waterproofing
flat
roofs
and
tanking
underground.
Mastic
asphalt
is
heated
to
a
temperature
of
and
is
spread
in
layers
to
form
a
impervious
barrier
about
thick.
There
is
a
proper
apprenticeship
and
trainees
go
to
college
to
learn
this
trade.
Asphalt
emulsion.
A
number
of
technologies
allow
asphalt
to
be
mixed
at
much
lower
temperatures.
These
involve
mixing
the
asphalt
with
petroleum
solvents
to
form
"cutbacks"
with
reduced
melting
point
or
mixtures
with
water
to
turn
the
asphalt
into
an
emulsion.
Asphalt
emulsions
contain
up
to
70%
asphalt
and
typically
less
than
1.5%
chemical
additives.
There
are
two
main
types
of
emulsions
with
different
affinity
for
aggregates,
cationic
and
anionic.
Asphalt
emulsions
are
used
in
a
wide
variety
of
applications.
Chipseal
involves
spraying
the
road
surface
with
asphalt
emulsion
followed
by
a
layer
of
crushed
rock
or
gravel.
Slurry
Seal
involves
the
creation
of
a
mixture
of
asphalt
emulsion
and
fine
crushed
aggregate
that
is
spread
on
the
surface
of
a
road.
Cold
mixed
asphalt
can
also
be
made
from
asphalt
emulsion
to
create
pavements
similar
to
hot-mixed
asphalt,
several
inches
in
depth
and
asphalt
emulsions
are
also
blended
into
recycled
hot-mix
asphalt
to
create
low
cost
pavements.
Alternatives
and
bioasphalt.
Certain
activist
groups
have
become
increasingly
concerned
about
the
global
peak
oil
and
climate
change
problem
in
recent
years
due
to
by-products
that
are
released
into
the
atmosphere.
Most
of
the
emissions
are
derived
primarily
from
burning
fossil
fuels.
This
has
led
to
the
introduction
of
petroleum
bitumen
alternatives
that
are
more
environmentally
friendly
and
non-toxic.
---END.OF.DOCUMENT---
American
National
Standards
Institute.
The
American
National
Standards
Institute
or
ANSI
()
is
a
private
non-profit
organization
that
oversees
the
development
of
voluntary
consensus
standards
for
products,
services,
processes,
systems,
and
personnel
in
the
United
States.
The
organization
also
coordinates
U.S.
standards
with
international
standards
so
that
American
products
can
be
used
worldwide.
For
example,
standards
make
sure
that
people
who
own
cameras
can
find
the
film
they
need
for
that
camera
anywhere
around
the
globe.
ANSI
accredits
standards
that
are
developed
by
representatives
of
standards
developing
organizations,
government
agencies,
consumer
groups,
companies,
and
others.
These
standards
ensure
that
the
characteristics
and
performance
of
products
are
consistent,
that
people
use
the
same
definitions
and
terms,
and
that
products
are
tested
the
same
way.
ANSI
also
accredits
organizations
that
carry
out
product
or
personnel
certification
in
accordance
with
requirements
defined
in
international
standards.
The
organization's
headquarters
are
in
Washington,
DC.
ANSI's
operations
office
is
located
in
New
York
City.
History.
ANSI
was
originally
formed
in
1918,
when
five
engineering
societies
and
three
government
agencies
founded
the
American
Engineering
Standards
Committee
(AESC).
In
1928,
the
AESC
became
the
American
Standards
Association
(ASA).
In
1966,
the
ASA
was
reorganized
and
became
the
United
States
of
America
Standards
Institute
(USASI).
The
present
name
was
adopted
in
1969.
Prior
to
1918,
these
five
engineering
societies,
the
American
Institute
of
Electrical
Engineers
(AIEE,
now
IEEE),
American
Society
of
Mechanical
Engineers
(ASME),
American
Society
of
Civil
Engineers
(ASCE),
the
American
Institute
of
Mining
and
Metallurgical
Engineers
(now
AIME),
and
the
American
Society
for
Testing
Materials
(now
ASTM
International),
had
been
members
of
the
United
Engineering
Society
(UES).
At
the
behest
of
the
AIEE,
they
invited
the
U.S.
government
Departments
of
War,
Navy
and
Commerce
to
join
in
founding
a
national
standards
organization.
According
to
Paul
G.
Agnew,
the
first
permanent
secretary
and
head
of
staff
in
1919,
AESC
started
as
an
ambitious
program
and
little
else.
Staff
for
the
first
year
consisted
of
one
executive,
Clifford
B.
LePage,
who
was
on
loan
from
a
founding
member,
ASME.
An
annual
budget
of
$7,500
was
provided
by
the
founding
bodies.
In
1931,
the
organization
(renamed
ASA
in
1928)
became
affiliated
with
the
U.S.
National
Committee
of
the
International
Electrotechnical
Commission
(IEC),
which
had
been
formed
in
1904
to
develop
electrical
and
electronics
standards.http://www.iec.ch/
Members.
ANSI's
membership
comprises
government
agencies,
organizations,
corporations,
academic
and
international
bodies,
and
individuals.
In
total,
the
Institute
represents
the
interests
of
more
than
125,000
companies
and
3.5
million
professionals.
Process.
Though
ANSI
itself
does
not
develop
standards,
the
Institute
oversees
the
development
and
use
of
standards
by
accrediting
the
procedures
of
standards
developing
organizations.
ANSI
accreditation
signifies
that
the
procedures
used
by
standards
developing
organizations
meet
the
Institute's
requirements
for
openness,
balance,
consensus,
and
due
process.
ANSI
also
designates
specific
standards
as
American
National
Standards,
or
ANS,
when
the
Institute
determines
that
the
standards
were
developed
in
an
environment
that
is
equitable,
accessible
and
responsive
to
the
requirements
of
various
stakeholders.
Voluntary
consensus
standards
quicken
the
market
acceptance
of
products
while
making
clear
how
to
improve
the
safety
of
those
products
for
the
protection
of
consumers.
There
are
approximately
9,500
American
National
Standards
that
carry
the
ANSI
designation.
International
activities.
In
addition
to
facilitating
the
formation
of
standards
in
the
U.S.,
ANSI
promotes
the
use
of
U.S.
standards
internationally,
advocates
U.S.
policy
and
technical
positions
in
international
and
regional
standards
organizations,
and
encourages
the
adoption
of
international
standards
as
national
standards
where
appropriate.
The
Institute
is
the
official
U.S.
representative
to
the
two
major
international
standards
organizations,
the
International
Organization
for
Standardization
(ISO)
and
the
International
Electrotechnical
Commission
(IEC),
via
the
U.S.
National
Committee
(USNC).
ANSI
participates
in
almost
the
entire
technical
program
of
both
the
ISO
and
the
IEC,
and
administers
many
key
committees
and
subgroups.
In
many
instances,
U.S.
standards
are
taken
forward
to
ISO
and
IEC,
through
ANSI
or
the
USNC,
where
they
are
adopted
in
whole
or
in
part
as
international
standards.
Examples.
Each
of
the
panels
works
to
identify,
coordinate,
and
harmonize
voluntary
standards
relevant
to
these
areas.
In
2009,
ANSI
and
the
National
Institute
for
Standards
and
Technology
(NIST)
formed
the
Nuclear
Energy
Standards
Coordination
Collaborative
(NESCC).
NESCC
is
a
joint
initiative
to
identify
and
respond
to
the
current
need
for
standards
in
the
nuclear
industry.
---END.OF.DOCUMENT---
Apollo
11.
The
Apollo
11
mission
landed
the
first
humans
on
the
Moon.
Launched
on
July
16,
1969,
the
third
lunar
mission
of
NASA's
Apollo
Program
was
crewed
by
Commander
Neil
Alden
Armstrong,
Command
Module
Pilot
Michael
Collins,
and
Lunar
Module
Pilot
Edwin
Eugene
'Buzz'
Aldrin,
Jr.
On
July
20,
Armstrong
and
Aldrin
became
the
first
humans
to
walk
on
the
Moon,
while
Collins
orbited
in
the
Command
Module.
The
mission
fulfilled
President
John
F.
Kennedy's
goal
of
reaching
the
moon
by
the
end
of
the
1960s,
which
he
had
expressed
during
a
speech
given
before
a
joint
session
of
Congress
on
May
25,
1961:
"I
believe
that
this
nation
should
commit
itself
to
achieving
the
goal,
before
this
decade
is
out,
of
landing
a
man
on
the
Moon
and
returning
him
safely
to
the
Earth."
Crew.
Each
crewmember
of
Apollo
11
had
made
a
spaceflight
before
this
mission,
making
it
the
third
all-veteran
crew
in
manned
spaceflight
history.
Collins
was
originally
slated
to
be
the
Command
Module
Pilot
(CMP)
on
Apollo
8
but
was
removed
when
he
required
surgery
on
his
back
and
was
replaced
by
Jim
Lovell,
his
backup
for
that
flight.
After
Collins
was
medically
cleared,
he
took
what
would
have
been
Lovell's
spot
on
Apollo
11;
as
a
veteran
of
Apollo
8,
Lovell
was
transferred
to
Apollo
11's
backup
crew,
but
promoted
to
backup
commander.
Backup
crew.
In
early
1969
Bill
Anders
accepted
a
job
with
the
National
Space
Council
effective
in
August
1969
and
announced
his
retirement
as
an
astronaut.
At
that
point
Ken
Mattingly
was
moved
from
the
support
crew
into
parallel
training
with
Anders
as
backup
Command
Module
Pilot
in
case
Apollo
11
was
delayed
past
its
intended
July
launch
(at
which
point
Anders
would
be
unavailable
if
needed)
and
would
later
join
Lovell's
crew
and
ultimately
be
assigned
as
the
original
Apollo
13
CMP.
Nomenclature.
The
lunar
module
was
named
"Eagle"
for
the
national
bird
of
the
United
States,
the
bald
eagle,
and
featured
prominently
on
the
mission
insignia.
The
command
module
was
named
"Columbia"
for
the
feminine
personification
of
the
United
States
used
traditionally
in
song
and
poetry.
During
early
mission
planning,
the
names
"Snowcone"
and
"Haystack"
were
used
but
changed
before
announcement
to
the
press.
Launch
and
lunar
orbit
injection.
In
addition
to
throngs
of
people
crowding
highways
and
beaches
near
the
launch
site,
millions
watched
the
event
on
television,
with
NASA
Chief
of
Public
Information
Jack
King
providing
commentary.
President
Richard
Nixon
viewed
the
proceedings
from
the
Oval
Office
of
the
White
House.
A
Saturn
V
launched
"Apollo
11"
from
Launch
Pad
39A,
part
of
the
Launch
Complex
39
site
at
the
Kennedy
Space
Center
on
July
16,
1969
at
13:32:00
UTC
(9:32:00
a.m.
local
time).
It
entered
orbit
12
minutes
later.
After
one
and
a
half
orbits,
the
S-IVB
third-stage
engine
pushed
the
spacecraft
onto
its
trajectory
toward
the
Moon
with
the
Trans
Lunar
Injection
burn
at
16:22:13
UTC.
About
30
minutes
later
the
service
module
pair
separated
from
this
last
remaining
Saturn
V
stage
and
docked
with
the
lunar
module
still
nestled
in
the
Lunar
Module
Adaptor.
After
the
lunar
module
was
extracted,
the
combined
spacecraft
headed
for
the
Moon,
while
the
third
stage
booster
flew
on
a
trajectory
past
the
moon
and
into
solar
orbit.
On
July
19
at
17:21:50
UTC,
"Apollo
11"
passed
behind
the
Moon
and
fired
its
service
propulsion
engine
to
enter
lunar
orbit.
In
the
thirty
orbits
that
followed,
the
crew
saw
passing
views
of
their
landing
site
in
the
southern
Sea
of
Tranquility
(Mare
Tranquillitatis)
about
20
kilometers
(12 mi)
southwest
of
the
crater
Sabine
D
(0.67408N,
23.47297E).
The
landing
site
was
selected
in
part
because
it
had
been
characterized
as
relatively
flat
and
smooth
by
the
automated
"Ranger
8"
and
"Surveyor
5"
landers
along
with
the
"Lunar
Orbiter"
mapping
spacecraft
and
unlikely
to
present
major
landing
or
extra-vehicular
activity
(EVA)
challenges.
Lunar
descent.
On
July
20,
1969
the
lunar
module
(LM)
"Eagle"
separated
from
the
command
module
"Columbia".
Collins,
alone
aboard
"Columbia",
inspected
"Eagle"
as
it
pirouetted
before
him
to
ensure
the
craft
was
not
damaged.
As
the
descent
began,
Armstrong
and
Aldrin
found
that
they
were
passing
landmarks
on
the
surface
4
seconds
early
and
reported
that
they
were
"long":
they
would
land
miles
west
of
their
target
point.
Five
minutes
into
the
descent
burn,
and
6000
feet
above
the
surface
of
the
moon,
the
LM
navigation
and
guidance
computer
distracted
the
crew
with
the
first
of
several
unexpected
"1202"
and
"1201"
program
alarms.
Inside
Mission
Control
Center
in
Houston,
Texas,
computer
engineer
Jack
Garman
told
guidance
officer
Steve
Bales
it
was
safe
to
continue
the
descent
and
this
was
relayed
to
the
crew.
The
program
alarms
indicated
"executive
overflows",
where
the
guidance
computer
could
not
complete
all
of
its
tasks
in
real
time
and
had
to
postpone
some
of
them.
This
was
neither
a
computer
error
nor
an
astronaut
error,
but
stemmed
from
a
mistake
in
how
the
astronauts
had
been
trained.
Although
unneeded
for
the
landing,
the
rendezvous
radar
was
intentionally
turned
on
to
make
ready
for
a
fast
abort.
Ground
simulation
setups
had
not
foreseen
that
a
fast
stream
of
spurious
interrupts
from
this
radar
could
happen,
depending
upon
how
the
hardware
randomly
powered
up
before
the
LM
then
began
nearing
the
lunar
surface:
hence
the
computer
had
to
deal
with
data
from
two
radars,
not
the
landing
radar
alone,
which
led
to
the
overload.
When
Armstrong
again
looked
outside,
he
saw
that
the
computer's
landing
target
was
in
a
boulder-strewn
area
just
north
and
east
of
a
300
meter
diameter
crater
(later
determined
to
be
"West
crater",
named
for
its
location
in
the
western
part
of
the
originally
planned
landing
ellipse).
Armstrong
took
semi-automatic
control
and,
with
Aldrin
calling
out
altitude
and
velocity
data,
landed
at
20:17
UTC
on
July
20
with
about
25
seconds
of
fuel
left.
"Apollo
11"
landed
with
less
fuel
than
other
missions,
and
the
astronauts
also
encountered
a
premature
low
fuel
warning.
This
was
later
found
to
have
been
due
to
greater
propellant
'slosh'
than
expected
uncovering
a
fuel
sensor.
On
subsequent
missions,
extra
baffles
were
added
to
the
tanks
to
prevent
this.
Throughout
the
descent
Aldrin
had
called
out
navigation
data
to
Armstrong,
who
was
busy
piloting
the
LM.
A
few
moments
before
the
landing,
a
light
informed
Aldrin
that
at
least
one
of
the
67-inch
probes
hanging
from
"Eagles
footpads
had
touched
the
surface,
and
he
said
"Contact
light!".
Three
seconds
later,
"Eagle"
landed
and
Armstrong
said
"Shutdown".
Aldrin
immediately
said
"Okay,
engine
stop.
ACA
-
out
of
detent."
Armstrong
acknowledged
"Out
of
detent.
Auto"
and
Aldrin
continued
"Mode
control
-
both
auto.
Descent
engine
command
override
off.
Engine
arm
-
off.
413
is
in."
Charles
Duke,
acting
as
CAPCOM
during
the
landing
phase,
acknowledged
their
landing
by
saying
"We
copy
you
down,
Eagle".
Armstrong
continued
with
the
remainder
of
the
post
landing
checklist,
"Engine
arm
is
off."
before
responding
to
Duke
with
the
famous
words,
"Houston,
Tranquility
Base
here.
The
"Eagle"
has
landed."
Armstrong's
abrupt
change
of
call
sign
from
"Eagle"
to
"Tranquility
Base"
caused
momentary
confusion
at
Mission
Control
and
Duke
remained
silent
for
a
couple
of
seconds
before
replying:
"Roger,
Twank...Tranquility,
we
copy
you
on
the
ground.
You
got
a
bunch
of
guys
about
to
turn
blue.
We're
breathing
again.
Thanks
a
lot!"
expressing
the
relief
of
Mission
Control
after
the
unexpectedly
drawn-out
descent.
He
then
took
Communion
privately.
At
this
time
NASA
was
still
fighting
a
lawsuit
brought
by
atheist
Madalyn
Murray
O'Hair
(who
had
objected
to
the
"Apollo
8"
crew
reading
from
the
Book
of
Genesis)
which
demanded
that
their
astronauts
refrain
from
religious
activities
while
in
space.
As
such,
Aldrin
chose
to
refrain
from
directly
mentioning
this.
He
had
kept
the
plan
quiet
(not
even
mentioning
it
to
his
wife)
and
did
not
reveal
it
publicly
for
several
years.
Buzz
Aldrin
was
an
elder
at
Webster
Presbyterian
Church
in
Webster,
TX.
His
communion
kit
was
prepared
by
the
pastor
of
the
church,
the
Rev.
Dean
Woodruff.
Aldrin
described
communion
on
the
moon
and
the
involvement
of
his
church
and
pastor
in
the
October,
1970
edition
of
Guideposts
magazine
and
in
his
book
"Return
to
Earth."
Webster
Presbyterian
possesses
the
chalice
used
on
the
moon,
and
commemorates
the
Lunar
Communion
each
year
on
the
Sunday
closest
to
July
20.
The
schedule
for
the
mission
called
for
the
astronauts
to
follow
the
landing
with
a
five-hour
sleep
period,
since
they
had
been
awake
since
early
morning.
However,
they
elected
to
forgo
the
sleep
period
and
begin
the
preparations
for
the
EVA
early,
thinking
that
they
would
be
unable
to
sleep.
Lunar
surface
operations.
The
astronauts
planned
placement
of
the
Early
Apollo
Scientific
Experiment
Package
(EASEP)
and
the
U.S.
flag
by
studying
their
landing
site
through
"Eagles
twin
triangular
windows,
which
gave
them
a
60°
field
of
view.
Preparation
required
longer
than
the
two
hours
scheduled.
Armstrong
initially
had
some
difficulties
squeezing
through
the
hatch
with
his
Portable
Life
Support
System
(PLSS).
According
to
veteran
moonwalker
John
Young,
a
redesign
of
the
LM
to
incorporate
a
smaller
hatch
had
not
been
followed
by
a
redesign
of
the
PLSS
backpack,
so
some
of
the
highest
heart
rates
recorded
from
"Apollo"
astronauts
occurred
during
LM
egress
and
ingress.
At
02:39
UTC
on
Monday
July
21
(10:39pm
EDT,
Sunday
July
20),
1969,
Armstrong
opened
the
hatch,
and
at
02:51
UTC
began
his
descent
to
the
Moon's
surface.
The
Remote
Control
Unit
controls
on
his
chest
kept
him
from
seeing
his
feet.
Climbing
down
the
nine-rung
ladder,
Armstrong
pulled
a
D-ring
to
deploy
the
Modular
Equipment
Stowage
Assembly
(MESA)
folded
against
"Eagles
side
and
activate
the
TV
camera,
and
at
02:56
UTC
(10:56pm
EDT)
he
set
his
left
foot
on
the
surface.
The
first
landing
used
slow-scan
television
incompatible
with
commercial
TV,
so
it
was
displayed
on
a
special
monitor
and
a
conventional
TV
camera
viewed
this
monitor,
significantly
reducing
the
quality
of
the
picture.
The
signal
was
received
at
Goldstone
in
the
USA
but
with
better
fidelity
by
Honeysuckle
Creek
Tracking
Station
in
Australia.
Minutes
later
the
feed
was
switched
to
the
more
sensitive
Parkes
radio
telescope
in
Australia.
Despite
some
technical
and
weather
difficulties,
ghostly
black
and
white
images
of
the
first
lunar
EVA
were
received
and
broadcast
to
at
least
600
million
people
on
Earth.
Although
copies
of
this
video
in
broadcast
format
were
saved
and
are
widely
available,
recordings
of
the
original
slow
scan
source
transmission
from
the
moon
were
accidentally
destroyed
during
routine
magnetic
tape
re-use
at
NASA.
Archived
copies
of
the
footage
were
eventually
located
in
Perth,
Australia,
which
was
one
of
the
sites
that
originally
received
the
Moon
broadcast.
After
describing
the
surface
dust
("fine
and
almost
like
a
powder"),
Armstrong
stepped
off
"Eagles
footpad
and
into
history
as
the
first
human
to
set
foot
on
another
world.
It
was
then
that
he
uttered
his
famous
line
"That's
one
small
step
for
[a]
man,
one
giant
leap
for
mankind"
six
and
a
half
hours
after
landing.
Aldrin
joined
him,
describing
the
view
as
"Magnificent
desolation."
Armstrong
said
that
moving
in
the
Moon's
gravity,
one-sixth
of
Earth's,
was
"even
perhaps
easier
than
the
simulations...
It's
absolutely
no
trouble
to
walk
around".
In
addition
to
fulfilling
President
John
F.
Kennedy's
mandate
to
land
a
man
on
the
Moon
before
the
end
of
the
1960s,
"Apollo
11"
was
an
engineering
test
of
the
Apollo
system;
therefore,
Armstrong
snapped
photos
of
the
LM
so
engineers
would
be
able
to
judge
its
post-landing
condition.
He
then
collected
a
contingency
soil
sample
using
a
sample
bag
on
a
stick.
He
folded
the
bag
and
tucked
it
into
a
pocket
on
his
right
thigh.
He
removed
the
TV
camera
from
the
MESA,
made
a
panoramic
sweep,
and
mounted
it
on
a
tripod
12 m
(40 ft)
from
the
LM.
The
TV
camera
cable
remained
partly
coiled
and
presented
a
tripping
hazard
throughout
the
EVA.
Aldrin
joined
him
on
the
surface
and
tested
methods
for
moving
around,
including
two-footed
kangaroo
hops.
The
PLSS
backpack
created
a
tendency
to
tip
backwards,
but
neither
astronaut
had
serious
problems
maintaining
balance.
Loping
became
the
preferred
method
of
movement.
The
astronauts
reported
that
they
needed
to
plan
their
movements
six
or
seven
steps
ahead.
The
fine
soil
was
quite
slippery.
Aldrin
remarked
that
moving
from
sunlight
into
"Eagles
shadow
produced
no
temperature
change
inside
the
suit,
though
the
helmet
was
warmer
in
sunlight,
so
he
felt
cooler
in
shadow.
After
the
astronauts
planted
a
U.S.
flag
on
the
lunar
surface,
they
spoke
with
President
Richard
Nixon
through
a
telephone-radio
transmission
which
Nixon
called
"the
most
historic
phone
call
ever
made
from
the
White
House."
Nixon
originally
had
a
long
speech
prepared
to
read
during
the
phone
call,
but
Frank
Borman,
who
was
at
the
White
House
as
a
NASA
liaison
during
Apollo
11,
convinced
Nixon
to
keep
his
words
brief,
out
of
respect
of
the
lunar
landing
being
Kennedy's
legacy.
The
MESA
failed
to
provide
a
stable
work
platform
and
was
in
shadow,
slowing
work
somewhat.
As
they
worked,
the
moonwalkers
kicked
up
gray
dust
which
soiled
the
outer
part
of
their
suits,
the
integrated
thermal
meteoroid
garment.
They
deployed
the
EASEP,
which
included
a
passive
seismograph
and
a
laser
ranging
retroreflector.
Then
Armstrong
loped
about
120 m
(400 ft)
from
the
LM
to
snap
photos
at
the
rim
of
East
Crater
while
Aldrin
collected
two
core
tubes.
He
used
the
geological
hammer
to
pound
in
the
tubes
-
the
only
time
the
hammer
was
used
on
"Apollo
11".
The
astronauts
then
collected
rock
samples
using
scoops
and
tongs
on
extension
handles.
Many
of
the
surface
activities
took
longer
than
expected,
so
they
had
to
stop
documenting
sample
collection
halfway
through
the
allotted
34
min.
During
this
period
Mission
Control
used
a
coded
phrase
to
warn
Armstrong
that
his
metabolic
rates
were
high
and
that
he
should
slow
down.
He
was
moving
rapidly
from
task
to
task
as
time
ran
out.
However,
as
metabolic
rates
remained
generally
lower
than
expected
for
both
astronauts
throughout
the
walk,
Mission
Control
granted
the
astronauts
a
15-minute
extension.
Lunar
ascent
and
return.
Aldrin
entered
"Eagle"
first.
With
some
difficulty
the
astronauts
lifted
film
and
two
sample
boxes
containing
more
than
22 kg
(48 lb)
of
lunar
surface
material
to
the
LM
hatch
using
a
flat
cable
pulley
device
called
the
Lunar
Equipment
Conveyor.
Armstrong
reminded
Aldrin
of
a
bag
of
memorial
items
in
his
suit
pocket
sleeve,
and
Aldrin
tossed
the
bag
down;
Armstrong
then
jumped
to
the
ladder's
third
rung
and
climbed
into
the
LM.
After
transferring
to
LM
life
support,
the
explorers
lightened
the
ascent
stage
for
return
to
lunar
orbit
by
tossing
out
their
PLSS
backpacks,
lunar
overshoes,
one
Hasselblad
camera,
and
other
equipment.
They
then
repressurised
the
LM,
and
settled
down
to
sleep.
During
this
time
another
spacecraft,
Luna
15
-
an
unmanned
Soviet
spacecraft
in
lunar
orbit,
began
its
own
descent
to
the
lunar
surface.
Launched
only
three
days
before
the
Apollo
11
mission,
this
was
the
third
Soviet
attempt
to
return
lunar
soil
back
to
Earth.
The
Russian
craft
crashed
on
the
moon
at
15:50
UT
–
just
a
few
hours
before
the
scheduled
American
liftoff.
In
a
race
to
reach
the
Moon
and
return
to
Earth,
the
parallel
missions
of
Luna
15
and
Apollo
11
were,
in
many
ways,
the
culmination
of
the
space
race
that
underlay
the
space
programs
of
both
the
United
States
and
the
Soviet
Union
in
the
1960s.
The
simultaneous
missions
became
one
of
the
first
instances
of
Soviet/American
space
cooperation
as
the
USSR
released
Luna
15's
flight
plan
to
ensure
it
would
not
collide
with
Apollo
11,
though
its
exact
mission
was
unknown.
While
moving
within
the
cabin,
Aldrin
accidentally
broke
the
circuit
breaker
that
would
arm
the
main
engine
for
lift
off
from
the
moon.
There
was
concern
this
would
prevent
firing
the
engine,
stranding
them
on
the
moon.
Fortunately
a
felt-tip
pen
was
sufficient
to
activate
the
switch.
Had
this
not
worked,
the
Lunar
Module
circuitry
could
have
been
reconfigured
to
allow
firing
the
ascent
engine.
After
about
seven
hours
of
rest,
the
crew
were
awakened
by
Houston
to
prepare
for
the
return
flight.
Two
and
a
half
hours
later,
at
17:54
UTC,
they
lifted
off
in
"Eagles
ascent
stage,
carrying
21.5
kilograms
of
lunar
samples
with
them,
to
rejoin
CMP
Michael
Collins
aboard
"Columbia"
in
lunar
orbit.
After
more
than
2½
hours
on
the
lunar
surface,
they
had
left
behind
scientific
instruments
which
included
a
retroreflector
array
used
for
the
Lunar
Laser
Ranging
Experiment
and
a
Passive
Seismic
Experiment
used
to
measure
moonquakes.
They
also
left
an
American
flag,
an
Apollo
1
mission
patch,
and
a
plaque
(mounted
on
the
LM
Descent
Stage
ladder)
bearing
two
drawings
of
Earth
(of
the
Western
and
Eastern
Hemispheres),
an
inscription,
and
signatures
of
the
astronauts
and
President
Richard
M.
Nixon.
The
inscription
read
"Here
Men
From
The
Planet
Earth
First
Set
Foot
Upon
the
Moon,
July
1969
A.D.
We
Came
in
Peace
For
All
Mankind."
They
also
left
behind
a
memorial
bag
containing
a
gold
replica
of
an
olive
branch
as
a
traditional
symbol
of
peace
and
a
silicon
message
disk.
The
disk
carries
the
goodwill
statements
by
Presidents
Eisenhower,
Kennedy,
Johnson
and
Nixon
and
messages
from
leaders
of
73
countries
around
the
world.
The
disc
also
carries
a
listing
of
the
leadership
of
the
US
Congress,
a
listing
of
members
of
the
four
committees
of
the
House
and
Senate
responsible
for
the
NASA
legislation,
and
the
names
of
NASA's
past
and
present
top
management.
(In
his
1989
book,
"Men
from
Earth",
Aldrin
says
that
the
items
included
Soviet
medals
commemorating
Cosmonauts
Vladimir
Komarov
and
Yuri
Gagarin.)
Also,
according
to
Deke
Slayton's
book
'Moonshot',
Armstrong
carried
with
him
a
special
diamond-studded
Astronaut
pin
from
Deke.
Film
taken
from
the
LM
Ascent
Stage
upon
liftoff
from
the
moon
reveals
the
American
flag,
planted
some
from
the
descent
stage,
whipping
violently
in
the
exhaust
of
the
ascent
stage
engine.
Buzz
Aldrin
witnessed
it
topple:
"The
ascent
stage
of
the
LM
separated...I
was
concentrating
on
the
computers,
and
Neil
was
studying
the
attitude
indicator,
but
I
looked
up
long
enough
to
see
the
flag
fall
over."
Subsequent
Apollo
missions
usually
planted
the
American
flags
at
least
from
the
LM
to
prevent
its
being
blown
over
by
the
ascent
engine
exhaust.
After
rendezvous
with
"Columbia",
"Eagle's"
ascent
stage
was
jettisoned
into
lunar
orbit
at
July
21,
1969
at
23:41
UT
(7:41
PM
EDT).
Just
before
the
Apollo
12
flight,
it
was
noted
that
"Eagle"
was
still
likely
to
be
orbiting
the
moon.
Later
NASA
reports
mentioned
that
"Eagle's"
orbit
had
decayed
resulting
in
it
impacting
in
an
"uncertain
location"
on
the
lunar
surface.
The
location
is
uncertain
because
the
"Eagle"
ascent
stage
was
not
tracked
after
it
was
jettisoned
and
the
lunar
gravity
field
is
sufficiently
uncertain
to
make
the
orbit
of
the
spacecraft
unpredictable
after
a
short
time.
NASA
estimated
that
the
orbit
had
decayed
within
months
and
would
have
impacted
on
the
Moon.
On
July
23,
the
last
night
before
splashdown,
the
three
astronauts
made
a
television
broadcast
in
which
Collins
commented,
On
the
return
to
Earth,
the
Guam
tracking
station
failed,
which
would
have
prevented
communication
on
the
last
segment
of
the
Earth
return.
Repair
was
not
possible
until
a
staff
member
had
his
ten-year
old
son,
Greg
Force,
do
repairs
made
possible
by
his
small
hands.
Force
later
was
thanked
by
Armstrong.
Splashdown
and
quarantine.
On
July
24,
the
astronauts
returned
home
aboard
the
command
module
Columbia
just
before
dawn
at,
in
the
Pacific
Ocean
2,660 km
(1,440
nm)
east
of
Wake
Island,
or
380 km
(210 nm)
south
of
Johnston
Atoll,
and
24 km
(15
mi)
from
the
recovery
ship,
USS
"Hornet".
Initially
the
command
module
landed
upside
down
but
was
righted
in
several
minutes
by
flotation
bags
triggered
by
the
astronauts.
A
diver
from
the
Navy
helicopter
hovering
above
attached
an
anchor
to
the
command
module
to
prevent
it
from
drifting.
Additional
divers
attached
additional
flotation
collars
to
stabilize
the
module
and
position
rafts
for
astronaut
extraction.
Though
the
possibility
of
bringing
back
pathogen
from
the
lunar
surface
was
considered
remote,
it
was
not
considered
impossible
and
NASA
took
great
precautions
at
the
recovery
site.
Astronauts
were
provided
Biological
Isolation
Garment
(BIG
suit)
by
divers
which
were
worn
until
they
reached
isolation
facilities
onboard
the
Hornet.
Additionally
astronauts
were
rubbed
down
with
a
sodium-hypochlorite
solution
and
the
command
module
wiped
with
betadine
to
remove
any
lunar
dust
that
might
be
present.
The
raft
containing
decontamination
materials
was
then
intentionally
sunk.
A
second
Sea
King
helicopter
hoisted
the
astronauts
aboard
one
by
one
where
a
NASA
flight
surgeon
gave
each
a
brief
physical
check
during
the
half
mile
trip
back
to
the
Hornet.
After
touchdown
on
the
Hornet,
all
crew
exited
the
helicopter,
leaving
the
flight
surgeon
and
3
crew.
The
helicopter
was
then
lowered
into
hangar
bay
#2
where
the
astronauts
walked
the
30
feet
to
the
Mobile
Quarantine
Facility
(MQF)
where
they
would
begin
their
21
days
of
quarantine,
a
practice
that
would
continue
for
the
next
3
Apollo
missions
before
the
moon
was
proven
to
be
barren
of
life
and
quarantine
process
dropped
for
Apollo
XV
through
XVII.
President
Richard
Nixon
was
aboard
"Hornet"
to
personally
welcome
the
astronauts
back
to
Earth.
He
told
the
astronauts:
"As
a
result
of
what
you've
done,
the
world
has
never
been
closer
together
before."
Years
later,
it
was
publicly
revealed
that
Nixon
had
prepared
a
speech
to
be
given
if
the
mission
resulted
in
death.
The
lunar
module
had
not
been
tested
to
assess
if
it
could
launch
from
the
moon
surface.
After
Nixon
departed,
the
"Hornet"
was
brought
alongside
the
5
ton
command
module
where
it
was
placed
aboard
by
the
ship's
crane,
placed
on
a
dolly
and
moved
next
to
the
MQF.
The
Hornet
steamed
for
Pearl
Harbor
where
the
command
module
and
MQF
were
airlifted
to
the
Johnson
Space
Center.
The
astronauts
were
placed
in
quarantine
after
their
landing
on
the
moon
for
fear
that
the
moon
might
contain
undiscovered
pathogens,
and
that
the
astronauts
might
have
been
exposed
to
them
during
their
moon
walks.
(The
decision
to
do
so
was
made
in
accordance
with
the
recently
passed
Extra-Terrestrial
Exposure
Law).
However,
after
almost
three
weeks
in
confinement
(first
in
their
trailer
and
later
in
the
Lunar
Receiving
Laboratory
at
the
Manned
Spacecraft
Center),
the
astronauts
were
given
a
clean
bill
of
health.
On
August
13,
1969,
the
astronauts
exited
quarantine
to
the
cheers
of
the
American
public.
Parades
were
held
in
their
honor
in
New
York,
Chicago,
and
Los
Angeles
on
the
same
day.
A
few
weeks
later,
they
were
invited
by
Mexico
for
a
parade
honoring
them
in
Mexico
City.
That
evening
in
Los
Angeles
there
was
an
official
State
Dinner
to
celebrate
"Apollo
11",
attended
by
Members
of
Congress,
44
Governors,
the
Chief
Justice,
and
ambassadors
from
83
nations
at
the
Century
Plaza
Hotel.
President
Richard
Nixon
and
Vice
President
Spiro
T.
Agnew
honored
each
astronaut
with
a
presentation
of
the
Presidential
Medal
of
Freedom.
This
celebration
was
the
beginning
of
a
45-day
"Giant
Leap"
tour
that
brought
the
astronauts
to
25
foreign
countries
and
included
visits
with
prominent
leaders
such
as
Queen
Elizabeth
II
of
the
United
Kingdom.
Many
nations
would
honor
the
first
manned
moon
landing
by
issuing
"Apollo
11"
commemorative
postage
stamps
or
coins.
Also,
a
few
POWs
held
in
Vietnam
received
letters
from
home
a
few
months
after
the
landings
with
those
stamps
to
covertly
let
the
POWs
know
that
the
United
States
had
landed
men
on
the
moon.
On
September
16,
1969,
the
three
astronauts
spoke
before
a
joint
meeting
of
Congress
on
Capitol
Hill.
They
presented
two
U.S.
flags,
one
to
the
House
of
Representatives
and
the
other
to
the
Senate,
that
had
been
carried
to
the
surface
of
the
moon
with
them.
Spacecraft
location.
The
command
module
is
displayed
at
the
National
Air
and
Space
Museum,
Washington,
D.C..
It
is
placed
in
the
central
exhibition
hall
in
front
of
the
Jefferson
Drive
entrance,
sharing
the
main
hall
with
other
pioneering
flight
vehicles
such
as
the
"Spirit
of
St.
Louis",
the
Bell
X-1,
the
North
American
X-15,
Mercury
spacecraft
"Friendship
7",
and
Gemini
4.
The
quarantine
trailer,
the
flotation
collar,
and
the
righting
spheres
are
displayed
at
the
Smithsonian's
Udvar-Hazy
Center
annex
near
Washington
Dulles
International
Airport
in
Virginia.
In
2009
the
Lunar
Reconnaissance
Orbiter
imaged
the
various
Apollo
landing
sites
on
the
surface
of
the
moon
with
sufficient
resolution
to
see
the
descent
stages
of
the
lunar
modules,
scientific
instruments,
and
foot
trails
made
by
the
astronauts.
Mission
insignia.
The
patch
of
"Apollo
11"
was
designed
by
Collins,
who
wanted
a
symbol
for
"peaceful
lunar
landing
by
the
United
States."
He
chose
an
eagle
as
the
symbol,
put
an
olive
branch
in
its
beak,
and
drew
a
moon
background
with
the
earth
in
the
distance.
NASA
officials
said
the
talons
of
the
eagle
looked
too
"warlike"
and
after
some
discussion,
the
olive
branch
was
moved
to
the
claws.
The
crew
decided
the
Roman
numeral
XI
would
not
be
understood
in
some
nations
and
went
with
"Apollo
11";
they
decided
not
to
put
their
names
on
the
patch,
so
it
would
"be
representative
of
"everyone"
who
had
worked
toward
a
lunar
landing."
All
colors
are
natural,
with
blue
and
gold
borders
around
the
patch.
The
LM
was
named
"Eagle"
to
match
the
insignia.
When
the
Eisenhower
dollar
coin
was
released
a
few
years
later,
the
patch
design
provided
the
eagle
for
its
reverse
side.
The
design
was
retained
for
the
smaller
Susan
B.
Anthony
dollar
which
was
unveiled
in
1979,
ten
years
after
the
Apollo
11
mission.
40th
anniversary
events.
On
July
15,
2009,
LIFE.com
published
a
photo
gallery
of
never-before-seen
photos
of
Aldrin,
Collins,
and
Armstrong
in
the
days
before
their
mission.
LIFE
Photographer
Ralph
Morse
covered
the
astronauts
for
years—especially
in
the
months
leading
up
to
the
July
16,
1969
launch—chronicling
the
crew's
public
and
private
lives.
In
the
gallery,
Morse
talks
with
LIFE
about
the
astronauts,
the
moon
landing,
quarantine,
and
rare
and
never-before-published
photographs
capturing
that
thrilling
time.
From
July
16-24
2009
NASA
streamed
the
original
mission
audio
on
its
website
in
real
time
40
years
to
the
minute
after
the
events
occurred.
In
addition,
it
is
in
the
process
of
restoring
the
video
footage
and
have
released
a
preview
of
key
moments.
More
events
are
listed
at
the
website.
The
John
F.
Kennedy
Library
set
up
a
Flash
website
that
rebroadcasts
the
transmissions
of
Apollo
11
from
launch
to
landing
on
the
Moon.
It
was
carried
out
in
a
technically
brilliant
way
with
risks
taken...
that
would
be
inconceivable
in
the
risk-averse
world
of
today...The
Apollo
programme
is
arguably
the
greatest
technical
achievement
of
mankind
to
date...nothing
since
Apollo
has
come
close
[to]
the
excitement
that
was
generated
by
those
astronauts
-
Armstrong,
Aldrin
and
the
10
others
who
followed
them.
On
May
1,
2009,
Congress
introduced
a
bill
granting
the
three
astronauts
on
Apollo
11
a
Congressional
Gold
Medal,
the
highest
civilian
award
in
the
United
States.
The
bill
was
sponsored
by
Florida
Senator
Bill
Nelson
and
Florida
Congressman
Alan
Grayson.
---END.OF.DOCUMENT---
Apollo
8.
Apollo
8
was
the
first
human
spaceflight
mission
to
escape
from
the
gravitational
field
of
planet
Earth;
the
first
to
be
captured
by
and
escape
from
the
gravitational
field
of
another
celestial
body;
and
the
first
crewed
voyage
to
return
to
planet
Earth
from
another
celestial
body
-
Earth's
Moon.
The
three-man
crew
of
Mission
Commander
Frank
Borman,
Command
Module
Pilot
James
Lovell,
and
Lunar
Module
Pilot
William
Anders
became
the
first
humans
to
see
the
far
side
of
the
Moon
with
their
own
eyes,
as
well
as
the
first
humans
to
see
planet
Earth
from
beyond
low
Earth
orbit.
The
mission
was
accomplished
with
the
first
manned
launch
of
a
Saturn V
rocket.
Apollo
8
was
the
second
manned
mission
of
the
Apollo
Program.
Originally
planned
as
a
low
Earth
orbit
Lunar
Module/Command
Module
test,
the
mission
profile
was
changed
to
the
more
ambitious
lunar
orbital
flight
in
August 1968
when
the
Lunar
Module
scheduled
for
the
flight
became
delayed.
The
new
mission's
profile,
procedures
and
personnel
requirements
left
an
uncharacteristically
short
time
frame
for
training
and
preparation,
thus
placing
more
demands
than
usual
on
the
time,
talent,
and
discipline
of
the
crew.
After
launching
on
December
21,
1968,
the
crew
took
three
days
to
travel
to
the
Moon.
They
orbited
ten
times
over
the
course
of
20 hours,
during
which
the
crew
made
a
Christmas
Eve
television
broadcast
in
which
they
read
the
first
10
verses
from
the
Book
of
Genesis.
At
the
time,
the
broadcast
was
the
most
watched
TV
program
ever.
Apollo 8's
successful
mission
paved
the
way
for
Apollo
11
to
fulfill
U.S.
President
John
F.
Kennedy's
goal
of
landing
a
man
on
the
Moon
before
the
end
of
the
decade.
Backup
crew.
Lovell
was
originally
the
CMP
on
the
back-up
crew,
with
Michael
Collins
as
the
prime
crew's
CMP.
However,
Collins
was
replaced
in
July 1968,
after
suffering
a
cervical
disc
herniation
that
required
surgery
to
repair.
Aldrin
was
originally
the
backup
LMP.
When
Lovell
was
rotated
to
the
prime
crew,
no
one
with
experience
on
CSM
103
(the
specific
spacecraft
used
for
the
mission)
was
available,
so
Aldrin
was
moved
to
CMP
and
Fred
Haise
brought
in
as
backup
LMP.
Armstrong
went
on
to
command
Apollo
11,
where
Aldrin
was
returned
to
the
Lunar
Module
Pilot
position.
Michael
Collins
was
assigned
as
Command
Module
Pilot,
although
Aldrin
was
seated
in
the
CMP
position
for
Apollo
11's
launch
due
to
his
training
advantage
via
Apollo
8.
Fred
Haise
later
flew
on
Apollo
13.
Mission
control.
The
Earth-based
mission
control
teams
for
Apollo 8
consisted
of
astronauts
assigned
to
the
support
crew,
as
well
as
non-astronaut
flight
directors
and
their
staffs.
The
support
crew
members
were
not
trained
to
fly
the
mission,
but
were
able
to
stand
in
for
astronauts
in
meetings
and
be
involved
in
the
minutiae
of
mission
planning,
while
the
prime
and
backup
crews
trained.
They
also
served
as
capcoms
during
the
mission.
For
Apollo 8,
these
crew
members
included
astronauts
John
S.
Bull,
Vance
D.
Brand,
Gerald
P.
Carr,
and
Ken
Mattingly.
The
mission
control
teams
on
Earth
rotated
in
three
shifts,
each
led
by
a
flight
director.
The
directors
for
Apollo 8
included
Cliff
Charlesworth
(Green
team),
Glynn
Lunney
(Black
team),
and
Milton
Windler
(Maroon
team).
Mission
insignia.
The
triangular
shape
of
the
insignia
symbolizes
the
shape
of
the
Apollo
command
module.
It
shows
a
red
figure
8
looping
around
the
earth
and
moon
representing
the
mission
number
as
well
as
the
circumlunar
nature
of
the
mission.
On
the
red
number
8
are
the
names
of
the
three
astronauts.
The
initial
design
of
the
insignia
was
developed
by
Jim
Lovell.
Lovell
reportedly
sketched
the
initial
design
while
riding
in
the
backseat
of
a
T-38
flight
from
California
to
Houston,
shortly
after
learning
of
the
re-designation
of
the
flight
to
become
a
lunar
orbital
mission.
Planning.
Apollo
4
and
Apollo
6
had
been
"A"
missions,
each
launching
an
unmanned
Block I
production
model
of
the
Apollo
Command
and
Service
Modules
into
Earth
Orbit.,
scheduled
for
October 1968,
would
be
a
manned
Earth
Orbit
flight
of
the
CSM,
completing
the
objectives
for
Mission "C".
Further
missions
relied
on
the
readiness
of
the
Lunar
Module
(LM).
Production
of
the
LM
was
behind
schedule,
with
the
first
model
arriving
at
Cape
Canaveral
in
June 1968.
Even
then,
significant
defects
were
discovered,
leading
Grumman,
the
lead
contractor
for
the
LM,
to
predict
that
the
first
mission-ready
LM
would
not
be
ready
until
at
least
February 1969.
This
would
mean
delaying
the
proposed
"D"
mission
and
endangering
the
program's
goal
of
a
lunar
landing
before
the
end
of
1969.
Even
more
pressing
was
a
CIA
report
that
the
Soviets
were
expected
to
attempt
to
send
cosmonauts
on
a
Zond
circumlunar
mission
before
the
end
of
the
year.
If
the
Soviets
were
successful
in
being
first
to
get
humans
around
the
Moon,
then
that
would
greatly
detract
from
having
Americans
being
first
to
land
on
the
Moon.
George
Low,
the
Manager
of
the
Apollo
Spacecraft
Program
Office,
proposed
a
solution
in
August.
Since
the
Service
Module
(CSM)
would
be
ready
three
months
before
the
Lunar
Module,
a
CSM-only
mission
could
be
flown
in
December 1968.
Instead
of
just
repeating
the
"C"
mission
flight
of
Apollo 7,
this
CSM
could
be
sent
all
the
way
to
the
Moon,
with
the
possibility
of
entering
a
lunar
orbit.
The
new
mission
would
also
allow
NASA
to
test
lunar
landing
procedures
that
would
otherwise
have
to
wait
until
Apollo
10,
the
scheduled
"F"
mission.
Almost
every
senior
manager
at
NASA
agreed
with
this
new
mission,
citing
both
confidence
in
the
hardware
and
personnel,
and
the
potential
for
a
significant
morale
boost
provided
by
a
circumlunar
flight.
The
only
person
who
needed
some
convincing
was
James
E.
Webb,
the
NASA
administrator.
With
the
rest
of
his
agency
in
support
of
the
new
mission,
Webb
eventually
approved
the
mission
change.
The
mission
was
officially
changed
from
a
"D"
mission
to
a
"C-Prime"
Lunar
Orbit
mission,
but
was
still
referred
to
in
press
releases
as
an
Earth
Orbit
mission
at
Webb's
direction.
No
public
announcement
was
made
about
the
change
in
mission
until
November
12,
three
weeks
after
Apollo
7's
successful
Earth
Orbit
mission
and
less
than
40
days
before
launch.
With
the
change
in
mission
for
Apollo 8,
Director
of
Flight
Crew
Operations
Deke
Slayton
decided
to
swap
the
crews
of
the
D
and
E
missions.
James
McDivitt,
the
original
commander
of
the
D
mission,
has
said
he
was
never
offered
the
circumlunar
flight,
but
would
probably
have
turned
it
down,
as
he
wanted
to
fly
the
lunar
module.
Borman,
on
the
other
hand,
jumped
at
the
chance:
his
original
mission
would
just
have
been
a
repeat
of
the
previous
flight,
except
in
a
higher
orbit.
This
swap
also
meant
a
swap
of
spacecraft,
requiring
Borman's
crew
to
use
CSM-103,
while
McDivitt's
crew
would
use
CSM-104.
On
September
9,
the
crew
entered
the
simulators
to
begin
their
preparation
for
the
flight.
By
the
time
the
mission
flew,
the
crew
had
spent
seven hours
training
for
every
actual
hour
of
flight.
Although
all
crew
members
were
trained
in
all
aspects
of
the
mission,
it
was
necessary
to
specialize.
Borman,
as
commander,
was
given
training
on
controlling
the
spacecraft
during
the
re-entry.
Lovell
was
trained
on
navigating
the
spacecraft
in
case
communication
was
lost
with
the
Earth.
Anders
was
placed
in
charge
of
checking
that
the
spacecraft
was
in
working
order.
The
crew,
now
living
in
the
crew
quarters
at
Kennedy
Space
Center,
received
a
visit
from
Charles
Lindbergh
and
his
wife,
Anne
Morrow
Lindbergh,
the
night
before
the
launch.
They
talked
about
how
before
his
1927
flight,
Lindbergh
had
used
a
piece
of
string
to
measure
the
distance
from
New
York
City
to
Paris
on
a
globe
and
from
that
calculated
the
fuel
needed
for
the
flight.
The
total
was
a
tenth
of
the
amount
that
the
Saturn V
would
burn
every
second.
The
next
day,
the
Lindberghs
watched
the
launch
of
Apollo 8
from
a
nearby
dune.
Anne
Morrow
Lindbergh
would
later
write
a
book
about
the
Apollo
program,
entitled
"Earth
Shine",
which
mentions
both
the
launch
and
the
mission.
Saturn
V.
The
Saturn V
rocket
used
by
Apollo 8
was
designated
SA-503,
or
the
"03rd"
model
of
the
Saturn V
("5")
Rocket
to
be
used
in
the
Saturn-Apollo
("SA")
program.
When
it
was
erected
in
the
Vertical
Assembly
Building
on
December
20,
1967,
it
was
thought
that
the
rocket
would
be
used
for
an
unmanned
Earth-orbit
test
flight
carrying
a
boilerplate
Command/Service
Module.
Apollo 6
had
suffered
several
major
problems
during
its
April 1968
flight,
including
severe
pogo
oscillation
during
its
first
stage,
two
second
stage
engine
failures,
and
a
third
stage
that
failed
to
reignite
in
orbit.
Without
assurances
that
these
problems
had
been
rectified,
NASA
administrators
could
not
justify
risking
a
manned
mission
until
additional
unmanned
test
flights
proved
that
the
Saturn V
was
ready.
Teams
from
the
Marshall
Space
Flight
Center
(MSFC)
went
to
work
on
the
problems.
Of
primary
concern
was
the
pogo
oscillation,
which
would
not
only
hamper
engine
performance,
but
could
exert
significant
g-forces
on
a
crew.
A
task
force
of
contractors,
NASA
agency
representatives,
and
MSFC
researchers
concluded
that
the
engines
vibrated
at
a
frequency
similar
to
the
frequency
at
which
the
spacecraft
itself
vibrated,
causing
a
resonance
effect
that
induced
oscillations
in
the
rocket.
A
system
using
helium
gas
to
absorb
some
of
these
vibrations
was
installed.
Of
equal
importance
was
the
failure
of
three
engines
during
flight.
Researchers
quickly
determined
that
a
leaking
hydrogen
fuel
line
ruptured
when
exposed
to
vacuum,
causing
a
loss
of
fuel
pressure
in
engine
two.
When
an
automatic
shutoff
attempted
to
close
the
liquid
hydrogen
valve
and
shut
down
engine
two,
it
accidentally
shut
down
engine
three's
liquid
oxygen
due
to
a
miswired
connection.
As
a
result,
engine
three
failed
within
one
second
of
engine
two's
shutdown.
Further
investigation
revealed
the
same
problem
for
the
third-stage
engine —
a
faulty
igniter
line.
The
team
modified
the
igniter
lines
and
fuel
conduits,
hoping
to
avoid
similar
problems
on
future
launches.
The
teams
tested
their
solutions
in
August 1968
at
the
Marshall
Space
Flight
Center.
A
Saturn
stage
IC
was
equipped
with
shock
absorbing
devices
to
demonstrate
the
team's
solution
to
the
problem
of
pogo
oscillation,
while
a
Saturn
Stage II
was
retrofitted
with
modified
fuel
lines
to
demonstrate
their
resistance
to
leaks
and
ruptures
in
vacuum
conditions.
Once
NASA
administrators
were
convinced
that
the
problems
were
solved,
they
gave
their
approval
for
a
manned
mission
using
SA-503.
The
Apollo 8
spacecraft
was
placed
on
top
of
the
rocket
on
September
21
and
the
rocket
made
the
slow
3-mile
(5 km)
journey
to
the
launch
pad
on
October
9.
Testing
continued
all
through
December
until
the
day
before
launch,
including
various
levels
of
readiness
testing
from
5
December
through
11
December.
Final
testing
of
modifications
to
address
the
problems
of
pogo
oscillation,
ruptured
fuel
lines,
and
bad
igniter
lines
took
place
on
18
December,
a
mere
three
days
before
the
scheduled
launch.
Launch
and
trans-lunar
injection.
Apollo 8
launched
at
7:51:00 a.m.
Eastern
Standard
Time
on
December
21,
1968,
using
the
Saturn V's
three
stages,
S-IC,
S-II,
and
S-IVB,
to
achieve
Earth
orbit.
The
launch
phase
experienced
only
three
minor
problems:
The
engines
of
the
first
stage,
S-IC,
underperformed
by
0.75%,
causing
the
engines
to
burn
for
2.45 seconds
longer
than
planned,
and
toward
the
end
of
the
second
stage
burn,
S-II,
the
rocket
underwent
pogo
oscillations.
Frank
Borman
estimated
the
oscillations
were
approximately
and
(±2.5 m/s²).
The
apogee
was
also
slightly
higher
than
the
planned
circular
orbit
of.
In
its
first
manned
mission,
the
Saturn V
rocket
placed
Apollo 8
into
a
Earth
orbit
with
a
period
of
88 minutes
and
10 seconds.
All
three
rocket
stages
fired
during
launch;
the
S-IC
and
S-II
detached
during
launch.
The
S-IC
impacted
the
Atlantic
Ocean
at
and
the
S-II
second
stage
at.
The
third
stage
of
the
rocket,
S-IVB,
assisted
in
driving
the
craft
into
Earth
orbit
but
remained
attached
to
later
perform
the
Trans-Lunar
Injection
(TLI),
the
burn
that
would
put
the
spacecraft
on
a
trajectory
to
the
Moon.
Once
in
Earth
orbit,
both
the
Apollo 8
crew
and
Mission
Control
spent
the
next
2 hours
and
38 minutes
checking
that
the
spacecraft
was
in
proper
working
order
and
ready
for
TLI.
The
proper
operation
of
third
stage
of
the
rocket,
S-IVB
was
crucial;
In
the
last
unmanned
test,
the
S-IVB
had
failed
to
re-ignite
for
TLI.
During
the
flight,
three
fellow
astronauts
served
on
the
ground
as
capsule
communicators
(usually
referred
to
as
"CAPCOMs")
on
a
rotating
schedule.
The
CAPCOMs
were
the
only
people
who
regularly
communicated
with
the
crew.
Michael
Collins
was
the
first
CAPCOM
on
duty
and
at
2 hours,
27 minutes
and
22 seconds
after
launch
radioed,
"Apollo 8.
You
are
Go
for
TLI".
This
communication
signified
that
Mission
Control
had
given
official
permission
for
Apollo 8
to
go
to
the
moon.
Over
the
next
twelve minutes
before
the
TLI
burn,
the
Apollo 8
crew
continued
to
monitor
the
spacecraft
and
the
rocket.
The
S-IVB
third
stage
rocket
ignited
on
time
and
burned
perfectly
for
5 minutes
and
17 seconds.
The
burn
increased
the
velocity
of
Apollo 8
to
and
the
spacecraft's
altitude
at
the
end
of
the
burn
was.
At
this
time,
the
crew
also
set
the
record
for
the
highest
speed
humans
had
ever
traveled.
Although
the
S-IVB
was
sufficiently
powerful
to
accelerate
the
CSM
to
the
Earth's
escape
velocity,
the
TLI
burn
did
not
achieve
this;
the
CSM
remained
in
an
elongated
elliptical
Earth
orbit,
and
would
have
returned
to
the
Earth
if
it
had
not
encountered
the
Moon's
gravitational
field.
After
the
S-IVB
had
performed
its
required
tasks,
it
was
jettisoned.
The
crew
then
rotated
the
spacecraft
to
take
some
photographs
of
the
spent
stage
and
then
practiced
flying
in
formation
with
it.
As
the
crew
rotated
the
spacecraft,
they
had
their
first
views
of
the
Earth
as
they
moved
away
from
it.
This
marked
the
first
time
humans
could
view
the
whole
Earth
at
once.
Borman
became
worried
that
the
S-IVB
was
staying
too
close
to
the
Command/Service
Module
and
suggested
to
Mission
Control
that
the
crew
perform
a
separation
maneuver.
Mission
Control
first
suggested
pointing
the
spacecraft
towards
Earth
and
using
the
Reaction
Control
System
(RCS)
thrusters
on
the
Service
Module
to
add
away
from
the
Earth,
but
Borman
did
not
want
to
lose
sight
of
the
S-IVB.
After
discussion,
the
crew
and
Mission
Control
decided
to
burn
in
this
direction,
but
at
instead.
These
discussions
put
the
crew
an
hour
behind
their
flight
plan.
Five hours
after
launch,
Mission
Control
sent
a
command
to
the
S-IVB
booster
to
vent
its
remaining
fuel
through
its
engine
bell
to
change
the
booster's
trajectory.
This
S-IVB
would
then
pass
the
Moon
and
enter
into
a
solar
orbit,
posing
no
further
hazard
to
Apollo 8.
The
S-IVB
subsequently
went
into
a
solar
orbit
with
an
inclination
of
23.47°
and
a
period
of
340.80 days.
The
Apollo 8
crew
were
the
first
humans
to
pass
through
the
Van
Allen
radiation
belts,
which
extend
up
to
from
Earth.
Scientists
predicted
that
passing
through
the
belts
quickly
at
the
spacecraft's
high
speed
would
cause
a
radiation
dosage
of
no
more
than
a
chest
X-ray,
or
1
milligray
(during
the
course
of
a
year,
the
average
human
receives
a
dose
of
2 to 3 mGy).
To
record
the
actual
radiation
dosages,
each
crew
member
wore
a
Personal
Radiation
Dosimeter
that
transmitted
data
to
Earth
as
well
as
three
passive
film
dosimeters
that
showed
the
cumulative
radiation
experienced
by
the
crew.
By
the
end
of
the
mission,
the
crew
experienced
an
average
radiation
dose
of
1.6
mGy.
Lunar
trajectory.
Jim
Lovell's
main
job
as
Command
Module
Pilot
was
as
navigator.
Although
Mission
Control
performed
all
of
the
actual
navigation
calculation,
it
was
necessary
to
have
a
crew
member
serving
as
navigator
so
that
the
crew
could
successfully
return
to
Earth
in
case
of
communication
loss
with
Mission
Control.
Lovell
navigated
by
star
sightings
using
a
sextant
built
into
the
spacecraft,
measuring
the
angle
between
a
star
and
the
Earth's
(or
the
Moon's)
horizon.
This
task
proved
to
be
difficult,
as
a
large
cloud
of
debris
around
the
spacecraft
formed
by
the
venting
S-IVB
made
it
hard
to
distinguish
the
stars.
By
seven hours
into
the
mission,
the
crew
was
about
one
hour
and
40 minutes
behind
flight
plan
due
to
the
issues
of
moving
away
from
the
S-IVB
and
Lovell's
obscured
star
sightings.
The
crew
now
placed
the
spacecraft
into
Passive
Thermal
Control
(PTC),
also
known
as
"barbecue"
mode.
PTC
involved
the
spacecraft
rotating
about
once
per
hour
along
its
long
axis
to
ensure
even
heat
distribution
across
the
surface
of
the
spacecraft.
In
direct
sunlight,
the
spacecraft
could
be
heated
to
over
200 °C
while
the
parts
in
shadow
would
be
−100 °C.
These
temperatures
could
cause
the
heat
shield
to
crack
or
propellant
lines
to
burst.
As
it
was
impossible
to
get
a
perfect
roll,
the
spacecraft
actually
swept
out
a
cone
as
it
rotated.
The
crew
had
to
make
minor
adjustments
every
half
hour
as
the
cone
pattern
got
larger
and
larger.
The
first
mid-course
correction
came
11 hours
into
the
flight.
Testing
on
the
ground
had
shown
that
the
Service
Propulsion
System
(SPS)
engine
had
a
small
chance
of
exploding
when
burned
for
long
periods
unless
its
combustion
chamber
was
"coated"
first.
Burning
the
engine
for
a
short
period
would
accomplish
coating.
This
first
correction
burn
was
only
2.4 seconds
and
added
about
prograde
(in
the
direction
of
travel).
This
change
was
less
than
the
planned
due
to
a
bubble
of
helium
in
the
oxidizer
lines
causing
lower
than
expected
fuel
pressure.
The
crew
had
to
use
the
small
Reaction
Control
System
(RCS)
thrusters
to
make
up
the
shortfall.
Two
later
planned
mid-course
corrections
were
canceled
as
the
Apollo 8
trajectory
was
found
to
be
perfect.
Eleven hours
into
the
flight,
the
crew
had
been
awake
for
over
16 hours.
Before
launch,
NASA
had
decided
that
at
least
one
crew
member
should
be
awake
at
all
times
to
deal
with
any
issues
that
might
arise.
Borman
started
the
first
sleep
shift,
but
between
the
constant
radio
chatter
and
mechanical
noises,
he
found
sleep
difficult.
About
an
hour
after
starting
his
sleep
shift,
Borman
requested
clearance
to
take
a
Seconal
sleeping
pill.
However,
the
pill
had
little
effect.
Borman
eventually
fell
asleep
but
then
awoke
feeling
ill.
He
vomited
twice
and
had
a
bout
of
diarrhea
that
left
the
spacecraft
full
of
small
globules
of
vomit
and
feces
that
the
crew
cleaned
up
to
the
best
of
their
ability.
Borman
initially
decided
that
he
did
not
want
everyone
to
know
about
his
medical
problems,
but
Lovell
and
Anders
wanted
to
inform
Mission
Control.
The
crew
decided
to
use
the
Data
Storage
Equipment
(DSE),
which
could
tape
voice
recordings
and
telemetry
and
dump
them
to
Mission
Control
at
high
speed.
After
recording
a
description
of
Borman's
illness
they
requested
that
Mission
Control
check
the
recording,
stating
that
they
"would
like
an
evaluation
of
the
voice
comments".
The
Apollo 8
crew
and
Mission
Control
medical
personnel
held
a
conference
using
an
unoccupied
second
floor
control
room
(there
were
two
identical
control
rooms
in
Houston
on
the
second
and
third
floor,
only
one
of
which
was
used
during
a
mission).
The
conference
participants
decided
that
there
was
little
to
worry
about
and
that
Borman's
illness
was
either
a
24-hour
flu,
as
Borman
thought,
or
a
reaction
to
the
sleeping
pill.
Researchers
now
believe
that
he
was
suffering
from
space
adaptation
syndrome,
which
affects
about
a
third
of
astronauts
during
their
first
day
in
space
as
their
vestibular
system
adapts
to
weightlessness.
Space
adaptation
syndrome
had
not
been
an
issue
on
previous
spacecraft
(Mercury
and
Gemini),
as
those
astronauts
were
unable
to
move
freely
in
the
comparatively
smaller
cabins
of
those
spacecraft.
The
increased
cabin
space
in
the
Apollo
Command
Module
afforded
astronauts
greater
freedom
of
movement,
contributing
to
symptoms
of
spacesickness
for
Borman
and,
later,
astronaut
Russell
Schweickart
during
Apollo
9.
The
cruise
phase
was
a
relatively
uneventful
part
of
the
flight,
except
for
the
crew
checking
that
the
spacecraft
was
in
working
order
and
that
they
were
on
course.
During
this
time,
NASA
scheduled
a
television
broadcast
at
31 hours
after
launch.
The
Apollo 8
crew
used
a
2 kg
camera
that
broadcast
in
black-and-white
only,
using
a
Vidicon
tube.
The
camera
had
two
lenses,
a
very
wide-angle
(160°)
lens,
and
a
telephoto
(9°)
lens.
During
this
first
broadcast,
the
crew
gave
a
tour
of
the
spacecraft
and
attempted
to
show
how
the
Earth
appeared
from
space.
However,
difficulties
aiming
the
narrow-angle
lens
without
the
aid
of
a
monitor
to
show
what
it
was
looking
at
made
showing
the
Earth
impossible.
Additionally,
the
Earth
image
became
saturated
by
any
bright
source
without
proper
filters.
In
the
end,
all
the
crew
could
show
the
people
watching
back
on
Earth
was
a
bright
blob.
After
broadcasting
for
17 minutes,
the
rotation
of
the
spacecraft
took
the
high-gain
antenna
out
of
view
of
the
receiving
stations
on
Earth
and
they
ended
the
transmission
with
Lovell
wishing
his
mother
a
happy
birthday.
By
this
time,
the
crew
had
completely
abandoned
the
planned
sleep
shifts.
Lovell
went
to
sleep
32½ hours
into
the
flight —
3½ hours
before
he
had
planned
to.
A
short
while
later,
Anders
also
went
to
sleep
after
taking
a
sleeping
pill.
The
crew
was
unable
to
see
the
Moon
for
much
of
the
outward
cruise.
Two
factors
made
the
Moon
almost
impossible
to
see
from
inside
the
spacecraft:
three
of
the
five
windows
fogging
up
due
to
out-gassed
oils
from
the
silicone
sealant,
and
the
attitude
required
for
the
PTC.
It
was
not
until
the
crew
had
gone
behind
the
Moon
that
they
would
be
able
to
see
it
for
the
first
time.
The
Apollo 8
made
a
second
television
broadcast
at
55 hours
into
the
flight.
This
time,
the
crew
rigged
up
filters
meant
for
the
still
cameras
so
they
could
acquire
images
of
the
Earth
through
the
telephoto
lens.
Although
difficult
to
aim,
as
they
had
to
maneuver
the
entire
spacecraft,
the
crew
was
able
to
broadcast
back
to
Earth
the
first
television
pictures
of
the
Earth.
The
crew
spent
the
transmission
describing
the
Earth
and
what
was
visible
and
the
colors
they
could
see.
The
transmission
lasted
23 minutes.
Lunar
sphere
of
influence.
At
about
55 hours
and
40 minutes
into
the
flight,
the
crew
of
Apollo 8
became
the
first
humans
to
enter
the
gravitational
sphere
of
influence
of
another
celestial
body.
In
other
words,
the
effect
of
the
Moon's
gravitational
force
on
Apollo 8
became
stronger
than
that
of
the
Earth.
At
the
time
it
happened,
Apollo 8
was
from
the
Moon
and
had
a
speed
of
relative
to
the
Moon.
This
historic
moment
was
of
little
interest
to
the
crew
since
they
were
still
calculating
their
trajectory
with
respect
to
the
launch
pad
at
Kennedy
Space
Center.
They
would
continue
to
do
so
until
they
performed
their
last
mid-course
correction,
switching
to
a
reference
frame
based
on
ideal
orientation
for
the
second
engine
burn
they
would
make
in
lunar
orbit.
It
was
only
thirteen hours
until
they
would
be
in
lunar
orbit.
The
last
major
event
before
Lunar
Orbit
Insertion
was
a
second
mid-course
correction.
It
was
in
retrograde
(against
direction
of
travel)
and
slowed
the
spacecraft
down
by,
effectively
lowering
the
closest
distance
that
the
spacecraft
would
pass
the
moon.
At
exactly
61 hours
after
launch,
about
from
the
Moon,
the
crew
burned
the
RCS
for
11 seconds.
They
would
now
pass
from
the
lunar
surface.
At
64 hours
into
the
flight,
the
crew
began
to
prepare
for
Lunar
Orbit
Insertion-1
(LOI-1).
This
maneuver
had
to
be
performed
perfectly,
and
due
to
orbital
mechanics
had
to
be
on
the
far
side
of
the
Moon,
out
of
contact
with
the
Earth.
After
Mission
Control
was
polled
for
a
Go/No
Go
decision,
the
crew
was
told
at
68 hours,
they
were
Go
and
"riding
the
best
bird
we
can
find".
At
68 hours
and
58 minutes,
the
spacecraft
went
behind
the
Moon
and
out
of
radio
contact
with
the
Earth.
With
10 minutes
before
the
LOI-1,
the
crew
began
one
last
check
of
the
spacecraft
systems
and
made
sure
that
every
switch
was
in
the
correct
place.
At
that
time,
they
finally
got
their
first
glimpses
of
the
Moon.
They
had
been
flying
over
the
unlit
side,
and
it
was
Lovell
who
saw
the
first
shafts
of
sunlight
obliquely
illuminating
the
lunar
surface.
The
LOI
burn
was
only
two minutes
away,
so
the
crew
had
little
time
to
appreciate
the
view.
Lunar
orbit.
The
SPS
ignited
at
69 hours,
8 minutes,
and
16 seconds
after
launch
and
burned
for
4 minutes
and
13 seconds,
placing
the
Apollo 8
spacecraft
in
orbit
around
the
Moon.
The
crew
described
the
burn
as
being
the
longest
four
minutes
of
their
lives.
If
the
burn
had
not
lasted
exactly
the
correct
amount
of
time,
the
spacecraft
could
have
ended
up
in
a
highly
elliptical
lunar
orbit
or
even
flung
off
into
space.
If
it
lasted
too
long
they
could
have
impacted
the
Moon.
After
making
sure
the
spacecraft
was
working,
they
finally
had
a
chance
to
look
at
the
Moon,
which
they
would
orbit
for
the
next
20 hours.
On
Earth,
Mission
Control
continued
to
wait.
If
the
crew
had
not
burned
the
engine
or
the
burn
had
not
lasted
the
planned
length
of
time,
the
crew
would
appear
early
from
behind
the
Moon.
However,
this
time
came
and
went
without
Apollo 8
reappearing.
Exactly
at
the
calculated
moment,
the
signal
was
received
from
the
spacecraft,
indicating
it
was
in
a
orbit
about
the
Moon.
Lovell
continued
to
describe
the
terrain
they
were
passing
over.
One
of
the
crew's
major
tasks
was
reconnaissance
of
planned
future
landing
sites
on
the
Moon,
especially
one
in
Mare
Tranquillitatis
that
would
be
the
Apollo 11
landing
site.
The
launch
time
of
Apollo 8
had
been
chosen
to
give
the
best
lighting
conditions
for
examining
the
site.
A
film
camera
had
been
set
up
in
one
of
the
spacecraft
windows
to
record
a
frame
every
second
of
the
Moon
below.
Bill
Anders
spent
much
of
the
next
20 hours
taking
as
many
photographs
as
possible
of
targets
of
interest.
By
the
end
of
the
mission
the
crew
had
taken
700
photographs
of
the
Moon
and
150
of
the
Earth.
Throughout
the
hour
that
the
spacecraft
was
in
contact
with
Earth,
Borman
kept
asking
how
the
data
for
the
SPS
looked.
He
wanted
to
make
sure
that
the
engine
was
working
and
could
be
used
to
return
early
to
the
Earth
if
necessary.
He
also
asked
that
they
receive
a
Go/No
Go
decision
before
they
passed
behind
the
Moon
on
each
orbit.
As
they
reappeared
for
their
second
pass
in
front
of
the
Moon,
the
crew
set
up
the
equipment
to
broadcast
a
view
of
the
lunar
surface.
Anders
described
the
craters
that
they
were
passing
over.
At
the
end
of
this
second
orbit
they
performed
the
eleven-second
LOI-2
burn
of
the
SPS
to
circularize
the
orbit
to.
Through
the
next
two
orbits,
the
crew
continued
to
keep
check
of
the
spacecraft
and
to
observe
and
photograph
the
Moon.
During
the
third
pass,
Borman
read
a
small
prayer
for
his
church.
He
was
scheduled
to
participate
in
a
service
at
St.
Christopher's
Episcopal
Church
near
Seabrook,
Texas,
but
due
to
the
Apollo 8
flight
was
unable.
A
fellow
parishioner
and
engineer
at
Mission
Control,
Rod
Rose,
suggested
that
Borman
read
the
prayer
which
could
be
recorded
and
then
replayed
during
the
service.
Earthrise.
When
the
spacecraft
came
out
from
behind
the
Moon
for
its
fourth
pass
across
the
front,
the
crew
witnessed
Earthrise
for
the
first
time
in
human
history.
Borman
saw
the
Earth
emerging
from
behind
the
lunar
horizon
and
called
in
excitement
to
the
others,
taking
a
black-and-white
photo
as
he
did
so.
In
the
ensuing
scramble
Anders
took
the
more
famous
colour
photo,
later
picked
by
"Life"
magazine
as
one
of
its
hundred
photos
of
the
century.
Due
to
the
synchronous
rotation
of
the
Moon
about
the
Earth,
Earthrise
is
not
generally
visible
from
the
Lunar
surface.
Earthrise
is
generally
only
visible
when
orbiting
the
Moon,
other
than
at
selected
places
near
the
Moon's
limb,
where
libration
carries
the
Earth
slightly
above
and
below
the
lunar
horizon.
Anders
continued
to
take
photographs
while
Lovell
assumed
control
of
the
spacecraft
so
Borman
could
rest.
Despite
the
difficulty
resting
in
the
cramped
and
noisy
spacecraft,
Borman
was
able
to
sleep
for
two
orbits,
awakening
periodically
to
ask
questions
about
their
status.
Borman
awoke
fully,
however,
when
he
started
to
hear
his
fellow
crew
members
make
mistakes.
They
were
beginning
to
not
understand
questions
and
would
have
to
ask
for
the
answers
to
be
repeated.
Borman
realized
that
everyone
was
extremely
tired
having
not
had
a
good
night's
sleep
in
over
three
days.
Taking
command,
he
ordered
Anders
and
Lovell
to
get
some
sleep
and
that
the
rest
of
the
flight
plan
regarding
observing
the
Moon
be
scrubbed.
At
first
Anders
protested
saying
that
he
was
fine,
but
Borman
would
not
be
swayed.
At
last
Anders
agreed
as
long
as
Borman
would
set
up
the
camera
to
continue
to
take
automatic
shots
of
the
Moon.
Borman
also
remembered
that
there
was
a
second
television
broadcast
planned,
and
with
so
many
people
expected
to
be
watching
he
wanted
the
crew
to
be
alert.
For
the
next
two
orbits
Anders
and
Lovell
slept
while
Borman
sat
at
the
helm.
On
subsequent
Apollo
missions,
crews
would
avoid
this
situation
by
sleeping
on
the
same
schedule.
As
they
rounded
the
Moon
for
the
ninth
time,
the
second
television
transmission
began.
Borman
introduced
the
crew,
followed
by
each
man
giving
his
impression
of
the
lunar
surface
and
what
it
was
like
to
be
orbiting
the
Moon.
Borman
described
it
as
being
"a
vast,
lonely,
forbidding
expanse
of
nothing."
Then,
after
talking
about
what
they
were
flying
over,
Anders
said
that
the
crew
had
a
message
for
all
those
on
Earth.
Each
man
on
board
read
a
section
from
the
Biblical
creation
story
(verses
1-10)
from
the
Book
of
Genesis.
Borman
finished
the
broadcast
by
wishing
a
Merry
Christmas
to
everyone
on
Earth.
His
message
appeared
to
sum
up
the
feelings
that
all
three
crewmen
had
from
their
vantage
point
in
lunar
orbit.
Borman
said,
"And
from
the
crew
of
Apollo
8,
we
close
with
good
night,
good
luck,
and
a
Merry
Christmas,
God
bless
all
of
you,
"all
of
you
on
the
good
Earth".
The
only
task
left
for
the
crew
at
this
point
was
to
perform
the
Trans-Earth
Injection
(TEI),
which
was
scheduled
for
2½ hours
after
the
end
of
the
television
transmission.
The
TEI
was
the
most
critical
burn
of
the
flight,
as
any
failure
of
the
SPS
to
ignite
would
strand
the
crew
in
Lunar
orbit,
with
little
hope
of
escape.
As
with
the
previous
burn,
the
crew
had
to
perform
the
maneuver
above
the
far
side
of
the
Moon,
out
of
contact
with
Earth.
The
burn
occurred
exactly
on
time.
The
spacecraft
telemetry
was
reacquired
as
it
re-emerged
from
behind
the
Moon
at
89 hours,
28 minutes,
and
39 seconds,
the
exact
time
calculated.
When
voice
contact
was
regained,
Lovell
announced,
"Please
be
informed,
there
is
a
Santa
Claus",
to
which
Ken
Mattingly,
the
current
CAPCOM,
replied,
"That's
affirmative,
you
are
the
best
ones
to
know".
The
spacecraft
began
its
journey
back
to
Earth
on
December
25,
Christmas
Day.
Unplanned
manual
re-alignment.
Later,
Lovell
used
some
otherwise
idle
time
to
do
some
navigational
sightings,
maneuvering
the
module
to
view
various
stars
by
using
the
computer
keyboard.
However,
he
accidentally
erased
some
of
the
computer's
memory,
which
caused
the
inertial
measuring
unit
(IMU)
to
think
the
module
was
in
the
same
relative
position
it
had
been
in
before
lift-off
and
fire
the
thrusters
to
"correct"
the
module's
attitude.
Once
the
crew
realized
why
the
computer
had
changed
the
module's
attitude,
they
realized
they
would
have
to
re-enter
data
that
would
tell
the
computer
its
real
position.
It
took
Lovell
ten minutes
to
figure
out
the
right
numbers,
using
the
thrusters
to
get
the
stars
Rigel
and
Sirius
aligned,
and
another
fifteen minutes
to
enter
the
corrected
data
into
the
computer.
Sixteen
months
later,
Lovell
would
once
again
have
to
perform
a
similar
manual
re-alignment,
under
more
critical
conditions,
during
the
Apollo
13
mission,
after
that
module's
IMU
had
to
be
turned
off
to
conserve
energy.
In
his
1994
book,
',
Lovell
wrote,
"My
training
[on
Apollo
8]
came
in
handy!".
In
that
book
he
dismissed
the
incident
as
a
"planned
experiment",
requested
by
the
ground
crew.
However,
in
subsequent
interviews
Lovell
has
acknowledged
that
the
incident
was
an
accident,
caused
by
his
mistake.
Cruise
back
to
Earth
and
re-entry.
The
cruise
back
to
Earth
was
mostly
a
time
for
the
crew
to
relax
and
monitor
the
spacecraft.
As
long
as
the
trajectory
specialists
had
calculated
everything
correctly,
the
spacecraft
would
re-enter
2½ days
after
TEI
and
splashdown
in
the
Pacific.
On
Christmas
afternoon,
the
crew
made
their
fifth
television
broadcast.
This
time
they
gave
a
tour
of
the
spacecraft,
showing
how
an
astronaut
lived
in
space.
When
they
had
finished
broadcasting
they
found
a
small
present
from
Deke
Slayton
in
the
food
locker—real
turkey
with
stuffing
and
three
miniature
bottles
of
brandy
(which
remained
unopened).
There
were
also
small
presents
to
the
crew
from
their
wives.
The
next
day,
at
about
124
hours
into
the
mission,
the
sixth
and
final
TV
transmission
showed
the
mission's
best
video
images
of
the
earth,
in
a
short
four
minute
broadcast.
After
two
uneventful
days
the
crew
prepared
for
re-entry.
The
computer
would
control
the
re-entry
and
all
the
crew
had
to
do
was
put
the
spacecraft
in
the
correct
attitude,
blunt
end
forward.
If
the
computer
broke
down,
Borman
would
take
over.
Once
the
Command
Module
was
separated
from
the
Service
Module,
the
astronauts
were
committed
to
re-entry.
Six minutes
before
they
hit
the
top
of
the
atmosphere,
the
crew
saw
the
Moon
rising
above
the
Earth's
horizon,
just
as
had
been
predicted
by
the
trajectory
specialists.
As
they
hit
the
thin
outer
atmosphere
they
noticed
it
was
becoming
hazy
outside
as
glowing
plasma
formed
around
the
spacecraft.
The
spacecraft
started
slowing
down
and
the
deceleration
peaked
at
6 g
(59 m/s²).
With
the
computer
controlling
the
descent
by
changing
the
attitude
of
the
spacecraft,
Apollo 8
rose
briefly
like
a
skipping
stone
before
descending
to
the
ocean.
At
the
drogue
parachute
stabilized
the
spacecraft
and
was
followed
at
by
the
three
main
parachutes.
The
spacecraft
splashdown
position
was
estimated
to
be.
When
it
hit
the
water,
the
parachutes
dragged
the
spacecraft
over
and
left
it
upside
down,
in
what
was
termed
Stable 2
position.
As
they
were
buffeted
by
a
swell,
Borman
was
sick,
waiting
for
the
three
flotation
balloons
to
right
the
spacecraft.
It
was
43 minutes
after
splashdown
before
the
first
frogman
from
the
USS
"Yorktown"
arrived,
as
the
spacecraft
had
landed
before
sunrise.
Forty-five minutes
later,
the
crew
was
safe
on
the
deck
of
the
aircraft
carrier.
Historical
importance.
Apollo 8
came
at
the
end
of
1968,
a
year
that
had
seen
much
upheaval
around
the
world.
Yet,
"TIME"
magazine
chose
the
crew
of
Apollo 8
as
their
Men
of
the
Year
for
1968,
recognizing
them
as
the
people
who
most
influenced
events
in
the
preceding
year.
They
had
been
the
first
people
ever
to
leave
the
gravitational
influence
of
the
Earth
and
orbit
another
celestial
body.
They
had
survived
a
mission
that
even
the
crew
themselves
had
rated
as
only
having
a
fifty-fifty
chance
of
fully
succeeding.
The
effect
of
Apollo 8
can
be
summed
up
by
a
telegram
from
a
stranger,
received
by
Borman
after
the
mission,
that
simply
stated,
"Thank
you
Apollo 8.
You
saved
1968."
One
of
the
most
famous
aspects
of
the
flight
was
the
Earthrise
picture
that
was
taken
as
they
came
around
for
their
fourth
orbit
of
the
Moon.
This
was
the
first
time
that
humans
had
taken
such
a
picture
whilst
actually
behind
the
camera,
and
it
has
been
credited
with
a
role
in
inspiring
the
first
Earth
Day
in
1970.
It
was
selected
as
the
first
of
"Life"
magazine's
'hundred
photos
that
changed
the
world'.
Apollo
8
is
regarded
by
some
as
the
most
historically
significant
of
all
the
Apollo
missions.
The
mission
was
the
most
widely
covered
by
the
media
since
the
first
American
orbital
flight,
Mercury-Atlas
6
by
John
Glenn
in
1962.
There
were
1200
journalists
covering
the
mission,
with
the
BBC
coverage
being
broadcast
in
54
countries
in
15
different
languages.
The
Soviet
newspaper
"Pravda"
featured
a
quote
from
Boris
Nikolaevich
Petrov,
Chairman
of
the
Soviet
Intercosmos
program,
who
described
the
flight
as
an
"outstanding
achievement
of
American
space
sciences
and
technology".
It
is
estimated
that
a
quarter
of
the
people
alive
at
the
time
saw —
either
live
or
delayed —
the
Christmas
Eve
transmission
during
the
ninth
orbit
of
the
Moon.
The
Apollo
8
broadcasts
won
an
Emmy,
the
highest
honor
given
by
the
Academy
of
Television
Arts
and
Sciences.
Atheist
Madalyn
Murray
O'Hair
later
caused
controversy
by
bringing
a
lawsuit
against
NASA
over
the
reading
from
"Genesis".
O'Hair
wished
the
courts
to
ban
US
astronauts —
who
were
all
Government
employees —
from
public
prayer
in
space.
Though
the
case
was
rejected
by
the
US
Supreme
Court
for
lack
of
jurisdiction,
it
caused
NASA
to
be
skittish
about
the
issue
of
religion
throughout
the
rest
of
the
Apollo
program.
Buzz
Aldrin,
on
Apollo
11,
self-communicated
Presbyterian
Communion
on
the
surface
of
the
moon
after
landing;
he
refrained
from
mentioning
this
publicly
for
several
years,
and
only
obliquely
referred
to
it
at
the
time.
In
1969,
the
US
Postal
Service
issued
a
postage
stamp
(1371)
commemorating
the
Apollo 8
flight
around
the
moon.
The
stamp
featured
a
detail
of
the
famous
photograph
of
the
Earthrise
over
the
moon
taken
by
Anders
on
Christmas
Eve,
and
the
words,
"In
the
beginning
God..."
Mission
parameters.
The
mission
parameters
for
Apollo 8
differed
significantly
from
those
of
previous
flights,
for
several
reasons.
As
the
first
manned
spacecraft
to
orbit
multiple
celestial
bodies,
the
mission
recorded
two
different
sets
of
orbital
parameters.
The
mission
was
also
the
first
to
execute
a
translunar
injection.
While
in
parking
orbit
around
the
Earth,
Apollo 8
maintained
altitude
between
a
perigee
of
and
an
apogee
of.
The
inclination
of
this
orbit,
or
its
angle
in
relation
to
the
equator,
was
32.51°.
Each
orbit
had
a
period
of
88.17 minutes.
In
contrast,
the
spacecraft
orbited
the
Moon
at
more
varying
altitudes.
At
its
lowest
altitude
above
the
moon's
surface,
the
spacecraft
had
a
pericynthion
of,
while
the
highest
altitude,
or
apocynthion,
was.
The
spacecraft
took
128.7 minutes
to
complete
each
of
its
10
circuits
around
the
Moon,
at
an
inclination
of
12°.
The
spacecraft
began
its
translunar
injection
burn
on
December
21,
1968,
at
15:41:38
UTC.
The
burn
represented
the
second
of
two
burns
on
the
Saturn V
rocket's
S-IVB
third
stage.
The
rocket
burned
for
a
total
of
318
seconds,
propelling
the
spacecraft
from
an
Earth
parking
orbit
velocity
of
to
a
translunar
trajectory
velocity
of.
Spacecraft
location.
The
command
module
is
now
displayed
at
the
Chicago
Museum
of
Science
and
Industry,
along
with
a
collection
of
personal
items
from
the
flight
donated
by
Lovell
and
the
spacesuit
worn
by
Frank
Borman.
Jim
Lovell's
Apollo
8
spacesuit
is
on
public
display
in
the
Visitor
Center
at
NASA's
Glenn
Research
Center.
Bill
Anders'
spacesuit
is
on
display
at
the
Science
Museum
in
London,
England.
In
film.
Apollo 8's
historic
mission
has
been
shown
and
referred
to
in
several
forms,
both
documentary
and
fiction.
The
various
television
transmissions
and
16 mm
footage
shot
by
the
crew
of
Apollo 8
was
compiled
and
released
by
NASA
in
the
1969
documentary,
"Debrief:
Apollo
8",
which
was
hosted
by
Burgess
Meredith.
In
addition,
Spacecraft
Films
released
a
three-disc
DVD
set
covering
the
mission
in
2003.
Portions
of
the
Apollo 8
Mission
can
be
seen
in
the
1989
documentary
"For
All
Mankind",
which
won
the
Grand
Jury
Prize
at
the
Sundance
Film
Festival
for
Outstanding
Documentary.
The
Apollo
8
mission
was
well
covered
in
the
British
documentary:
'In
the
Shadow
of
the
Moon'.
Apollo
8
was
mentioned
in
the
film
"Apollo
13,"
though
only
briefly.
Portions
of
the
Apollo 8
mission
are
dramatized
in
the
miniseries
"From
the
Earth
to
the
Moon"
episode
"1968".
The
S-IVB
stage
of
Apollo 8
was
also
portrayed
as
the
location
of
an
alien
device
in
the
1970
"UFO"
episode
"Conflict".
---END.OF.DOCUMENT---
Astronaut.
An
astronaut
or
cosmonaut
is
a
person
trained
by
a
human
spaceflight
program
to
command,
pilot,
or
serve
as
a
crew
member
of
a
spacecraft.
While
generally
reserved
for
professional
space
travelers,
the
term
is
sometimes
applied
to
anyone
who
travels
into
space,
including
scientists,
politicians,
journalists,
and
tourists.
Until
2003,
astronauts
were
sponsored
and
trained
exclusively
by
governments,
either
by
the
military,
or
by
civilian
space
agencies.
With
the
sub-orbital
flight
of
the
privately-funded
SpaceShipOne
in
2004,
a
new
category
of
astronaut
was
created:
the
commercial
astronaut.
Definition.
The
criteria
for
what
constitutes
human
spaceflight
vary.
The
Fédération
Aéronautique
Internationale
(FAI)
Sporting
Code
for
astronautics
recognizes
only
flights
that
exceed
an
altitude
of.
In
the
United
States,
professional,
military,
and
commercial
astronauts
who
travel
above
an
altitude
of
are
awarded
astronaut
wings.
As
of
September
19,
2009,
a
total
of
505
humans
from
38
countries
have
reached
100 km
or
more
in
altitude,
of
which
502
reached
Low
Earth
orbit
or
beyond.
Of
these,
24
people
have
traveled
beyond
Low
Earth
orbit,
to
either
lunar
or
trans-lunar
orbit
or
to
the
surface
of
the
moon;
three
of
the
24
did
so
twice:
Jim
Lovell,
John
Young
and
Eugene
Cernan.
Under
the
U.
S.
definition,
496
people
qualify
as
having
reached
space,
above
altitude.
Of
eight
X-15
pilots
who
exceeded
50
miles
in
altitude,
seven
reached
above
but
below
100
kilometers
(about
62
miles).
Space
travelers
have
spent
over
30,400
person-days
(or
a
cumulative
total
of
over
83
years)
in
space,
including
over
100
astronaut-days
of
spacewalks.
As
of
2008,
the
man
with
the
longest
time
in
space
is
Sergei
K.
Krikalev,
who
has
spent
803
days,
9
hours
and
39
minutes,
or
2.2
years,
in
space.
Peggy
A.
Whitson
holds
the
record
for
most
time
in
space
by
a
woman,
377
days.
English-speaking
nations.
In
the
United
States,
Canada,
United
Kingdom,
and
many
other
English-speaking
nations,
a
professional
space
traveler
is
called
an
"astronaut".
The
term
derives
from
the
Greek
words
"ástron"
(ἄστρον),
meaning
"star",
and
"nautes"
(ναύτης),
meaning
"sailor".
The
first
known
use
of
the
term
"astronaut"
in
the
modern
sense
was
by
Neil
R.
Jones
in
his
short
story
"The
Death's
Head
Meteor"
in
1930.
The
word
itself
had
been
known
earlier.
For
example,
in
Percy
Greg's
1880
book
"Across
the
Zodiac",
"astronaut"
referred
to
a
spacecraft.
In
"Les
Navigateurs
de
l'Infini"
(1925)
of
J.-H.
Rosny
aîné,
the
word
"astronautique"
(astronautic)
was
used.
The
word
may
have
been
inspired
by
"aeronaut",
an
older
term
for
an
air
traveler
first
applied
(in
1784)
to
balloonists.
NASA
applies
the
term
astronaut
to
any
crew
member
aboard
NASA
spacecraft
bound
for
Earth
orbit
or
beyond.
NASA
also
uses
the
term
as
a
title
for
those
selected
to
join
its
Astronaut
Corps.
The
European
Space
Agency
similarly
uses
the
term
astronaut
for
members
of
its
Astronaut
Corps.
Russian.
By
convention,
an
astronaut
employed
by
the
Russian
Federal
Space
Agency
(or
its
Soviet
predecessor)
is
called
a
cosmonaut
in
English
texts.
The
word
is
an
anglicisation
of
the
Russian
word
"kosmonavt"
(),
which
in
turn
derives
from
the
Greek
words
"kosmos"
(κόσμος),
meaning
"universe",
and
"nautes"
(ναύτης),
meaning
"sailor".
For
the
most
part,
"cosmonaut"
and
"astronaut"
are
synonyms
in
all
languages,
and
the
usage
of
choice
is
often
dictated
by
political
reasons.
Yuri
Gagarin,
Russian,
is
the
first
human
cosmonaut.
Valentina
Tereshkova,
Russian,
is
the
first
woman
cosmonaut.
On
March
14,
1995,
Norman
Thagard
became
the
first
American
to
ride
to
space
on
board
a
Russian
launch
vehicle,
arguably
becoming
the
first
"American
cosmonaut"
in
the
process.
Chinese.
Official
English-language
texts
issued
by
the
government
of
the
People's
Republic
of
China
use
"astronaut"
while
texts
in
Russian
use
"космонавт"
("kosmonavt").
In
China,
the
terms
"yǔhángyuán"
(,
"sailing
personnel
in
universe")
or
"hángtiānyuán"
(,
"sailing
personnel
in
sky")
have
long
been
used
for
astronauts.
The
phrase
"tàikōng
rén"
(,
"spaceman")
is
often
used
in
Taiwan
and
Hong
Kong.
The
term
taikonaut
is
used
by
some
English-language
news
media
organizations
for
professional
space
travelers
from
China.
The
word
has
featured
in
the
Longman
and
Oxford
English
dictionaries,
the
latter
of
which
describes
it
as
"a
hybrid
of
the
Chinese
term
"taikong"
(space)
and
the
Greek
"naut"
(sailor)";
the
term
became
more
common
in
2003
when
China
sent
its
first
astronaut
Yang
Liwei
into
space
aboard
the
"Shenzhou
5"
spacecraft.
This
is
the
term
used
by
Xinhua
in
the
English
version
of
the
Chinese
People's
Daily
since
the
advent
of
the
Chinese
space
program.
The
origin
of
the
term
is
unclear;
as
early
as
May
1998,
Chiew
Lee
Yih
()
from
Malaysia,
used
it
in
newsgroups,
while
Chen
Lan
(),
almost
simultaneously,
announced
it
at
his
"Go
Taikonauts!"
GeoCities
page.
Other
terms.
With
the
rise
of
space
tourism,
NASA
and
the
Russian
Federal
Space
Agency
agreed
to
use
the
term
"spaceflight
participant"
to
distinguish
those
space
travelers
from
astronauts
on
missions
coordinated
by
those
two
agencies.
While
no
nation
other
than
Russia
(formerly
the
Soviet
Union),
the
United
States,
and
China
has
launched
a
manned
spacecraft,
several
other
nations
have
sent
people
into
space
in
cooperation
with
one
of
these
countries.
Inspired
partly
by
these
missions,
other
synonyms
for
astronaut
have
entered
occasional
English
usage.
For
example,
the
term
spationaut
(French
spelling:
"spationaute")
is
sometimes
used
to
describe
French
space
travelers,
from
the
Latin
word
"spatium"
or
space,
and
the
Malay
term
"angkasawan"
was
used
to
describe
participants
in
the
Angkasawan
program.
Space
travel
milestones.
The
first
human
in
space
was
Russian
Yuri
Gagarin,
who
was
launched
into
space
on
April
12,
1961
aboard
Vostok
1
and
orbited
around
the
Earth
for
108
minutes.
There
are
allegations
that
Gagarin
ejected
from
landing
module
after
re-entering
the
atmosphere
and
parachuted
back,
due
to
safety
concerns
about
the
craft's
landing
systems.
The
first
woman
in
space
was
Russian
Valentina
Tereshkova,
launched
in
June
1963
aboard
Vostok
6.
Alan
Shepard
became
the
first
American
and
second
person
in
space
on
May
5,
1961
on
a
15-minute
sub-orbital
flight.
The
first
American
woman
in
space
was
Sally
Ride,
during
Space
Shuttle
Challenger's
mission
STS-7,
on
June
18,
1983.
The
first
mission
to
orbit
the
moon
was
"Apollo
8",
which
included
William
Anders
who
was
born
in
Hong
Kong,
making
him
the
first
Asian-born
astronaut
in
1968.
In
April
1985,
Taylor
Wang
became
the
first
ethnic
Chinese
person
in
space.
On
15
October
2003,
Yang
Liwei
became
China's
first
astronaut
on
the
Shenzhou
5
spacecraft.
The
Soviet
Union,
through
its
Intercosmos
program,
allowed
people
from
other
"socialist"
(i.e.
Warsaw
Pact
and
other
Soviet-allied)
countries
to
fly
on
its
missions.
An
example
is
Vladimír
Remek,
a
Czechoslovak,
who
became
the
first
non-Soviet
European
in
space
in
1978
on
a
Russian
Soyuz-U
rocket.
On
July
23,
1980,
Pham
Tuan
of
Vietnam
became
the
first
Asian
in
space
when
he
flew
aboard
Soyuz
37.
Also
in
1980,
Cuban
Arnaldo
Tamayo
Méndez
became
the
first
person
of
Hispanic
and
black
African
descent
to
fly
in
space,
Guion
Bluford
became
the
first
African
American
to
fly
into
space.
The
first
person
born
in
Africa
to
fly
in
space
was
Patrick
Baudry,
in
1985.
In
1988,
Abdul
Ahad
Mohmand
became
the
first
Afghan
to
reach
space,
spending
nine
days
aboard
the
Mir
space
station.
With
the
larger
number
of
seats
available
on
the
Space
Shuttle,
the
U.S.
began
taking
international
astronauts.
In
1983,
Ulf
Merbold
of
West
Germany
became
the
first
non-US
citizen
to
fly
in
a
US
spacecraft.
In
1985,
Rodolfo
Neri
Vela
became
the
first
Mexican-born
person
in
space.
In
1991,
Helen
Sharman
became
the
first
Briton
to
fly
in
space.
In
2002,
Mark
Shuttleworth
became
the
first
citizen
of
an
African
country
to
fly
in
space,
as
a
paying
spaceflight
participant.
In
2003,
Ilan
Ramon
became
the
first
Israeli
to
fly
in
space,
although
he
died
during
a
re-entry
accident.
Age
milestones.
The
youngest
person
to
fly
in
space
is
Gherman
Titov,
who
was
25
years
old
when
he
flew
Vostok
2.
(Titov
was
also
the
first
person
to
suffer
space
sickness).
The
oldest
person
who
has
flown
in
space
is
John
Glenn,
who
was
77
when
he
flew
on
STS-95.
Duration
and
distance
milestones.
The
longest
stay
in
space
was
438
days,
by
Russian
Valeri
Polyakov.
As
of
2006,
the
most
spaceflights
by
an
individual
astronaut
is
seven,
a
record
held
by
both
Jerry
L.
Ross
and
Franklin
Chang-Diaz.
The
farthest
distance
from
Earth
an
astronaut
has
traveled
was
401,056 km,
when
Jim
Lovell,
John
Swigert,
and
Fred
Haise
went
around
the
Moon
during
the
Apollo
13
emergency.
Civilian
and
non-government
milestones.
The
first
civilian
in
space
was
Neil
Armstrong,
who
had
retired
from
the
United
States
Navy
before
his
first
spaceflight
on
Gemini
8.
The
first
person
in
space
who
had
never
been
a
member
of
any
country's
armed
forces
was
Harrison
Schmitt,
a
geologist
who
first
flew
in
space
on
Apollo
17.
Both
Armstrong
and
Schmitt
were
directly
employed
by
NASA.
The
first
non-governmental
space
traveler
was
Byron
K.
Lichtenberg,
a
researcher
from
the
Massachusetts
Institute
of
Technology
who
flew
on
STS-9
in
1983.
In
December
1990,
Toyohiro
Akiyama
became
the
first
paying
space
traveler
as
a
reporter
for
Tokyo
Broadcasting
System,
a
visit
to
Mir
as
part
of
an
estimated
$12
million
(USD)
deal
with
a
Japanese
TV
station,
although
at
the
time,
the
term
used
to
refer
to
Akiyama
was
"Research
Cosmonaut".
Akiyama
suffered
severe
space-sickness
during
his
mission,
which
affected
his
productivity.
The
first
self-funded
space
tourist
was
Dennis
Tito
onboard
the
Russian
spacecraft
Soyuz
TM-3
on
28
April
2001.
Training.
The
first
NASA
astronauts
were
selected
for
training
in
1959.
Early
in
the
space
program,
military
jet
test
piloting
and
engineering
training
were
often
cited
as
prerequisites
for
selection
as
an
astronaut
at
NASA,
although
neither
John
Glenn
nor
Scott
Carpenter
(of
the
Mercury
Seven)
had
any
university
degree,
in
engineering
or
any
other
discipline
at
the
time
of
their
selection.
Selection
was
initially
limited
to
military
pilots.
The
earliest
astronauts
for
both
America
and
Russia
tended
to
be
jet
fighter
pilots,
and
were
often
test
pilots.
Once
selected,
NASA
astronauts
go
through
20
months
of
training
in
a
variety
of
areas,
including
training
for
extra-vehicular
activity
in
a
facility
such
as
NASA's
Neutral
Buoyancy
Laboratory.
Astronauts-in-training
may
also
experience
short
periods
of
weightlessness
in
aircraft
called
the
"vomit
comet",
the
nickname
given
to
a
pair
of
modified
KC-135s
(retired
in
2000
and
2004
respectively,
and
replaced
in
2005
with
a
C-9)
which
perform
parabolic
flights.
Astronauts
are
also
required
to
accumulate
a
number
of
flight
hours
in
high-performance
jet
aircraft.
This
is
mostly
done
in
T-38
jet
aircraft
out
of
Ellington
Field,
due
to
its
proximity
to
the
Johnson
Space
Center.
Ellington
Field
is
also
where
the
Shuttle
Training
Aircraft
is
maintained
and
developed,
although
most
flights
of
the
aircraft
are
done
out
of
Edwards
Air
Force
Base.
Mission
Specialist
Educator.
Mission
Specialist
Educators,
or
"Educator
Astronauts",
were
first
selected
in
2004,
and
as
of
2007,
there
are
three
NASA
Educator
astronauts:
Joseph
M.
Acaba,
Richard
R.
Arnold,
and
Dorothy
Metcalf-Lindenburger.
Barbara
Morgan,
selected
as
back-up
teacher
to
Christa
McAuliffe
in
1985,
is
considered
to
be
the
first
Educator
astronaut
by
the
media,
but
she
trained
as
a
mission
specialist.
The
Educator
Astronaut
program
is
a
successor
to
the
Teacher
in
Space
program
from
the
1980s.
Health
risks
of
space
travel.
Astronauts
are
susceptible
to
a
variety
of
health
risks
including
decompression
sickness,
barotrauma,
immunodeficiencies,
loss
of
bone
and
muscle,
orthostatic
intolerance
due
to
volume
loss,
sleep
disturbances,
and
radiation
injury.
A
variety
of
large
scale
medical
studies
are
being
conducted
in
space
via
the
National
Space
and
Biomedical
Research
Institute
(NSBRI)
to
address
these
issues.
Prominent
among
these
is
the
Advanced
Diagnostic
Ultrasound
in
Microgravity
Study
in
which
astronauts
(including
former
ISS
commanders
Leroy
Chiao
and
Gennady
Padalka)
perform
ultrasound
scans
under
the
guidance
of
remote
experts
to
diagnose
and
potentially
treat
hundreds
of
medical
conditions
in
space.
This
study's
techniques
are
now
being
applied
to
cover
professional
and
Olympic
sports
injuries
as
well
as
ultrasound
performed
by
non-expert
operators
in
medical
and
high
school
students.
It
is
anticipated
that
remote
guided
ultrasound
will
have
application
on
Earth
in
emergency
and
rural
care
situations,
where
access
to
a
trained
physician
is
often
rare.
For
more
information
on
the
health
hazards
faced
by
astronauts,
go
to
the
article
entitled
Space
medicine.
Insignia.
At
NASA,
people
who
complete
astronaut
candidate
training
receive
a
silver
lapel
pin.
Once
they
have
flown
in
space,
they
receive
a
gold
pin.
U.S.
astronauts
who
also
have
active-duty
military
status
receive
a
special
qualification
badge,
known
as
the
Astronaut
Badge,
after
participation
on
a
spaceflight.
The
United
States
Air
Force
also
presents
an
Astronaut
Badge
to
its
pilots
who
exceed
50
miles
(80 km)
in
altitude.
Deaths.
Eighteen
astronauts
have
lost
their
lives
during
spaceflight,
on
four
missions.
By
nationality,
they
are
thirteen
Americans,
three
Russians,
one
Ukrainian,
and
one
Israeli.
Several
others
have
died
while
training
for
space
missions.
The
Space
Mirror
Memorial,
which
stands
on
the
grounds
of
the
John
F.
Kennedy
Space
Center
Visitor
Complex,
commemorates
the
lives
of
the
men
and
women
who
have
died
during
spaceflight
and
during
training
in
the
space
programs
of
the
United
States.
In
addition
to
twenty
NASA
career
astronauts,
the
memorial
includes
the
names
of
a
U.S.
Air
Force
X-15
test
pilot,
a
U.S.
Air
Force
officer
who
died
while
training
for
a
then-classified
military
space
program,
a
civilian
spaceflight
participant
who
died
in
the
Challenger
disaster,
and
an
international
astronaut
who
was
killed
in
the
Columbia
disaster.
---END.OF.DOCUMENT---
A
Modest
Proposal.
"A
Modest
Proposal:
For
Preventing
the
Children
of
Poor
People
in
Ireland
from
Being
a
Burden
to
Their
Parents
or
Country,
and
for
Making
Them
Beneficial
to
the
Publick",
commonly
referred
to
as
"A
Modest
Proposal",
is
a
Juvenalian
satirical
essay
written
and
published
anonymously
by
Jonathan
Swift
in
1729.
Swift
appears
to
suggest
in
his
essay
that
the
impoverished
Irish
might
ease
their
economic
troubles
by
selling
children
as
food
for
rich
gentlemen
and
ladies.
By
doing
this
he
mocks
the
authority
of
the
British
officials.
Details.
Swift
goes
to
great
lengths
to
support
his
argument,
including
a
list
of
possible
preparation
styles
for
the
children,
and
calculations
showing
the
financial
benefits
of
his
suggestion.
He
uses
common
methods
of
argument
throughout
his
essay,
such
as
appealing
to
the
authority
of
"a
very
knowing
American
of
my
acquaintance
in
London"
and
"the
famous
Psalmanazar,
a
native
of
the
island
Formosa"
(who
had
already
confessed
to
"not"
being
from
Formosa
in
1706).
Swift
couches
his
arguments
in
then-current
events,
exploiting
common
prejudice
against
Catholics
(misnomed
"Papists")
and
pointing
out
their
depredations
of
England.
After
enumerating
the
benefits
of
his
proposal,
Swift
addresses
possible
objections
including
the
depopulation
of
Ireland
and
a
litany
of
other
solutions
which
he
dismisses
as
impractical.
This
essay
is
widely
held
to
be
one
of
the
greatest
examples
of
sustained
irony
in
the
history
of
the
English
language.
Much
of
its
shock
value
derives
from
the
fact
that
the
first
portion
of
the
essay
describes
the
plight
of
starving
beggars
in
Ireland,
so
that
the
reader
is
unprepared
for
the
surprise
of
Swift's
solution
when
he
states,
"A
young
healthy
child
well
nursed,
is,
at
a
year
old,
a
most
delicious
nourishing
and
wholesome
food,
whether
stewed,
roasted,
baked,
or
boiled;
and
I
make
no
doubt
that
it
will
equally
serve
in
a
fricassee,
or
a
ragout."
Readers
unacquainted
with
its
reputation
as
a
satirical
work
often
do
not
immediately
realize
that
Swift
was
not
seriously
proposing
cannibalism
and
infanticide,
nor
would
readers
unfamiliar
with
the
satires
of
Horace
and
Juvenal
recognize
that
Swift's
essay
follows
the
rules
and
structure
of
Latin
satires.
The
satirical
element
of
the
pamphlet
is
often
only
understood
after
the
reader
notes
the
allusions
made
by
Swift
to
the
attitudes
of
landlords,
such
as
the
following:
"I
grant
this
food
may
be
somewhat
dear,
and
therefore
very
proper
for
Landlords,
who
as
they
have
already
devoured
most
of
the
Parents,
seem
to
have
the
best
Title
to
the
Children."
Swift
extends
the
metaphor
to
get
in
a
few
jibes
at
England’s
mistreatment
of
Ireland,
noting
that
"For
this
kind
of
commodity
will
not
bear
exportation,
and
flesh
being
of
too
tender
a
consistence,
to
admit
a
long
continuance
in
salt,
although
perhaps
I
could
name
a
country,
which
would
be
glad
to
eat
up
our
whole
nation
without
it."
Population
solutions.
It
has
been
argued
that
Swift’s
main
target
in
"A
Modest
Proposal"
was
not
the
conditions
in
Ireland,
but
rather
the
can-do
spirit
of
the
times
that
led
people
to
devise
a
number
of
illogical
schemes
that
would
purportedly
solve
social
and
economic
ills.
Swift
was
especially
insulted
by
projects
that
tried
to
fix
population
and
labor
issues
with
a
simple
cure-all
solution.
A
memorable
example
of
these
sorts
of
schemes
"involved
the
idea
of
running
the
poor
through
a
joint-stock
company".
In
response,
Swift’s
"Modest
Proposal"
was
"a
burlesque
of
projects
concerning
the
poor",
that
were
in
vogue
during
the
early
18th
century.
"A
Modest
Proposal"
also
targets
the
calculating
way
people
perceived
the
poor
in
designing
their
projects.
The
pamphlet
targets
reformers
who
"regard
people
as
commodities".
In
the
piece,
Swift
adopts
the
"technique
of
a
political
arithmetician"
to
show
the
utter
ridiculousness
of
trying
to
prove
any
proposal
with
dispassionate
statistics.
Critics
differ
about
Swift’s
intentions
in
using
this
faux-mathematical
philosophy.
Edmund
Wilson
argues
that
statistically
"the
logic
of
the
'Modest
proposal'
can
be
compared
with
defense
of
crime
(arrogated
to
Marx)
in
which
he
argues
that
crime
takes
care
of
the
superfluous
population".
Wittkowsky
counters
that
Swift's
satiric
use
of
statistical
analysis
is
an
effort
to
enhance
his
satire
that
"springs
from
a
spirit
of
bitter
mockery,
not
from
the
delight
in
calculations
for
their
own
sake".
Rhetoric.
Charles
K.
Smith
argues
that
Swift’s
rhetorical
style
persuades
the
reader
to
detest
the
speaker
and
pity
the
Irish.
Swift’s
specific
strategy
is
twofold,
using
a
"trap"
to
create
sympathy
for
the
Irish
and
a
dislike
of
the
narrator
who,
in
the
span
of
one
sentence,
"details
vividly
and
with
rhetorical
emphasis
the
grinding
poverty"
but
feels
emotion
solely
for
members
of
his
own
class.
Swift’s
use
of
gripping
details
of
poverty
and
his
narrator’s
cool
approach
towards
them
creates
"two
opposing
points
of
view"
which
"alienate
the
reader,
perhaps
unconsciously,
from
a
narrator
who
can
view
with
'melancholy'
detachment
a
subject
that
Swift
has
directed
us,
rhetorically,
to
see
in
a
much
less
detached
way".
Swift
has
his
proposer
further
degrade
the
Irish
by
using
language
ordinarily
reserved
for
animals.
Lewis
argues
that
the
speaker
uses
"the
vocabulary
of
animal
husbandry"
to
describe
the
Irish.
Once
the
children
have
been
commoditized,
Swift’s
rhetoric
can
easily
turn
"people
into
animals,
then
meat,
and
from
meat,
logically,
into
tonnage
worth
a
price
per
pound".
Swift
uses
the
proposer’s
serious
tone
to
highlight
the
absurdity
of
his
proposal.
In
making
his
argument,
the
speaker
uses
the
conventional,
text
book
approved
order
of
argument
from
Swift’s
time.
The
contrast
between
the
"careful
control
against
the
almost
inconceivable
perversion
of
his
scheme"
and
"the
ridiculousness
of
the
proposal"
create
a
situation
in
which
the
reader
has
"to
consider
just
what
perverted
values
and
assumptions
would
allow
such
a
diligent,
thoughtful,
and
conventional
man
to
propose
so
perverse
a
plan".
Tertullian’s
"Apology".
Some
scholars
have
argued
that
"A
Modest
Proposal"
was
largely
influenced
and
inspired
by
Tertullian’s
"Apology".
While
Tertullian’s
"Apology"
is
a
satirical
attack
against
early
Roman
persecution
of
Christianity,
Swift’s
"A
Modest
Proposal"
addresses
the
Anglo-Irish
situation
in
the
1720s.
James
William
Johnson
believes
that
Swift
saw
major
similarities
between
the
two
situations.
Johnson
notes
Swift’s
obvious
affinity
for
Tertullian
and
the
bold
stylistic
and
structural
similarities
between
the
works
"A
Modest
Proposal"
and
"Apology".
In
structure,
Johnson
points
out
the
same
central
theme;
that
of
cannibalism
and
the
eating
of
babies;
and
the
same
final
argument;
that
"human
depravity
is
such
that
men
will
attempt
to
justify
their
own
cruelty
by
accusing
their
victims
of
being
lower
than
human".
Stylistically,
Swift
and
Tertullian
share
the
same
command
of
sarcasm
and
language.
In
agreement
with
Johnson,
Donald
C.
Baker
points
out
the
similarity
between
both
authors'
tones
and
use
of
irony.
Baker
notes
the
uncanny
way
that
both
authors
imply
an
ironic
"justification
by
ownership"
over
the
subject
of
sacrificing
children—Tertullian
while
attacking
pagan
parents,
and
Swift
while
attacking
the
English
mistreatment
of
the
Irish
poor.
Economic
themes.
Robert
Phiddian's
article
"Have
you
eaten
yet?
The
Reader
in
A
Modest
Proposal"
focuses
on
two
aspects
of
"A
Modest
Proposal":
the
voice
of
Swift
and
the
voice
of
the
Proposer.
Phiddian
stresses
that
a
reader
of
the
pamphlet
must
learn
to
distinguish
between
the
satiric
voice
of
Jonathan
Swift
and
the
apparent
economic
projections
of
the
Proposer.
He
reminds
readers
that
"there
is
a
gap
between
the
narrator’s
meaning
and
the
text’s,
and
that
a
moral-political
argument
is
being
carried
out
by
means
of
parody".
While
Swift’s
proposal
is
obviously
not
a
serious
economic
proposal,
George
Wittkowsky,
author
of
"Swift’s
Modest
Proposal:
The
Biography
of
an
Early
Georgian
Pamphlet",
argues
that
it
in
order
to
understand
the
piece
fully,
it
is
important
to
understand
the
economics
of
Swift’s
time.
Wittowsky
argues
that
not
enough
critics
have
taken
the
time
to
focus
directly
on
the
mercantilism
and
theories
of
labor
in
18th
century
England.
"[I]f
one
regards
the
"Modest
Proposal"
simply
as
a
criticism
of
condition,
about
all
one
can
say
is
that
conditions
were
bad
and
that
Swift's
irony
brilliantly
underscored
this
fact".
At
the
start
of
a
new
industrial
age
in
the
18th
century,
it
was
believed
that
"people
are
the
riches
of
the
nation",
and
there
was
a
general
faith
in
an
economy
which
paid
its
workers
low
wages
because
high
wages
would
mean
workers
would
work
less.
Furthermore,
"in
the
mercantilist
view
no
child
was
too
young
to
go
into
industry".
In
those
times,
the
"somewhat
more
humane
attitudes
of
an
earlier
day
had
all
but
disappeared
and
the
laborer
had
come
to
be
regarded
as
a
commodity".
People
are
the
riches
of
a
nation.
Louis
A.
Landa
presents
Swift’s
"A
Modest
Proposal"
as
a
critique
of
the
popular
and
unjustified
maxim
of
mercantilism
in
the
eighteenth
century
that
"people
are
the
riches
of
a
nation".
Swift
presents
the
dire
state
of
Ireland
and
shows
that
mere
population
itself,
in
Ireland’s
case,
did
not
always
mean
greater
wealth
and
economy.
The
uncontrolled
maxim
fails
to
take
into
account
that
a
person
that
does
not
produce
in
an
economic
or
political
way
makes
a
country
poorer,
not
richer.
Swift
also
recognizes
the
implications
of
such
a
fact
in
making
mercantilist
philosophy
a
paradox:
the
wealth
of
a
country
is
based
on
the
poverty
of
the
majority
of
its
citizens.
Swift
however,
Landa
argues,
is
not
merely
criticizing
economic
maxims
but
also
addressing
the
fact
that
England
was
denying
Irish
citizens
their
natural
rights
and
dehumanizing
them
by
viewing
them
as
a
mere
commodity.
Modern
usage.
"A
Modest
Proposal"
is
included
in
many
literature
programs
as
an
example
of
early
modern
western
satire.
It
also
serves
as
an
exceptional
introduction
to
the
concept
and
use
of
argumentative
language,
lending
itself
well
to
secondary
and
post-secondary
essay
courses.
Outside
of
the
realm
of
English
studies,
"A
Modest
Proposal"
is
a
relevant
piece
included
in
many
comparative
and
global
literature
and
history
courses,
as
well
as
those
of
numerous
other
disciplines
in
the
arts,
humanities,
and
even
the
social
sciences.
It
has
been
emulated
many
times
as
well.
In
his
book
"A
Modest
Proposal"
(1984),
evangelical
author
Frank
Schaeffer
emulated
Swift's
work
in
social
conservative
polemic
against
abortion
and
euthanasia
in
a
future
dystopia
that
advocated
recycling
of
aborted
embryos
and
fetuses,
as
well
as
some
disabled
infants
with
compound
intellectual,
physical
and
physiological
difficulties.
(Such
Baby
Doe
Rules
cases
were
then
a
major
concern
of
the
pro-life
movement
of
the
early
1980s,
which
viewed
selective
treatment
of
those
infants
as
disability
discrimination.)
In
Hunter
S.
Thompson's,
which
contains
hundreds
of
private
letters
written
by
Thompson
over
the
years,
contains
a
letter
in
which
he
uses
"A
Modest
Proposals
satire
technique
against
the
Vietnam
War.
Thompson
writes
a
letter
to
a
local
Aspen
newspaper
informing
them
that,
on
Christmas
Eve,
he
was
going
to
use
napalm
to
burn
a
number
of
dogs
and
hopefully
any
humans
they
find.
This
letter
protests
the
burning
of
Vietnamese
people
occurring
overseas.
In
popular
culture.
The
game
"Orphan
Feast"
on
Cartoon
Network's
Adult
Swim
website
is
loosely
based
on
"A
Modest
Proposal".
The
show
"Sealab
2021"
references
"A
Modest
Proposal"
by
the
character
of
Jodene
Sparks.
It
was
suggested
as
recommended
reading
when
Debbie
wanted
a
child.
"A
Modest
Proposal"
is
the
name
of
The
University
of
Texas
at
Dallas',
the
monthly
opinion
paper
of
the
University;
and
was
the
name
of
a
regular
column
in
of
Harvard
University,
a
satire
publication
which
also
takes
its
name
from
Johnathan
Swift.
"A
Modest
Proposal"
is
mentioned
in
the
1996
film
"The
Birdcage".
Controversial
American
political
activist
and
disbarred
attorney
Jack
Thompson's
A
Modest
Video
Game
Proposal
draws
its
title
from
"A
Modest
Proposal".
One
of
the
radio
presenters
in
the
game
Saint's
Row
claims
he
has
"A
modest
proposal"
which
is
to
apply
shock
collars
to
all
immigrants
in
America.
---END.OF.DOCUMENT---
